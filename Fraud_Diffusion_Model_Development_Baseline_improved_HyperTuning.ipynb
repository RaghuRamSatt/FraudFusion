{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fraud Diffusion Model - Hyperparameter Tuning\n",
    "# \n",
    "# This notebook systematically tests different loss weight combinations for the diffusion model.\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import joblib\n",
    "from scipy.stats import wasserstein_distance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Step 1: Data and Artifact Loading\n",
    "#############################################\n",
    "# Load preprocessed data\n",
    "X_train_df = pd.read_csv(r\"D:\\DS Northeastern\\DS 5500 - Capstone\\FraudFusion\\Data\\processed\\X_train.csv\")\n",
    "y_train_df = pd.read_csv(r\"D:\\DS Northeastern\\DS 5500 - Capstone\\FraudFusion\\Data\\processed\\y_train.csv\")\n",
    "\n",
    "# Load preprocessing artifacts\n",
    "scaler = joblib.load(r\"D:\\DS Northeastern\\DS 5500 - Capstone\\FraudFusion\\Data\\processed\\standard_scaler.pkl\")\n",
    "cat_vocab = joblib.load(r\"D:\\DS Northeastern\\DS 5500 - Capstone\\FraudFusion\\Data\\processed\\cat_vocab.pkl\")\n",
    "cat_mapping = joblib.load(r\"D:\\DS Northeastern\\DS 5500 - Capstone\\FraudFusion\\Data\\processed\\cat_mapping.pkl\")\n",
    "\n",
    "# Feature setup\n",
    "numeric_features = ['amt', 'lat', 'long', 'city_pop', 'merch_lat', 'merch_long',\n",
    "                    'age', 'trans_hour', 'trans_day', 'trans_month', 'trans_dayofweek']\n",
    "cat_features = ['merchant', 'category', 'gender', 'street', 'city', 'state', 'zip', 'job']\n",
    "eng_indices = [numeric_features.index(f) for f in ['trans_hour', 'trans_day', 'trans_month', 'trans_dayofweek']]\n",
    "\n",
    "# Filter fraud samples\n",
    "fraud_mask = (y_train_df.iloc[:, 0] == 1)\n",
    "X_train_num = X_train_df[numeric_features].loc[fraud_mask].values\n",
    "X_train_cat = X_train_df[cat_features].loc[fraud_mask].values\n",
    "X_nonfraud_num = X_train_df[numeric_features].loc[~fraud_mask].values\n",
    "\n",
    "# Engineered feature constraints\n",
    "eng_min = torch.tensor(np.min(X_train_num[:, eng_indices], axis=0), \n",
    "                      dtype=torch.float32, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "eng_max = torch.tensor(np.max(X_train_num[:, eng_indices], axis=0),\n",
    "                     dtype=torch.float32, device='cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Step 2: Dataset and Model Setup\n",
    "#############################################\n",
    "class FraudDataset(Dataset):\n",
    "    def __init__(self, num_data, cat_data):\n",
    "        self.num_data = torch.tensor(num_data, dtype=torch.float32)\n",
    "        self.cat_data = torch.tensor(cat_data, dtype=torch.long)\n",
    "    def __len__(self): return len(self.num_data)\n",
    "    def __getitem__(self, idx): return self.num_data[idx], self.cat_data[idx]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "T_train = 800\n",
    "beta = torch.linspace(1e-4, 0.02, T_train).to(device)\n",
    "alpha = 1 - beta\n",
    "alpha_hat = torch.cumprod(alpha, dim=0)\n",
    "X_nonfraud_tensor = torch.tensor(X_nonfraud_num, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Step 3: Model Architecture\n",
    "#############################################\n",
    "class CombinedNoisePredictor(nn.Module):\n",
    "    def __init__(self, num_input_dim, cat_vocab_sizes, cat_embed_dim=2, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        for col, vocab_size in cat_vocab_sizes.items():\n",
    "            self.embeddings[col] = nn.Embedding(vocab_size, cat_embed_dim)\n",
    "        cat_total_dim = len(cat_vocab_sizes) * cat_embed_dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_input_dim + cat_total_dim + 1, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, num_input_dim)  # Only predict numeric noise\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_num, x_cat, t):\n",
    "        embeds = [self.embeddings[col](x_cat[:,i]) for i,col in enumerate(self.embeddings)]\n",
    "        x = torch.cat([x_num, *embeds, (t.float()/T_train).unsqueeze(1)], dim=1)\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Step 4: Core Training Functions\n",
    "#############################################\n",
    "def forward_diffusion(x0, t):\n",
    "    sqrt_alpha_hat_t = torch.sqrt(alpha_hat[t]).unsqueeze(1)\n",
    "    sqrt_one_minus_alpha_hat_t = torch.sqrt(1 - alpha_hat[t]).unsqueeze(1)\n",
    "    noise = torch.randn_like(x0)\n",
    "    x_t = sqrt_alpha_hat_t * x0 + sqrt_one_minus_alpha_hat_t * noise\n",
    "    return x_t, noise\n",
    "\n",
    "def train_model(config, train_loader, num_epochs=150):\n",
    "    model = CombinedNoisePredictor(\n",
    "        num_input_dim=len(numeric_features),\n",
    "        cat_vocab_sizes={col: cat_vocab[col] for col in cat_features}\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "    \n",
    "    mu_nf = X_nonfraud_tensor.mean(dim=0)\n",
    "    sigma_nf = X_nonfraud_tensor.std(dim=0) + 1e-5\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_num, x_cat in train_loader:\n",
    "            x_num, x_cat = x_num.to(device), x_cat.to(device)\n",
    "            t = torch.randint(0, T_train, (x_num.size(0),), device=device)\n",
    "            \n",
    "            # Forward diffusion and prediction\n",
    "            x_t, noise = forward_diffusion(x_num, t)\n",
    "            pred_noise = model(x_t, x_cat, t)\n",
    "            \n",
    "            # Loss calculations\n",
    "            mse_loss = F.mse_loss(pred_noise, noise)\n",
    "            \n",
    "            z = (pred_noise - mu_nf)/sigma_nf\n",
    "            prior_loss = (1 - 2*(1 - torch.distributions.Normal(0,1).cdf(torch.abs(z)))).mean()\n",
    "            \n",
    "            x0_est = (x_t - torch.sqrt(1-alpha_hat[t]).unsqueeze(1)*pred_noise)/torch.sqrt(alpha_hat[t]).unsqueeze(1)\n",
    "            rand_idx = torch.randint(0, len(X_nonfraud_tensor), (x_num.size(0),))\n",
    "            triplet_loss = F.relu(F.pairwise_distance(x0_est, x_num) - \n",
    "                                 F.pairwise_distance(x0_est, X_nonfraud_tensor[rand_idx]) + 1).mean()\n",
    "            \n",
    "            eng_loss = torch.mean(F.relu(eng_min - x0_est[:,eng_indices]) + \n",
    "                                F.relu(x0_est[:,eng_indices] - eng_max))\n",
    "            \n",
    "            total_loss += (mse_loss + config['w1']*prior_loss + \n",
    "                         config['w2']*triplet_loss + config['lambda_eng']*eng_loss).item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            (mse_loss + config['w1']*prior_loss + config['w2']*triplet_loss + config['lambda_eng']*eng_loss).backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Loss: {total_loss/len(train_loader):.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Step 5: Synthetic Generation & Evaluation\n",
    "#############################################\n",
    "def generate_categorical_samples(num_samples):\n",
    "    \"\"\"Generate categoricals from empirical distribution\"\"\"\n",
    "    synthetic_cat = {}\n",
    "    for col in cat_features:\n",
    "        value_counts = pd.Series(X_train_cat[:, cat_features.index(col)]).value_counts(normalize=True)\n",
    "        categories = value_counts.index.values\n",
    "        probabilities = value_counts.values\n",
    "        synthetic_cat[col] = np.random.choice(categories, size=num_samples, p=probabilities)\n",
    "    return torch.tensor(pd.DataFrame(synthetic_cat).values, dtype=torch.long).to(device)\n",
    "\n",
    "def generate_samples(model, num_samples):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        synthetic_cat = generate_categorical_samples(num_samples)\n",
    "        x_num = torch.randn(num_samples, len(numeric_features)).to(device)\n",
    "        \n",
    "        for t in reversed(range(T_train)):\n",
    "            t_tensor = torch.full((num_samples,), t, device=device)\n",
    "            pred_noise = model(x_num, synthetic_cat, t_tensor)\n",
    "            \n",
    "            beta_t = beta[t].view(1,1)\n",
    "            sqrt_alpha_t = torch.sqrt(1 - beta_t)\n",
    "            x_num = (x_num - beta_t*pred_noise/(torch.sqrt(1 - alpha_hat[t])))/sqrt_alpha_t\n",
    "            if t > 0: x_num += torch.sqrt(beta_t)*torch.randn_like(x_num)\n",
    "        \n",
    "        x_num[:, eng_indices] = torch.clamp(x_num[:, eng_indices], eng_min, eng_max)\n",
    "        return x_num.cpu().numpy(), synthetic_cat.cpu().numpy()\n",
    "\n",
    "def evaluate_model(model, num_samples=1000):\n",
    "    synthetic_num, _ = generate_samples(model, num_samples)\n",
    "    synthetic_num = scaler.inverse_transform(synthetic_num)\n",
    "    real_num = scaler.inverse_transform(X_train_num)\n",
    "    return np.mean([wasserstein_distance(real_num[:,i], synthetic_num[:,i]) \n",
    "                  for i in range(len(numeric_features))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training with {'w1': 0.1, 'w2': 0.5, 'lambda_eng': 0.1, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 0.9036\n",
      "Epoch 2/150 Loss: 0.6474\n",
      "Epoch 3/150 Loss: 0.6172\n",
      "Epoch 4/150 Loss: 0.5815\n",
      "Epoch 5/150 Loss: 0.5585\n",
      "Epoch 6/150 Loss: 0.5363\n",
      "Epoch 7/150 Loss: 0.5137\n",
      "Epoch 8/150 Loss: 0.5042\n",
      "Epoch 9/150 Loss: 0.4933\n",
      "Epoch 10/150 Loss: 0.4834\n",
      "Epoch 11/150 Loss: 0.4656\n",
      "Epoch 12/150 Loss: 0.4582\n",
      "Epoch 13/150 Loss: 0.4530\n",
      "Epoch 14/150 Loss: 0.4473\n",
      "Epoch 15/150 Loss: 0.4315\n",
      "Epoch 16/150 Loss: 0.4224\n",
      "Epoch 17/150 Loss: 0.4281\n",
      "Epoch 18/150 Loss: 0.4238\n",
      "Epoch 19/150 Loss: 0.4088\n",
      "Epoch 20/150 Loss: 0.4079\n",
      "Epoch 21/150 Loss: 0.4027\n",
      "Epoch 22/150 Loss: 0.4010\n",
      "Epoch 23/150 Loss: 0.3837\n",
      "Epoch 24/150 Loss: 0.3951\n",
      "Epoch 25/150 Loss: 0.3783\n",
      "Epoch 26/150 Loss: 0.3846\n",
      "Epoch 27/150 Loss: 0.3710\n",
      "Epoch 28/150 Loss: 0.3655\n",
      "Epoch 29/150 Loss: 0.3797\n",
      "Epoch 30/150 Loss: 0.3694\n",
      "Epoch 31/150 Loss: 0.3676\n",
      "Epoch 32/150 Loss: 0.3560\n",
      "Epoch 33/150 Loss: 0.3547\n",
      "Epoch 34/150 Loss: 0.3574\n",
      "Epoch 35/150 Loss: 0.3600\n",
      "Epoch 36/150 Loss: 0.3509\n",
      "Epoch 37/150 Loss: 0.3477\n",
      "Epoch 38/150 Loss: 0.3488\n",
      "Epoch 39/150 Loss: 0.3487\n",
      "Epoch 40/150 Loss: 0.3341\n",
      "Epoch 41/150 Loss: 0.3466\n",
      "Epoch 42/150 Loss: 0.3393\n",
      "Epoch 43/150 Loss: 0.3412\n",
      "Epoch 44/150 Loss: 0.3323\n",
      "Epoch 45/150 Loss: 0.3368\n",
      "Epoch 46/150 Loss: 0.3378\n",
      "Epoch 47/150 Loss: 0.3330\n",
      "Epoch 48/150 Loss: 0.3270\n",
      "Epoch 49/150 Loss: 0.3264\n",
      "Epoch 50/150 Loss: 0.3217\n",
      "Epoch 51/150 Loss: 0.3173\n",
      "Epoch 52/150 Loss: 0.3155\n",
      "Epoch 53/150 Loss: 0.3171\n",
      "Epoch 54/150 Loss: 0.3165\n",
      "Epoch 55/150 Loss: 0.3091\n",
      "Epoch 56/150 Loss: 0.3089\n",
      "Epoch 57/150 Loss: 0.3097\n",
      "Epoch 58/150 Loss: 0.3083\n",
      "Epoch 59/150 Loss: 0.2979\n",
      "Epoch 60/150 Loss: 0.3107\n",
      "Epoch 61/150 Loss: 0.3028\n",
      "Epoch 62/150 Loss: 0.3085\n",
      "Epoch 63/150 Loss: 0.2981\n",
      "Epoch 64/150 Loss: 0.3075\n",
      "Epoch 65/150 Loss: 0.2993\n",
      "Epoch 66/150 Loss: 0.3051\n",
      "Epoch 67/150 Loss: 0.2949\n",
      "Epoch 68/150 Loss: 0.2956\n",
      "Epoch 69/150 Loss: 0.2936\n",
      "Epoch 70/150 Loss: 0.2940\n",
      "Epoch 71/150 Loss: 0.2944\n",
      "Epoch 72/150 Loss: 0.2901\n",
      "Epoch 73/150 Loss: 0.2872\n",
      "Epoch 74/150 Loss: 0.2906\n",
      "Epoch 75/150 Loss: 0.2882\n",
      "Epoch 76/150 Loss: 0.2864\n",
      "Epoch 77/150 Loss: 0.2831\n",
      "Epoch 78/150 Loss: 0.2813\n",
      "Epoch 79/150 Loss: 0.2845\n",
      "Epoch 80/150 Loss: 0.2787\n",
      "Epoch 81/150 Loss: 0.2772\n",
      "Epoch 82/150 Loss: 0.2732\n",
      "Epoch 83/150 Loss: 0.2712\n",
      "Epoch 84/150 Loss: 0.2729\n",
      "Epoch 85/150 Loss: 0.2787\n",
      "Epoch 86/150 Loss: 0.2722\n",
      "Epoch 87/150 Loss: 0.2711\n",
      "Epoch 88/150 Loss: 0.2753\n",
      "Epoch 89/150 Loss: 0.2694\n",
      "Epoch 90/150 Loss: 0.2728\n",
      "Epoch 91/150 Loss: 0.2625\n",
      "Epoch 92/150 Loss: 0.2694\n",
      "Epoch 93/150 Loss: 0.2632\n",
      "Epoch 94/150 Loss: 0.2676\n",
      "Epoch 95/150 Loss: 0.2608\n",
      "Epoch 96/150 Loss: 0.2604\n",
      "Epoch 97/150 Loss: 0.2630\n",
      "Epoch 98/150 Loss: 0.2577\n",
      "Epoch 99/150 Loss: 0.2651\n",
      "Epoch 100/150 Loss: 0.2618\n",
      "Epoch 101/150 Loss: 0.2540\n",
      "Epoch 102/150 Loss: 0.2583\n",
      "Epoch 103/150 Loss: 0.2579\n",
      "Epoch 104/150 Loss: 0.2479\n",
      "Epoch 105/150 Loss: 0.2596\n",
      "Epoch 106/150 Loss: 0.2557\n",
      "Epoch 107/150 Loss: 0.2600\n",
      "Epoch 108/150 Loss: 0.2569\n",
      "Epoch 109/150 Loss: 0.2493\n",
      "Epoch 110/150 Loss: 0.2539\n",
      "Epoch 111/150 Loss: 0.2449\n",
      "Epoch 112/150 Loss: 0.2443\n",
      "Epoch 113/150 Loss: 0.2517\n",
      "Epoch 114/150 Loss: 0.2495\n",
      "Epoch 115/150 Loss: 0.2517\n",
      "Epoch 116/150 Loss: 0.2402\n",
      "Epoch 117/150 Loss: 0.2405\n",
      "Epoch 118/150 Loss: 0.2468\n",
      "Epoch 119/150 Loss: 0.2467\n",
      "Epoch 120/150 Loss: 0.2481\n",
      "Epoch 121/150 Loss: 0.2434\n",
      "Epoch 122/150 Loss: 0.2376\n",
      "Epoch 123/150 Loss: 0.2455\n",
      "Epoch 124/150 Loss: 0.2376\n",
      "Epoch 125/150 Loss: 0.2363\n",
      "Epoch 126/150 Loss: 0.2372\n",
      "Epoch 127/150 Loss: 0.2425\n",
      "Epoch 128/150 Loss: 0.2364\n",
      "Epoch 129/150 Loss: 0.2357\n",
      "Epoch 130/150 Loss: 0.2404\n",
      "Epoch 131/150 Loss: 0.2383\n",
      "Epoch 132/150 Loss: 0.2370\n",
      "Epoch 133/150 Loss: 0.2339\n",
      "Epoch 134/150 Loss: 0.2307\n",
      "Epoch 135/150 Loss: 0.2255\n",
      "Epoch 136/150 Loss: 0.2315\n",
      "Epoch 137/150 Loss: 0.2357\n",
      "Epoch 138/150 Loss: 0.2301\n",
      "Epoch 139/150 Loss: 0.2332\n",
      "Epoch 140/150 Loss: 0.2287\n",
      "Epoch 141/150 Loss: 0.2351\n",
      "Epoch 142/150 Loss: 0.2311\n",
      "Epoch 143/150 Loss: 0.2351\n",
      "Epoch 144/150 Loss: 0.2319\n",
      "Epoch 145/150 Loss: 0.2347\n",
      "Epoch 146/150 Loss: 0.2297\n",
      "Epoch 147/150 Loss: 0.2268\n",
      "Epoch 148/150 Loss: 0.2317\n",
      "Epoch 149/150 Loss: 0.2336\n",
      "Epoch 150/150 Loss: 0.2315\n",
      "Config {'w1': 0.1, 'w2': 0.5, 'lambda_eng': 0.1, 'lr': 0.001} achieved Wasserstein: 0.5978\n",
      "\n",
      "=== Training with {'w1': 0.1, 'w2': 0.5, 'lambda_eng': 0.15, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.0147\n",
      "Epoch 2/150 Loss: 0.7054\n",
      "Epoch 3/150 Loss: 0.6597\n",
      "Epoch 4/150 Loss: 0.6203\n",
      "Epoch 5/150 Loss: 0.5824\n",
      "Epoch 6/150 Loss: 0.5674\n",
      "Epoch 7/150 Loss: 0.5453\n",
      "Epoch 8/150 Loss: 0.5297\n",
      "Epoch 9/150 Loss: 0.5242\n",
      "Epoch 10/150 Loss: 0.5042\n",
      "Epoch 11/150 Loss: 0.4921\n",
      "Epoch 12/150 Loss: 0.4691\n",
      "Epoch 13/150 Loss: 0.4695\n",
      "Epoch 14/150 Loss: 0.4659\n",
      "Epoch 15/150 Loss: 0.4713\n",
      "Epoch 16/150 Loss: 0.4476\n",
      "Epoch 17/150 Loss: 0.4491\n",
      "Epoch 18/150 Loss: 0.4449\n",
      "Epoch 19/150 Loss: 0.4361\n",
      "Epoch 20/150 Loss: 0.4281\n",
      "Epoch 21/150 Loss: 0.4183\n",
      "Epoch 22/150 Loss: 0.4182\n",
      "Epoch 23/150 Loss: 0.4115\n",
      "Epoch 24/150 Loss: 0.4146\n",
      "Epoch 25/150 Loss: 0.4001\n",
      "Epoch 26/150 Loss: 0.3969\n",
      "Epoch 27/150 Loss: 0.3982\n",
      "Epoch 28/150 Loss: 0.3896\n",
      "Epoch 29/150 Loss: 0.3869\n",
      "Epoch 30/150 Loss: 0.3761\n",
      "Epoch 31/150 Loss: 0.3877\n",
      "Epoch 32/150 Loss: 0.3775\n",
      "Epoch 33/150 Loss: 0.3698\n",
      "Epoch 34/150 Loss: 0.3644\n",
      "Epoch 35/150 Loss: 0.3547\n",
      "Epoch 36/150 Loss: 0.3658\n",
      "Epoch 37/150 Loss: 0.3591\n",
      "Epoch 38/150 Loss: 0.3532\n",
      "Epoch 39/150 Loss: 0.3522\n",
      "Epoch 40/150 Loss: 0.3507\n",
      "Epoch 41/150 Loss: 0.3517\n",
      "Epoch 42/150 Loss: 0.3469\n",
      "Epoch 43/150 Loss: 0.3413\n",
      "Epoch 44/150 Loss: 0.3455\n",
      "Epoch 45/150 Loss: 0.3375\n",
      "Epoch 46/150 Loss: 0.3332\n",
      "Epoch 47/150 Loss: 0.3369\n",
      "Epoch 48/150 Loss: 0.3345\n",
      "Epoch 49/150 Loss: 0.3347\n",
      "Epoch 50/150 Loss: 0.3275\n",
      "Epoch 51/150 Loss: 0.3265\n",
      "Epoch 52/150 Loss: 0.3255\n",
      "Epoch 53/150 Loss: 0.3192\n",
      "Epoch 54/150 Loss: 0.3205\n",
      "Epoch 55/150 Loss: 0.3170\n",
      "Epoch 56/150 Loss: 0.3203\n",
      "Epoch 57/150 Loss: 0.3197\n",
      "Epoch 58/150 Loss: 0.3116\n",
      "Epoch 59/150 Loss: 0.3085\n",
      "Epoch 60/150 Loss: 0.3114\n",
      "Epoch 61/150 Loss: 0.3064\n",
      "Epoch 62/150 Loss: 0.3055\n",
      "Epoch 63/150 Loss: 0.3076\n",
      "Epoch 64/150 Loss: 0.3074\n",
      "Epoch 65/150 Loss: 0.2909\n",
      "Epoch 66/150 Loss: 0.2968\n",
      "Epoch 67/150 Loss: 0.3012\n",
      "Epoch 68/150 Loss: 0.2962\n",
      "Epoch 69/150 Loss: 0.3005\n",
      "Epoch 70/150 Loss: 0.2918\n",
      "Epoch 71/150 Loss: 0.2907\n",
      "Epoch 72/150 Loss: 0.2946\n",
      "Epoch 73/150 Loss: 0.2886\n",
      "Epoch 74/150 Loss: 0.2933\n",
      "Epoch 75/150 Loss: 0.2895\n",
      "Epoch 76/150 Loss: 0.2932\n",
      "Epoch 77/150 Loss: 0.2829\n",
      "Epoch 78/150 Loss: 0.2793\n",
      "Epoch 79/150 Loss: 0.2911\n",
      "Epoch 80/150 Loss: 0.2891\n",
      "Epoch 81/150 Loss: 0.2823\n",
      "Epoch 82/150 Loss: 0.2869\n",
      "Epoch 83/150 Loss: 0.2863\n",
      "Epoch 84/150 Loss: 0.2789\n",
      "Epoch 85/150 Loss: 0.2860\n",
      "Epoch 86/150 Loss: 0.2796\n",
      "Epoch 87/150 Loss: 0.2809\n",
      "Epoch 88/150 Loss: 0.2732\n",
      "Epoch 89/150 Loss: 0.2774\n",
      "Epoch 90/150 Loss: 0.2770\n",
      "Epoch 91/150 Loss: 0.2775\n",
      "Epoch 92/150 Loss: 0.2747\n",
      "Epoch 93/150 Loss: 0.2699\n",
      "Epoch 94/150 Loss: 0.2751\n",
      "Epoch 95/150 Loss: 0.2668\n",
      "Epoch 96/150 Loss: 0.2652\n",
      "Epoch 97/150 Loss: 0.2696\n",
      "Epoch 98/150 Loss: 0.2643\n",
      "Epoch 99/150 Loss: 0.2714\n",
      "Epoch 100/150 Loss: 0.2667\n",
      "Epoch 101/150 Loss: 0.2670\n",
      "Epoch 102/150 Loss: 0.2686\n",
      "Epoch 103/150 Loss: 0.2620\n",
      "Epoch 104/150 Loss: 0.2647\n",
      "Epoch 105/150 Loss: 0.2596\n",
      "Epoch 106/150 Loss: 0.2632\n",
      "Epoch 107/150 Loss: 0.2593\n",
      "Epoch 108/150 Loss: 0.2586\n",
      "Epoch 109/150 Loss: 0.2528\n",
      "Epoch 110/150 Loss: 0.2558\n",
      "Epoch 111/150 Loss: 0.2541\n",
      "Epoch 112/150 Loss: 0.2518\n",
      "Epoch 113/150 Loss: 0.2613\n",
      "Epoch 114/150 Loss: 0.2626\n",
      "Epoch 115/150 Loss: 0.2497\n",
      "Epoch 116/150 Loss: 0.2485\n",
      "Epoch 117/150 Loss: 0.2453\n",
      "Epoch 118/150 Loss: 0.2468\n",
      "Epoch 119/150 Loss: 0.2456\n",
      "Epoch 120/150 Loss: 0.2479\n",
      "Epoch 121/150 Loss: 0.2437\n",
      "Epoch 122/150 Loss: 0.2389\n",
      "Epoch 123/150 Loss: 0.2502\n",
      "Epoch 124/150 Loss: 0.2448\n",
      "Epoch 125/150 Loss: 0.2379\n",
      "Epoch 126/150 Loss: 0.2431\n",
      "Epoch 127/150 Loss: 0.2399\n",
      "Epoch 128/150 Loss: 0.2476\n",
      "Epoch 129/150 Loss: 0.2421\n",
      "Epoch 130/150 Loss: 0.2467\n",
      "Epoch 131/150 Loss: 0.2398\n",
      "Epoch 132/150 Loss: 0.2388\n",
      "Epoch 133/150 Loss: 0.2393\n",
      "Epoch 134/150 Loss: 0.2408\n",
      "Epoch 135/150 Loss: 0.2351\n",
      "Epoch 136/150 Loss: 0.2318\n",
      "Epoch 137/150 Loss: 0.2356\n",
      "Epoch 138/150 Loss: 0.2403\n",
      "Epoch 139/150 Loss: 0.2310\n",
      "Epoch 140/150 Loss: 0.2390\n",
      "Epoch 141/150 Loss: 0.2342\n",
      "Epoch 142/150 Loss: 0.2387\n",
      "Epoch 143/150 Loss: 0.2357\n",
      "Epoch 144/150 Loss: 0.2340\n",
      "Epoch 145/150 Loss: 0.2344\n",
      "Epoch 146/150 Loss: 0.2354\n",
      "Epoch 147/150 Loss: 0.2360\n",
      "Epoch 148/150 Loss: 0.2353\n",
      "Epoch 149/150 Loss: 0.2358\n",
      "Epoch 150/150 Loss: 0.2329\n",
      "Config {'w1': 0.1, 'w2': 0.5, 'lambda_eng': 0.15, 'lr': 0.001} achieved Wasserstein: 0.8793\n",
      "\n",
      "=== Training with {'w1': 0.1, 'w2': 0.5, 'lambda_eng': 0.2, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.0845\n",
      "Epoch 2/150 Loss: 0.7459\n",
      "Epoch 3/150 Loss: 0.6871\n",
      "Epoch 4/150 Loss: 0.6596\n",
      "Epoch 5/150 Loss: 0.6450\n",
      "Epoch 6/150 Loss: 0.5882\n",
      "Epoch 7/150 Loss: 0.5843\n",
      "Epoch 8/150 Loss: 0.5647\n",
      "Epoch 9/150 Loss: 0.5428\n",
      "Epoch 10/150 Loss: 0.5289\n",
      "Epoch 11/150 Loss: 0.5205\n",
      "Epoch 12/150 Loss: 0.5027\n",
      "Epoch 13/150 Loss: 0.5048\n",
      "Epoch 14/150 Loss: 0.4802\n",
      "Epoch 15/150 Loss: 0.4780\n",
      "Epoch 16/150 Loss: 0.4748\n",
      "Epoch 17/150 Loss: 0.4623\n",
      "Epoch 18/150 Loss: 0.4574\n",
      "Epoch 19/150 Loss: 0.4577\n",
      "Epoch 20/150 Loss: 0.4378\n",
      "Epoch 21/150 Loss: 0.4376\n",
      "Epoch 22/150 Loss: 0.4280\n",
      "Epoch 23/150 Loss: 0.4220\n",
      "Epoch 24/150 Loss: 0.4165\n",
      "Epoch 25/150 Loss: 0.4115\n",
      "Epoch 26/150 Loss: 0.4065\n",
      "Epoch 27/150 Loss: 0.4027\n",
      "Epoch 28/150 Loss: 0.4030\n",
      "Epoch 29/150 Loss: 0.3952\n",
      "Epoch 30/150 Loss: 0.3853\n",
      "Epoch 31/150 Loss: 0.3954\n",
      "Epoch 32/150 Loss: 0.3934\n",
      "Epoch 33/150 Loss: 0.3676\n",
      "Epoch 34/150 Loss: 0.3779\n",
      "Epoch 35/150 Loss: 0.3795\n",
      "Epoch 36/150 Loss: 0.3669\n",
      "Epoch 37/150 Loss: 0.3609\n",
      "Epoch 38/150 Loss: 0.3631\n",
      "Epoch 39/150 Loss: 0.3568\n",
      "Epoch 40/150 Loss: 0.3537\n",
      "Epoch 41/150 Loss: 0.3444\n",
      "Epoch 42/150 Loss: 0.3475\n",
      "Epoch 43/150 Loss: 0.3429\n",
      "Epoch 44/150 Loss: 0.3502\n",
      "Epoch 45/150 Loss: 0.3382\n",
      "Epoch 46/150 Loss: 0.3252\n",
      "Epoch 47/150 Loss: 0.3433\n",
      "Epoch 48/150 Loss: 0.3424\n",
      "Epoch 49/150 Loss: 0.3347\n",
      "Epoch 50/150 Loss: 0.3288\n",
      "Epoch 51/150 Loss: 0.3384\n",
      "Epoch 52/150 Loss: 0.3254\n",
      "Epoch 53/150 Loss: 0.3338\n",
      "Epoch 54/150 Loss: 0.3224\n",
      "Epoch 55/150 Loss: 0.3185\n",
      "Epoch 56/150 Loss: 0.3266\n",
      "Epoch 57/150 Loss: 0.3317\n",
      "Epoch 58/150 Loss: 0.3198\n",
      "Epoch 59/150 Loss: 0.3212\n",
      "Epoch 60/150 Loss: 0.3225\n",
      "Epoch 61/150 Loss: 0.3277\n",
      "Epoch 62/150 Loss: 0.3168\n",
      "Epoch 63/150 Loss: 0.3109\n",
      "Epoch 64/150 Loss: 0.3092\n",
      "Epoch 65/150 Loss: 0.3101\n",
      "Epoch 66/150 Loss: 0.3086\n",
      "Epoch 67/150 Loss: 0.3074\n",
      "Epoch 68/150 Loss: 0.3129\n",
      "Epoch 69/150 Loss: 0.3148\n",
      "Epoch 70/150 Loss: 0.3061\n",
      "Epoch 71/150 Loss: 0.2996\n",
      "Epoch 72/150 Loss: 0.2964\n",
      "Epoch 73/150 Loss: 0.2973\n",
      "Epoch 74/150 Loss: 0.3047\n",
      "Epoch 75/150 Loss: 0.3002\n",
      "Epoch 76/150 Loss: 0.2989\n",
      "Epoch 77/150 Loss: 0.2918\n",
      "Epoch 78/150 Loss: 0.2985\n",
      "Epoch 79/150 Loss: 0.2907\n",
      "Epoch 80/150 Loss: 0.2854\n",
      "Epoch 81/150 Loss: 0.3023\n",
      "Epoch 82/150 Loss: 0.2873\n",
      "Epoch 83/150 Loss: 0.2923\n",
      "Epoch 84/150 Loss: 0.2776\n",
      "Epoch 85/150 Loss: 0.2880\n",
      "Epoch 86/150 Loss: 0.2823\n",
      "Epoch 87/150 Loss: 0.2842\n",
      "Epoch 88/150 Loss: 0.2874\n",
      "Epoch 89/150 Loss: 0.2840\n",
      "Epoch 90/150 Loss: 0.2777\n",
      "Epoch 91/150 Loss: 0.2847\n",
      "Epoch 92/150 Loss: 0.2828\n",
      "Epoch 93/150 Loss: 0.2818\n",
      "Epoch 94/150 Loss: 0.2756\n",
      "Epoch 95/150 Loss: 0.2732\n",
      "Epoch 96/150 Loss: 0.2751\n",
      "Epoch 97/150 Loss: 0.2695\n",
      "Epoch 98/150 Loss: 0.2817\n",
      "Epoch 99/150 Loss: 0.2722\n",
      "Epoch 100/150 Loss: 0.2696\n",
      "Epoch 101/150 Loss: 0.2727\n",
      "Epoch 102/150 Loss: 0.2701\n",
      "Epoch 103/150 Loss: 0.2674\n",
      "Epoch 104/150 Loss: 0.2699\n",
      "Epoch 105/150 Loss: 0.2654\n",
      "Epoch 106/150 Loss: 0.2615\n",
      "Epoch 107/150 Loss: 0.2553\n",
      "Epoch 108/150 Loss: 0.2655\n",
      "Epoch 109/150 Loss: 0.2619\n",
      "Epoch 110/150 Loss: 0.2651\n",
      "Epoch 111/150 Loss: 0.2620\n",
      "Epoch 112/150 Loss: 0.2617\n",
      "Epoch 113/150 Loss: 0.2564\n",
      "Epoch 114/150 Loss: 0.2520\n",
      "Epoch 115/150 Loss: 0.2546\n",
      "Epoch 116/150 Loss: 0.2535\n",
      "Epoch 117/150 Loss: 0.2553\n",
      "Epoch 118/150 Loss: 0.2618\n",
      "Epoch 119/150 Loss: 0.2526\n",
      "Epoch 120/150 Loss: 0.2522\n",
      "Epoch 121/150 Loss: 0.2550\n",
      "Epoch 122/150 Loss: 0.2542\n",
      "Epoch 123/150 Loss: 0.2515\n",
      "Epoch 124/150 Loss: 0.2501\n",
      "Epoch 125/150 Loss: 0.2446\n",
      "Epoch 126/150 Loss: 0.2467\n",
      "Epoch 127/150 Loss: 0.2465\n",
      "Epoch 128/150 Loss: 0.2434\n",
      "Epoch 129/150 Loss: 0.2505\n",
      "Epoch 130/150 Loss: 0.2467\n",
      "Epoch 131/150 Loss: 0.2458\n",
      "Epoch 132/150 Loss: 0.2471\n",
      "Epoch 133/150 Loss: 0.2510\n",
      "Epoch 134/150 Loss: 0.2383\n",
      "Epoch 135/150 Loss: 0.2468\n",
      "Epoch 136/150 Loss: 0.2437\n",
      "Epoch 137/150 Loss: 0.2348\n",
      "Epoch 138/150 Loss: 0.2374\n",
      "Epoch 139/150 Loss: 0.2361\n",
      "Epoch 140/150 Loss: 0.2475\n",
      "Epoch 141/150 Loss: 0.2416\n",
      "Epoch 142/150 Loss: 0.2391\n",
      "Epoch 143/150 Loss: 0.2367\n",
      "Epoch 144/150 Loss: 0.2426\n",
      "Epoch 145/150 Loss: 0.2437\n",
      "Epoch 146/150 Loss: 0.2456\n",
      "Epoch 147/150 Loss: 0.2327\n",
      "Epoch 148/150 Loss: 0.2429\n",
      "Epoch 149/150 Loss: 0.2325\n",
      "Epoch 150/150 Loss: 0.2321\n",
      "Config {'w1': 0.1, 'w2': 0.5, 'lambda_eng': 0.2, 'lr': 0.001} achieved Wasserstein: 0.9748\n",
      "\n",
      "=== Training with {'w1': 0.1, 'w2': 0.55, 'lambda_eng': 0.1, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 0.9817\n",
      "Epoch 2/150 Loss: 0.6994\n",
      "Epoch 3/150 Loss: 0.6804\n",
      "Epoch 4/150 Loss: 0.6582\n",
      "Epoch 5/150 Loss: 0.6357\n",
      "Epoch 6/150 Loss: 0.5949\n",
      "Epoch 7/150 Loss: 0.5743\n",
      "Epoch 8/150 Loss: 0.5629\n",
      "Epoch 9/150 Loss: 0.5363\n",
      "Epoch 10/150 Loss: 0.5275\n",
      "Epoch 11/150 Loss: 0.5087\n",
      "Epoch 12/150 Loss: 0.4957\n",
      "Epoch 13/150 Loss: 0.4986\n",
      "Epoch 14/150 Loss: 0.4814\n",
      "Epoch 15/150 Loss: 0.4700\n",
      "Epoch 16/150 Loss: 0.4728\n",
      "Epoch 17/150 Loss: 0.4527\n",
      "Epoch 18/150 Loss: 0.4484\n",
      "Epoch 19/150 Loss: 0.4490\n",
      "Epoch 20/150 Loss: 0.4334\n",
      "Epoch 21/150 Loss: 0.4381\n",
      "Epoch 22/150 Loss: 0.4350\n",
      "Epoch 23/150 Loss: 0.4270\n",
      "Epoch 24/150 Loss: 0.4196\n",
      "Epoch 25/150 Loss: 0.4150\n",
      "Epoch 26/150 Loss: 0.4022\n",
      "Epoch 27/150 Loss: 0.4090\n",
      "Epoch 28/150 Loss: 0.4017\n",
      "Epoch 29/150 Loss: 0.3990\n",
      "Epoch 30/150 Loss: 0.3952\n",
      "Epoch 31/150 Loss: 0.3925\n",
      "Epoch 32/150 Loss: 0.3862\n",
      "Epoch 33/150 Loss: 0.3919\n",
      "Epoch 34/150 Loss: 0.3736\n",
      "Epoch 35/150 Loss: 0.3754\n",
      "Epoch 36/150 Loss: 0.3807\n",
      "Epoch 37/150 Loss: 0.3641\n",
      "Epoch 38/150 Loss: 0.3667\n",
      "Epoch 39/150 Loss: 0.3682\n",
      "Epoch 40/150 Loss: 0.3623\n",
      "Epoch 41/150 Loss: 0.3674\n",
      "Epoch 42/150 Loss: 0.3445\n",
      "Epoch 43/150 Loss: 0.3606\n",
      "Epoch 44/150 Loss: 0.3453\n",
      "Epoch 45/150 Loss: 0.3544\n",
      "Epoch 46/150 Loss: 0.3378\n",
      "Epoch 47/150 Loss: 0.3406\n",
      "Epoch 48/150 Loss: 0.3431\n",
      "Epoch 49/150 Loss: 0.3419\n",
      "Epoch 50/150 Loss: 0.3341\n",
      "Epoch 51/150 Loss: 0.3342\n",
      "Epoch 52/150 Loss: 0.3371\n",
      "Epoch 53/150 Loss: 0.3261\n",
      "Epoch 54/150 Loss: 0.3326\n",
      "Epoch 55/150 Loss: 0.3211\n",
      "Epoch 56/150 Loss: 0.3149\n",
      "Epoch 57/150 Loss: 0.3194\n",
      "Epoch 58/150 Loss: 0.3280\n",
      "Epoch 59/150 Loss: 0.3122\n",
      "Epoch 60/150 Loss: 0.3151\n",
      "Epoch 61/150 Loss: 0.3183\n",
      "Epoch 62/150 Loss: 0.3138\n",
      "Epoch 63/150 Loss: 0.3037\n",
      "Epoch 64/150 Loss: 0.3065\n",
      "Epoch 65/150 Loss: 0.3120\n",
      "Epoch 66/150 Loss: 0.3049\n",
      "Epoch 67/150 Loss: 0.3058\n",
      "Epoch 68/150 Loss: 0.3054\n",
      "Epoch 69/150 Loss: 0.3012\n",
      "Epoch 70/150 Loss: 0.3087\n",
      "Epoch 71/150 Loss: 0.3059\n",
      "Epoch 72/150 Loss: 0.2980\n",
      "Epoch 73/150 Loss: 0.2999\n",
      "Epoch 74/150 Loss: 0.3046\n",
      "Epoch 75/150 Loss: 0.2964\n",
      "Epoch 76/150 Loss: 0.2915\n",
      "Epoch 77/150 Loss: 0.2880\n",
      "Epoch 78/150 Loss: 0.2853\n",
      "Epoch 79/150 Loss: 0.2871\n",
      "Epoch 80/150 Loss: 0.2873\n",
      "Epoch 81/150 Loss: 0.2915\n",
      "Epoch 82/150 Loss: 0.2801\n",
      "Epoch 83/150 Loss: 0.2892\n",
      "Epoch 84/150 Loss: 0.2838\n",
      "Epoch 85/150 Loss: 0.2763\n",
      "Epoch 86/150 Loss: 0.2808\n",
      "Epoch 87/150 Loss: 0.2768\n",
      "Epoch 88/150 Loss: 0.2776\n",
      "Epoch 89/150 Loss: 0.2778\n",
      "Epoch 90/150 Loss: 0.2731\n",
      "Epoch 91/150 Loss: 0.2702\n",
      "Epoch 92/150 Loss: 0.2772\n",
      "Epoch 93/150 Loss: 0.2761\n",
      "Epoch 94/150 Loss: 0.2706\n",
      "Epoch 95/150 Loss: 0.2687\n",
      "Epoch 96/150 Loss: 0.2682\n",
      "Epoch 97/150 Loss: 0.2698\n",
      "Epoch 98/150 Loss: 0.2709\n",
      "Epoch 99/150 Loss: 0.2682\n",
      "Epoch 100/150 Loss: 0.2630\n",
      "Epoch 101/150 Loss: 0.2656\n",
      "Epoch 102/150 Loss: 0.2604\n",
      "Epoch 103/150 Loss: 0.2706\n",
      "Epoch 104/150 Loss: 0.2620\n",
      "Epoch 105/150 Loss: 0.2617\n",
      "Epoch 106/150 Loss: 0.2599\n",
      "Epoch 107/150 Loss: 0.2577\n",
      "Epoch 108/150 Loss: 0.2625\n",
      "Epoch 109/150 Loss: 0.2560\n",
      "Epoch 110/150 Loss: 0.2597\n",
      "Epoch 111/150 Loss: 0.2523\n",
      "Epoch 112/150 Loss: 0.2557\n",
      "Epoch 113/150 Loss: 0.2557\n",
      "Epoch 114/150 Loss: 0.2578\n",
      "Epoch 115/150 Loss: 0.2588\n",
      "Epoch 116/150 Loss: 0.2483\n",
      "Epoch 117/150 Loss: 0.2533\n",
      "Epoch 118/150 Loss: 0.2512\n",
      "Epoch 119/150 Loss: 0.2458\n",
      "Epoch 120/150 Loss: 0.2546\n",
      "Epoch 121/150 Loss: 0.2481\n",
      "Epoch 122/150 Loss: 0.2437\n",
      "Epoch 123/150 Loss: 0.2409\n",
      "Epoch 124/150 Loss: 0.2444\n",
      "Epoch 125/150 Loss: 0.2459\n",
      "Epoch 126/150 Loss: 0.2374\n",
      "Epoch 127/150 Loss: 0.2426\n",
      "Epoch 128/150 Loss: 0.2338\n",
      "Epoch 129/150 Loss: 0.2399\n",
      "Epoch 130/150 Loss: 0.2401\n",
      "Epoch 131/150 Loss: 0.2448\n",
      "Epoch 132/150 Loss: 0.2316\n",
      "Epoch 133/150 Loss: 0.2396\n",
      "Epoch 134/150 Loss: 0.2410\n",
      "Epoch 135/150 Loss: 0.2475\n",
      "Epoch 136/150 Loss: 0.2391\n",
      "Epoch 137/150 Loss: 0.2382\n",
      "Epoch 138/150 Loss: 0.2326\n",
      "Epoch 139/150 Loss: 0.2364\n",
      "Epoch 140/150 Loss: 0.2310\n",
      "Epoch 141/150 Loss: 0.2317\n",
      "Epoch 142/150 Loss: 0.2316\n",
      "Epoch 143/150 Loss: 0.2325\n",
      "Epoch 144/150 Loss: 0.2319\n",
      "Epoch 145/150 Loss: 0.2383\n",
      "Epoch 146/150 Loss: 0.2340\n",
      "Epoch 147/150 Loss: 0.2366\n",
      "Epoch 148/150 Loss: 0.2313\n",
      "Epoch 149/150 Loss: 0.2334\n",
      "Epoch 150/150 Loss: 0.2344\n",
      "Config {'w1': 0.1, 'w2': 0.55, 'lambda_eng': 0.1, 'lr': 0.001} achieved Wasserstein: 0.9334\n",
      "\n",
      "=== Training with {'w1': 0.1, 'w2': 0.55, 'lambda_eng': 0.15, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 0.9923\n",
      "Epoch 2/150 Loss: 0.6858\n",
      "Epoch 3/150 Loss: 0.6412\n",
      "Epoch 4/150 Loss: 0.6131\n",
      "Epoch 5/150 Loss: 0.5804\n",
      "Epoch 6/150 Loss: 0.5519\n",
      "Epoch 7/150 Loss: 0.5408\n",
      "Epoch 8/150 Loss: 0.5310\n",
      "Epoch 9/150 Loss: 0.5184\n",
      "Epoch 10/150 Loss: 0.4978\n",
      "Epoch 11/150 Loss: 0.4943\n",
      "Epoch 12/150 Loss: 0.4886\n",
      "Epoch 13/150 Loss: 0.4746\n",
      "Epoch 14/150 Loss: 0.4737\n",
      "Epoch 15/150 Loss: 0.4664\n",
      "Epoch 16/150 Loss: 0.4405\n",
      "Epoch 17/150 Loss: 0.4485\n",
      "Epoch 18/150 Loss: 0.4367\n",
      "Epoch 19/150 Loss: 0.4342\n",
      "Epoch 20/150 Loss: 0.4243\n",
      "Epoch 21/150 Loss: 0.4282\n",
      "Epoch 22/150 Loss: 0.4210\n",
      "Epoch 23/150 Loss: 0.4071\n",
      "Epoch 24/150 Loss: 0.4007\n",
      "Epoch 25/150 Loss: 0.3996\n",
      "Epoch 26/150 Loss: 0.3927\n",
      "Epoch 27/150 Loss: 0.3931\n",
      "Epoch 28/150 Loss: 0.3884\n",
      "Epoch 29/150 Loss: 0.3869\n",
      "Epoch 30/150 Loss: 0.3763\n",
      "Epoch 31/150 Loss: 0.3791\n",
      "Epoch 32/150 Loss: 0.3826\n",
      "Epoch 33/150 Loss: 0.3712\n",
      "Epoch 34/150 Loss: 0.3651\n",
      "Epoch 35/150 Loss: 0.3680\n",
      "Epoch 36/150 Loss: 0.3688\n",
      "Epoch 37/150 Loss: 0.3550\n",
      "Epoch 38/150 Loss: 0.3598\n",
      "Epoch 39/150 Loss: 0.3556\n",
      "Epoch 40/150 Loss: 0.3532\n",
      "Epoch 41/150 Loss: 0.3563\n",
      "Epoch 42/150 Loss: 0.3501\n",
      "Epoch 43/150 Loss: 0.3518\n",
      "Epoch 44/150 Loss: 0.3382\n",
      "Epoch 45/150 Loss: 0.3515\n",
      "Epoch 46/150 Loss: 0.3354\n",
      "Epoch 47/150 Loss: 0.3461\n",
      "Epoch 48/150 Loss: 0.3338\n",
      "Epoch 49/150 Loss: 0.3402\n",
      "Epoch 50/150 Loss: 0.3325\n",
      "Epoch 51/150 Loss: 0.3286\n",
      "Epoch 52/150 Loss: 0.3198\n",
      "Epoch 53/150 Loss: 0.3283\n",
      "Epoch 54/150 Loss: 0.3321\n",
      "Epoch 55/150 Loss: 0.3284\n",
      "Epoch 56/150 Loss: 0.3163\n",
      "Epoch 57/150 Loss: 0.3197\n",
      "Epoch 58/150 Loss: 0.3205\n",
      "Epoch 59/150 Loss: 0.3182\n",
      "Epoch 60/150 Loss: 0.3149\n",
      "Epoch 61/150 Loss: 0.3186\n",
      "Epoch 62/150 Loss: 0.3103\n",
      "Epoch 63/150 Loss: 0.3138\n",
      "Epoch 64/150 Loss: 0.3062\n",
      "Epoch 65/150 Loss: 0.3133\n",
      "Epoch 66/150 Loss: 0.3085\n",
      "Epoch 67/150 Loss: 0.3079\n",
      "Epoch 68/150 Loss: 0.3078\n",
      "Epoch 69/150 Loss: 0.3011\n",
      "Epoch 70/150 Loss: 0.3046\n",
      "Epoch 71/150 Loss: 0.3041\n",
      "Epoch 72/150 Loss: 0.3056\n",
      "Epoch 73/150 Loss: 0.2943\n",
      "Epoch 74/150 Loss: 0.2951\n",
      "Epoch 75/150 Loss: 0.2988\n",
      "Epoch 76/150 Loss: 0.2945\n",
      "Epoch 77/150 Loss: 0.3012\n",
      "Epoch 78/150 Loss: 0.2901\n",
      "Epoch 79/150 Loss: 0.2891\n",
      "Epoch 80/150 Loss: 0.2866\n",
      "Epoch 81/150 Loss: 0.2845\n",
      "Epoch 82/150 Loss: 0.2903\n",
      "Epoch 83/150 Loss: 0.2856\n",
      "Epoch 84/150 Loss: 0.2838\n",
      "Epoch 85/150 Loss: 0.2851\n",
      "Epoch 86/150 Loss: 0.2902\n",
      "Epoch 87/150 Loss: 0.2851\n",
      "Epoch 88/150 Loss: 0.2762\n",
      "Epoch 89/150 Loss: 0.2857\n",
      "Epoch 90/150 Loss: 0.2781\n",
      "Epoch 91/150 Loss: 0.2651\n",
      "Epoch 92/150 Loss: 0.2747\n",
      "Epoch 93/150 Loss: 0.2682\n",
      "Epoch 94/150 Loss: 0.2691\n",
      "Epoch 95/150 Loss: 0.2741\n",
      "Epoch 96/150 Loss: 0.2629\n",
      "Epoch 97/150 Loss: 0.2733\n",
      "Epoch 98/150 Loss: 0.2651\n",
      "Epoch 99/150 Loss: 0.2691\n",
      "Epoch 100/150 Loss: 0.2664\n",
      "Epoch 101/150 Loss: 0.2587\n",
      "Epoch 102/150 Loss: 0.2636\n",
      "Epoch 103/150 Loss: 0.2657\n",
      "Epoch 104/150 Loss: 0.2551\n",
      "Epoch 105/150 Loss: 0.2542\n",
      "Epoch 106/150 Loss: 0.2595\n",
      "Epoch 107/150 Loss: 0.2587\n",
      "Epoch 108/150 Loss: 0.2601\n",
      "Epoch 109/150 Loss: 0.2510\n",
      "Epoch 110/150 Loss: 0.2520\n",
      "Epoch 111/150 Loss: 0.2539\n",
      "Epoch 112/150 Loss: 0.2488\n",
      "Epoch 113/150 Loss: 0.2585\n",
      "Epoch 114/150 Loss: 0.2549\n",
      "Epoch 115/150 Loss: 0.2475\n",
      "Epoch 116/150 Loss: 0.2477\n",
      "Epoch 117/150 Loss: 0.2530\n",
      "Epoch 118/150 Loss: 0.2579\n",
      "Epoch 119/150 Loss: 0.2446\n",
      "Epoch 120/150 Loss: 0.2395\n",
      "Epoch 121/150 Loss: 0.2454\n",
      "Epoch 122/150 Loss: 0.2471\n",
      "Epoch 123/150 Loss: 0.2485\n",
      "Epoch 124/150 Loss: 0.2406\n",
      "Epoch 125/150 Loss: 0.2481\n",
      "Epoch 126/150 Loss: 0.2488\n",
      "Epoch 127/150 Loss: 0.2385\n",
      "Epoch 128/150 Loss: 0.2458\n",
      "Epoch 129/150 Loss: 0.2408\n",
      "Epoch 130/150 Loss: 0.2380\n",
      "Epoch 131/150 Loss: 0.2381\n",
      "Epoch 132/150 Loss: 0.2341\n",
      "Epoch 133/150 Loss: 0.2376\n",
      "Epoch 134/150 Loss: 0.2452\n",
      "Epoch 135/150 Loss: 0.2383\n",
      "Epoch 136/150 Loss: 0.2380\n",
      "Epoch 137/150 Loss: 0.2390\n",
      "Epoch 138/150 Loss: 0.2389\n",
      "Epoch 139/150 Loss: 0.2374\n",
      "Epoch 140/150 Loss: 0.2370\n",
      "Epoch 141/150 Loss: 0.2329\n",
      "Epoch 142/150 Loss: 0.2425\n",
      "Epoch 143/150 Loss: 0.2307\n",
      "Epoch 144/150 Loss: 0.2289\n",
      "Epoch 145/150 Loss: 0.2390\n",
      "Epoch 146/150 Loss: 0.2340\n",
      "Epoch 147/150 Loss: 0.2372\n",
      "Epoch 148/150 Loss: 0.2318\n",
      "Epoch 149/150 Loss: 0.2300\n",
      "Epoch 150/150 Loss: 0.2326\n",
      "Config {'w1': 0.1, 'w2': 0.55, 'lambda_eng': 0.15, 'lr': 0.001} achieved Wasserstein: 0.7290\n",
      "\n",
      "=== Training with {'w1': 0.1, 'w2': 0.55, 'lambda_eng': 0.2, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.1303\n",
      "Epoch 2/150 Loss: 0.7667\n",
      "Epoch 3/150 Loss: 0.7136\n",
      "Epoch 4/150 Loss: 0.6731\n",
      "Epoch 5/150 Loss: 0.6332\n",
      "Epoch 6/150 Loss: 0.6195\n",
      "Epoch 7/150 Loss: 0.5889\n",
      "Epoch 8/150 Loss: 0.5618\n",
      "Epoch 9/150 Loss: 0.5390\n",
      "Epoch 10/150 Loss: 0.5390\n",
      "Epoch 11/150 Loss: 0.5234\n",
      "Epoch 12/150 Loss: 0.5096\n",
      "Epoch 13/150 Loss: 0.4992\n",
      "Epoch 14/150 Loss: 0.4874\n",
      "Epoch 15/150 Loss: 0.4856\n",
      "Epoch 16/150 Loss: 0.4790\n",
      "Epoch 17/150 Loss: 0.4615\n",
      "Epoch 18/150 Loss: 0.4570\n",
      "Epoch 19/150 Loss: 0.4553\n",
      "Epoch 20/150 Loss: 0.4462\n",
      "Epoch 21/150 Loss: 0.4308\n",
      "Epoch 22/150 Loss: 0.4456\n",
      "Epoch 23/150 Loss: 0.4309\n",
      "Epoch 24/150 Loss: 0.4250\n",
      "Epoch 25/150 Loss: 0.4206\n",
      "Epoch 26/150 Loss: 0.4098\n",
      "Epoch 27/150 Loss: 0.4195\n",
      "Epoch 28/150 Loss: 0.4069\n",
      "Epoch 29/150 Loss: 0.3963\n",
      "Epoch 30/150 Loss: 0.3902\n",
      "Epoch 31/150 Loss: 0.3813\n",
      "Epoch 32/150 Loss: 0.3956\n",
      "Epoch 33/150 Loss: 0.3854\n",
      "Epoch 34/150 Loss: 0.3826\n",
      "Epoch 35/150 Loss: 0.3776\n",
      "Epoch 36/150 Loss: 0.3755\n",
      "Epoch 37/150 Loss: 0.3707\n",
      "Epoch 38/150 Loss: 0.3606\n",
      "Epoch 39/150 Loss: 0.3575\n",
      "Epoch 40/150 Loss: 0.3631\n",
      "Epoch 41/150 Loss: 0.3605\n",
      "Epoch 42/150 Loss: 0.3500\n",
      "Epoch 43/150 Loss: 0.3592\n",
      "Epoch 44/150 Loss: 0.3507\n",
      "Epoch 45/150 Loss: 0.3478\n",
      "Epoch 46/150 Loss: 0.3543\n",
      "Epoch 47/150 Loss: 0.3406\n",
      "Epoch 48/150 Loss: 0.3409\n",
      "Epoch 49/150 Loss: 0.3451\n",
      "Epoch 50/150 Loss: 0.3421\n",
      "Epoch 51/150 Loss: 0.3386\n",
      "Epoch 52/150 Loss: 0.3292\n",
      "Epoch 53/150 Loss: 0.3309\n",
      "Epoch 54/150 Loss: 0.3321\n",
      "Epoch 55/150 Loss: 0.3232\n",
      "Epoch 56/150 Loss: 0.3372\n",
      "Epoch 57/150 Loss: 0.3330\n",
      "Epoch 58/150 Loss: 0.3199\n",
      "Epoch 59/150 Loss: 0.3263\n",
      "Epoch 60/150 Loss: 0.3242\n",
      "Epoch 61/150 Loss: 0.3290\n",
      "Epoch 62/150 Loss: 0.3189\n",
      "Epoch 63/150 Loss: 0.3185\n",
      "Epoch 64/150 Loss: 0.3159\n",
      "Epoch 65/150 Loss: 0.3152\n",
      "Epoch 66/150 Loss: 0.3177\n",
      "Epoch 67/150 Loss: 0.3134\n",
      "Epoch 68/150 Loss: 0.3142\n",
      "Epoch 69/150 Loss: 0.3126\n",
      "Epoch 70/150 Loss: 0.3080\n",
      "Epoch 71/150 Loss: 0.3085\n",
      "Epoch 72/150 Loss: 0.3117\n",
      "Epoch 73/150 Loss: 0.3037\n",
      "Epoch 74/150 Loss: 0.3035\n",
      "Epoch 75/150 Loss: 0.3020\n",
      "Epoch 76/150 Loss: 0.3071\n",
      "Epoch 77/150 Loss: 0.3091\n",
      "Epoch 78/150 Loss: 0.3012\n",
      "Epoch 79/150 Loss: 0.2953\n",
      "Epoch 80/150 Loss: 0.2848\n",
      "Epoch 81/150 Loss: 0.2990\n",
      "Epoch 82/150 Loss: 0.2895\n",
      "Epoch 83/150 Loss: 0.2899\n",
      "Epoch 84/150 Loss: 0.2914\n",
      "Epoch 85/150 Loss: 0.2875\n",
      "Epoch 86/150 Loss: 0.2950\n",
      "Epoch 87/150 Loss: 0.2849\n",
      "Epoch 88/150 Loss: 0.2838\n",
      "Epoch 89/150 Loss: 0.2868\n",
      "Epoch 90/150 Loss: 0.2850\n",
      "Epoch 91/150 Loss: 0.2828\n",
      "Epoch 92/150 Loss: 0.2813\n",
      "Epoch 93/150 Loss: 0.2796\n",
      "Epoch 94/150 Loss: 0.2799\n",
      "Epoch 95/150 Loss: 0.2784\n",
      "Epoch 96/150 Loss: 0.2680\n",
      "Epoch 97/150 Loss: 0.2814\n",
      "Epoch 98/150 Loss: 0.2724\n",
      "Epoch 99/150 Loss: 0.2754\n",
      "Epoch 100/150 Loss: 0.2793\n",
      "Epoch 101/150 Loss: 0.2709\n",
      "Epoch 102/150 Loss: 0.2690\n",
      "Epoch 103/150 Loss: 0.2675\n",
      "Epoch 104/150 Loss: 0.2674\n",
      "Epoch 105/150 Loss: 0.2693\n",
      "Epoch 106/150 Loss: 0.2641\n",
      "Epoch 107/150 Loss: 0.2647\n",
      "Epoch 108/150 Loss: 0.2696\n",
      "Epoch 109/150 Loss: 0.2649\n",
      "Epoch 110/150 Loss: 0.2655\n",
      "Epoch 111/150 Loss: 0.2614\n",
      "Epoch 112/150 Loss: 0.2598\n",
      "Epoch 113/150 Loss: 0.2595\n",
      "Epoch 114/150 Loss: 0.2592\n",
      "Epoch 115/150 Loss: 0.2545\n",
      "Epoch 116/150 Loss: 0.2543\n",
      "Epoch 117/150 Loss: 0.2567\n",
      "Epoch 118/150 Loss: 0.2577\n",
      "Epoch 119/150 Loss: 0.2533\n",
      "Epoch 120/150 Loss: 0.2557\n",
      "Epoch 121/150 Loss: 0.2527\n",
      "Epoch 122/150 Loss: 0.2519\n",
      "Epoch 123/150 Loss: 0.2534\n",
      "Epoch 124/150 Loss: 0.2544\n",
      "Epoch 125/150 Loss: 0.2523\n",
      "Epoch 126/150 Loss: 0.2545\n",
      "Epoch 127/150 Loss: 0.2454\n",
      "Epoch 128/150 Loss: 0.2492\n",
      "Epoch 129/150 Loss: 0.2486\n",
      "Epoch 130/150 Loss: 0.2465\n",
      "Epoch 131/150 Loss: 0.2506\n",
      "Epoch 132/150 Loss: 0.2488\n",
      "Epoch 133/150 Loss: 0.2421\n",
      "Epoch 134/150 Loss: 0.2511\n",
      "Epoch 135/150 Loss: 0.2400\n",
      "Epoch 136/150 Loss: 0.2459\n",
      "Epoch 137/150 Loss: 0.2455\n",
      "Epoch 138/150 Loss: 0.2471\n",
      "Epoch 139/150 Loss: 0.2416\n",
      "Epoch 140/150 Loss: 0.2386\n",
      "Epoch 141/150 Loss: 0.2491\n",
      "Epoch 142/150 Loss: 0.2339\n",
      "Epoch 143/150 Loss: 0.2466\n",
      "Epoch 144/150 Loss: 0.2457\n",
      "Epoch 145/150 Loss: 0.2423\n",
      "Epoch 146/150 Loss: 0.2479\n",
      "Epoch 147/150 Loss: 0.2436\n",
      "Epoch 148/150 Loss: 0.2420\n",
      "Epoch 149/150 Loss: 0.2437\n",
      "Epoch 150/150 Loss: 0.2382\n",
      "Config {'w1': 0.1, 'w2': 0.55, 'lambda_eng': 0.2, 'lr': 0.001} achieved Wasserstein: 0.5906\n",
      "\n",
      "=== Training with {'w1': 0.1, 'w2': 0.6, 'lambda_eng': 0.1, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 0.9601\n",
      "Epoch 2/150 Loss: 0.7005\n",
      "Epoch 3/150 Loss: 0.6297\n",
      "Epoch 4/150 Loss: 0.6001\n",
      "Epoch 5/150 Loss: 0.5726\n",
      "Epoch 6/150 Loss: 0.5576\n",
      "Epoch 7/150 Loss: 0.5376\n",
      "Epoch 8/150 Loss: 0.5222\n",
      "Epoch 9/150 Loss: 0.5045\n",
      "Epoch 10/150 Loss: 0.4941\n",
      "Epoch 11/150 Loss: 0.4839\n",
      "Epoch 12/150 Loss: 0.4767\n",
      "Epoch 13/150 Loss: 0.4679\n",
      "Epoch 14/150 Loss: 0.4658\n",
      "Epoch 15/150 Loss: 0.4595\n",
      "Epoch 16/150 Loss: 0.4545\n",
      "Epoch 17/150 Loss: 0.4339\n",
      "Epoch 18/150 Loss: 0.4270\n",
      "Epoch 19/150 Loss: 0.4254\n",
      "Epoch 20/150 Loss: 0.4264\n",
      "Epoch 21/150 Loss: 0.4148\n",
      "Epoch 22/150 Loss: 0.4126\n",
      "Epoch 23/150 Loss: 0.4016\n",
      "Epoch 24/150 Loss: 0.4009\n",
      "Epoch 25/150 Loss: 0.3891\n",
      "Epoch 26/150 Loss: 0.3854\n",
      "Epoch 27/150 Loss: 0.3954\n",
      "Epoch 28/150 Loss: 0.3942\n",
      "Epoch 29/150 Loss: 0.3818\n",
      "Epoch 30/150 Loss: 0.3812\n",
      "Epoch 31/150 Loss: 0.3741\n",
      "Epoch 32/150 Loss: 0.3702\n",
      "Epoch 33/150 Loss: 0.3702\n",
      "Epoch 34/150 Loss: 0.3608\n",
      "Epoch 35/150 Loss: 0.3602\n",
      "Epoch 36/150 Loss: 0.3572\n",
      "Epoch 37/150 Loss: 0.3580\n",
      "Epoch 38/150 Loss: 0.3636\n",
      "Epoch 39/150 Loss: 0.3576\n",
      "Epoch 40/150 Loss: 0.3462\n",
      "Epoch 41/150 Loss: 0.3466\n",
      "Epoch 42/150 Loss: 0.3393\n",
      "Epoch 43/150 Loss: 0.3370\n",
      "Epoch 44/150 Loss: 0.3370\n",
      "Epoch 45/150 Loss: 0.3353\n",
      "Epoch 46/150 Loss: 0.3349\n",
      "Epoch 47/150 Loss: 0.3313\n",
      "Epoch 48/150 Loss: 0.3346\n",
      "Epoch 49/150 Loss: 0.3304\n",
      "Epoch 50/150 Loss: 0.3351\n",
      "Epoch 51/150 Loss: 0.3264\n",
      "Epoch 52/150 Loss: 0.3285\n",
      "Epoch 53/150 Loss: 0.3260\n",
      "Epoch 54/150 Loss: 0.3168\n",
      "Epoch 55/150 Loss: 0.3181\n",
      "Epoch 56/150 Loss: 0.3195\n",
      "Epoch 57/150 Loss: 0.3191\n",
      "Epoch 58/150 Loss: 0.3131\n",
      "Epoch 59/150 Loss: 0.3116\n",
      "Epoch 60/150 Loss: 0.3170\n",
      "Epoch 61/150 Loss: 0.3047\n",
      "Epoch 62/150 Loss: 0.3043\n",
      "Epoch 63/150 Loss: 0.3104\n",
      "Epoch 64/150 Loss: 0.3031\n",
      "Epoch 65/150 Loss: 0.3063\n",
      "Epoch 66/150 Loss: 0.3017\n",
      "Epoch 67/150 Loss: 0.3003\n",
      "Epoch 68/150 Loss: 0.2989\n",
      "Epoch 69/150 Loss: 0.2986\n",
      "Epoch 70/150 Loss: 0.2943\n",
      "Epoch 71/150 Loss: 0.2983\n",
      "Epoch 72/150 Loss: 0.2954\n",
      "Epoch 73/150 Loss: 0.2878\n",
      "Epoch 74/150 Loss: 0.2870\n",
      "Epoch 75/150 Loss: 0.2901\n",
      "Epoch 76/150 Loss: 0.2874\n",
      "Epoch 77/150 Loss: 0.2830\n",
      "Epoch 78/150 Loss: 0.2802\n",
      "Epoch 79/150 Loss: 0.2785\n",
      "Epoch 80/150 Loss: 0.2741\n",
      "Epoch 81/150 Loss: 0.2835\n",
      "Epoch 82/150 Loss: 0.2811\n",
      "Epoch 83/150 Loss: 0.2773\n",
      "Epoch 84/150 Loss: 0.2835\n",
      "Epoch 85/150 Loss: 0.2784\n",
      "Epoch 86/150 Loss: 0.2793\n",
      "Epoch 87/150 Loss: 0.2666\n",
      "Epoch 88/150 Loss: 0.2735\n",
      "Epoch 89/150 Loss: 0.2794\n",
      "Epoch 90/150 Loss: 0.2760\n",
      "Epoch 91/150 Loss: 0.2780\n",
      "Epoch 92/150 Loss: 0.2692\n",
      "Epoch 93/150 Loss: 0.2651\n",
      "Epoch 94/150 Loss: 0.2661\n",
      "Epoch 95/150 Loss: 0.2674\n",
      "Epoch 96/150 Loss: 0.2724\n",
      "Epoch 97/150 Loss: 0.2647\n",
      "Epoch 98/150 Loss: 0.2647\n",
      "Epoch 99/150 Loss: 0.2601\n",
      "Epoch 100/150 Loss: 0.2614\n",
      "Epoch 101/150 Loss: 0.2644\n",
      "Epoch 102/150 Loss: 0.2635\n",
      "Epoch 103/150 Loss: 0.2669\n",
      "Epoch 104/150 Loss: 0.2630\n",
      "Epoch 105/150 Loss: 0.2537\n",
      "Epoch 106/150 Loss: 0.2573\n",
      "Epoch 107/150 Loss: 0.2542\n",
      "Epoch 108/150 Loss: 0.2539\n",
      "Epoch 109/150 Loss: 0.2544\n",
      "Epoch 110/150 Loss: 0.2523\n",
      "Epoch 111/150 Loss: 0.2528\n",
      "Epoch 112/150 Loss: 0.2516\n",
      "Epoch 113/150 Loss: 0.2478\n",
      "Epoch 114/150 Loss: 0.2456\n",
      "Epoch 115/150 Loss: 0.2457\n",
      "Epoch 116/150 Loss: 0.2464\n",
      "Epoch 117/150 Loss: 0.2450\n",
      "Epoch 118/150 Loss: 0.2516\n",
      "Epoch 119/150 Loss: 0.2434\n",
      "Epoch 120/150 Loss: 0.2450\n",
      "Epoch 121/150 Loss: 0.2449\n",
      "Epoch 122/150 Loss: 0.2415\n",
      "Epoch 123/150 Loss: 0.2463\n",
      "Epoch 124/150 Loss: 0.2456\n",
      "Epoch 125/150 Loss: 0.2383\n",
      "Epoch 126/150 Loss: 0.2436\n",
      "Epoch 127/150 Loss: 0.2362\n",
      "Epoch 128/150 Loss: 0.2430\n",
      "Epoch 129/150 Loss: 0.2415\n",
      "Epoch 130/150 Loss: 0.2364\n",
      "Epoch 131/150 Loss: 0.2353\n",
      "Epoch 132/150 Loss: 0.2421\n",
      "Epoch 133/150 Loss: 0.2395\n",
      "Epoch 134/150 Loss: 0.2344\n",
      "Epoch 135/150 Loss: 0.2383\n",
      "Epoch 136/150 Loss: 0.2328\n",
      "Epoch 137/150 Loss: 0.2378\n",
      "Epoch 138/150 Loss: 0.2294\n",
      "Epoch 139/150 Loss: 0.2339\n",
      "Epoch 140/150 Loss: 0.2260\n",
      "Epoch 141/150 Loss: 0.2317\n",
      "Epoch 142/150 Loss: 0.2277\n",
      "Epoch 143/150 Loss: 0.2315\n",
      "Epoch 144/150 Loss: 0.2275\n",
      "Epoch 145/150 Loss: 0.2324\n",
      "Epoch 146/150 Loss: 0.2304\n",
      "Epoch 147/150 Loss: 0.2292\n",
      "Epoch 148/150 Loss: 0.2255\n",
      "Epoch 149/150 Loss: 0.2294\n",
      "Epoch 150/150 Loss: 0.2272\n",
      "Config {'w1': 0.1, 'w2': 0.6, 'lambda_eng': 0.1, 'lr': 0.001} achieved Wasserstein: 0.9238\n",
      "\n",
      "=== Training with {'w1': 0.1, 'w2': 0.6, 'lambda_eng': 0.15, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.0870\n",
      "Epoch 2/150 Loss: 0.7262\n",
      "Epoch 3/150 Loss: 0.6878\n",
      "Epoch 4/150 Loss: 0.6602\n",
      "Epoch 5/150 Loss: 0.6183\n",
      "Epoch 6/150 Loss: 0.5961\n",
      "Epoch 7/150 Loss: 0.5814\n",
      "Epoch 8/150 Loss: 0.5628\n",
      "Epoch 9/150 Loss: 0.5395\n",
      "Epoch 10/150 Loss: 0.5288\n",
      "Epoch 11/150 Loss: 0.5152\n",
      "Epoch 12/150 Loss: 0.5139\n",
      "Epoch 13/150 Loss: 0.5054\n",
      "Epoch 14/150 Loss: 0.5025\n",
      "Epoch 15/150 Loss: 0.4980\n",
      "Epoch 16/150 Loss: 0.4730\n",
      "Epoch 17/150 Loss: 0.4607\n",
      "Epoch 18/150 Loss: 0.4449\n",
      "Epoch 19/150 Loss: 0.4605\n",
      "Epoch 20/150 Loss: 0.4478\n",
      "Epoch 21/150 Loss: 0.4376\n",
      "Epoch 22/150 Loss: 0.4395\n",
      "Epoch 23/150 Loss: 0.4319\n",
      "Epoch 24/150 Loss: 0.4333\n",
      "Epoch 25/150 Loss: 0.4162\n",
      "Epoch 26/150 Loss: 0.4153\n",
      "Epoch 27/150 Loss: 0.4125\n",
      "Epoch 28/150 Loss: 0.4104\n",
      "Epoch 29/150 Loss: 0.4028\n",
      "Epoch 30/150 Loss: 0.4035\n",
      "Epoch 31/150 Loss: 0.4040\n",
      "Epoch 32/150 Loss: 0.3978\n",
      "Epoch 33/150 Loss: 0.3956\n",
      "Epoch 34/150 Loss: 0.3977\n",
      "Epoch 35/150 Loss: 0.3873\n",
      "Epoch 36/150 Loss: 0.3864\n",
      "Epoch 37/150 Loss: 0.3779\n",
      "Epoch 38/150 Loss: 0.3717\n",
      "Epoch 39/150 Loss: 0.3723\n",
      "Epoch 40/150 Loss: 0.3775\n",
      "Epoch 41/150 Loss: 0.3756\n",
      "Epoch 42/150 Loss: 0.3692\n",
      "Epoch 43/150 Loss: 0.3630\n",
      "Epoch 44/150 Loss: 0.3654\n",
      "Epoch 45/150 Loss: 0.3594\n",
      "Epoch 46/150 Loss: 0.3603\n",
      "Epoch 47/150 Loss: 0.3517\n",
      "Epoch 48/150 Loss: 0.3513\n",
      "Epoch 49/150 Loss: 0.3571\n",
      "Epoch 50/150 Loss: 0.3508\n",
      "Epoch 51/150 Loss: 0.3545\n",
      "Epoch 52/150 Loss: 0.3530\n",
      "Epoch 53/150 Loss: 0.3483\n",
      "Epoch 54/150 Loss: 0.3538\n",
      "Epoch 55/150 Loss: 0.3407\n",
      "Epoch 56/150 Loss: 0.3391\n",
      "Epoch 57/150 Loss: 0.3400\n",
      "Epoch 58/150 Loss: 0.3354\n",
      "Epoch 59/150 Loss: 0.3423\n",
      "Epoch 60/150 Loss: 0.3287\n",
      "Epoch 61/150 Loss: 0.3291\n",
      "Epoch 62/150 Loss: 0.3326\n",
      "Epoch 63/150 Loss: 0.3356\n",
      "Epoch 64/150 Loss: 0.3232\n",
      "Epoch 65/150 Loss: 0.3281\n",
      "Epoch 66/150 Loss: 0.3205\n",
      "Epoch 67/150 Loss: 0.3146\n",
      "Epoch 68/150 Loss: 0.3202\n",
      "Epoch 69/150 Loss: 0.3182\n",
      "Epoch 70/150 Loss: 0.3199\n",
      "Epoch 71/150 Loss: 0.3218\n",
      "Epoch 72/150 Loss: 0.3128\n",
      "Epoch 73/150 Loss: 0.3124\n",
      "Epoch 74/150 Loss: 0.3142\n",
      "Epoch 75/150 Loss: 0.3055\n",
      "Epoch 76/150 Loss: 0.3138\n",
      "Epoch 77/150 Loss: 0.3165\n",
      "Epoch 78/150 Loss: 0.3057\n",
      "Epoch 79/150 Loss: 0.3077\n",
      "Epoch 80/150 Loss: 0.3130\n",
      "Epoch 81/150 Loss: 0.3017\n",
      "Epoch 82/150 Loss: 0.2978\n",
      "Epoch 83/150 Loss: 0.2967\n",
      "Epoch 84/150 Loss: 0.2984\n",
      "Epoch 85/150 Loss: 0.3054\n",
      "Epoch 86/150 Loss: 0.3011\n",
      "Epoch 87/150 Loss: 0.2894\n",
      "Epoch 88/150 Loss: 0.2910\n",
      "Epoch 89/150 Loss: 0.2849\n",
      "Epoch 90/150 Loss: 0.2912\n",
      "Epoch 91/150 Loss: 0.2855\n",
      "Epoch 92/150 Loss: 0.2914\n",
      "Epoch 93/150 Loss: 0.2889\n",
      "Epoch 94/150 Loss: 0.2815\n",
      "Epoch 95/150 Loss: 0.2870\n",
      "Epoch 96/150 Loss: 0.2784\n",
      "Epoch 97/150 Loss: 0.2779\n",
      "Epoch 98/150 Loss: 0.2808\n",
      "Epoch 99/150 Loss: 0.2803\n",
      "Epoch 100/150 Loss: 0.2832\n",
      "Epoch 101/150 Loss: 0.2817\n",
      "Epoch 102/150 Loss: 0.2791\n",
      "Epoch 103/150 Loss: 0.2710\n",
      "Epoch 104/150 Loss: 0.2673\n",
      "Epoch 105/150 Loss: 0.2781\n",
      "Epoch 106/150 Loss: 0.2712\n",
      "Epoch 107/150 Loss: 0.2757\n",
      "Epoch 108/150 Loss: 0.2683\n",
      "Epoch 109/150 Loss: 0.2704\n",
      "Epoch 110/150 Loss: 0.2662\n",
      "Epoch 111/150 Loss: 0.2689\n",
      "Epoch 112/150 Loss: 0.2652\n",
      "Epoch 113/150 Loss: 0.2662\n",
      "Epoch 114/150 Loss: 0.2589\n",
      "Epoch 115/150 Loss: 0.2611\n",
      "Epoch 116/150 Loss: 0.2600\n",
      "Epoch 117/150 Loss: 0.2624\n",
      "Epoch 118/150 Loss: 0.2573\n",
      "Epoch 119/150 Loss: 0.2605\n",
      "Epoch 120/150 Loss: 0.2560\n",
      "Epoch 121/150 Loss: 0.2634\n",
      "Epoch 122/150 Loss: 0.2604\n",
      "Epoch 123/150 Loss: 0.2559\n",
      "Epoch 124/150 Loss: 0.2559\n",
      "Epoch 125/150 Loss: 0.2532\n",
      "Epoch 126/150 Loss: 0.2547\n",
      "Epoch 127/150 Loss: 0.2531\n",
      "Epoch 128/150 Loss: 0.2530\n",
      "Epoch 129/150 Loss: 0.2530\n",
      "Epoch 130/150 Loss: 0.2494\n",
      "Epoch 131/150 Loss: 0.2481\n",
      "Epoch 132/150 Loss: 0.2569\n",
      "Epoch 133/150 Loss: 0.2493\n",
      "Epoch 134/150 Loss: 0.2520\n",
      "Epoch 135/150 Loss: 0.2478\n",
      "Epoch 136/150 Loss: 0.2531\n",
      "Epoch 137/150 Loss: 0.2506\n",
      "Epoch 138/150 Loss: 0.2427\n",
      "Epoch 139/150 Loss: 0.2447\n",
      "Epoch 140/150 Loss: 0.2487\n",
      "Epoch 141/150 Loss: 0.2369\n",
      "Epoch 142/150 Loss: 0.2373\n",
      "Epoch 143/150 Loss: 0.2446\n",
      "Epoch 144/150 Loss: 0.2409\n",
      "Epoch 145/150 Loss: 0.2456\n",
      "Epoch 146/150 Loss: 0.2417\n",
      "Epoch 147/150 Loss: 0.2378\n",
      "Epoch 148/150 Loss: 0.2411\n",
      "Epoch 149/150 Loss: 0.2461\n",
      "Epoch 150/150 Loss: 0.2384\n",
      "Config {'w1': 0.1, 'w2': 0.6, 'lambda_eng': 0.15, 'lr': 0.001} achieved Wasserstein: 0.6705\n",
      "\n",
      "=== Training with {'w1': 0.1, 'w2': 0.6, 'lambda_eng': 0.2, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.1047\n",
      "Epoch 2/150 Loss: 0.7721\n",
      "Epoch 3/150 Loss: 0.7127\n",
      "Epoch 4/150 Loss: 0.6608\n",
      "Epoch 5/150 Loss: 0.6437\n",
      "Epoch 6/150 Loss: 0.6221\n",
      "Epoch 7/150 Loss: 0.6022\n",
      "Epoch 8/150 Loss: 0.5682\n",
      "Epoch 9/150 Loss: 0.5622\n",
      "Epoch 10/150 Loss: 0.5546\n",
      "Epoch 11/150 Loss: 0.5285\n",
      "Epoch 12/150 Loss: 0.5191\n",
      "Epoch 13/150 Loss: 0.5069\n",
      "Epoch 14/150 Loss: 0.4937\n",
      "Epoch 15/150 Loss: 0.4896\n",
      "Epoch 16/150 Loss: 0.4804\n",
      "Epoch 17/150 Loss: 0.4749\n",
      "Epoch 18/150 Loss: 0.4654\n",
      "Epoch 19/150 Loss: 0.4503\n",
      "Epoch 20/150 Loss: 0.4459\n",
      "Epoch 21/150 Loss: 0.4373\n",
      "Epoch 22/150 Loss: 0.4423\n",
      "Epoch 23/150 Loss: 0.4307\n",
      "Epoch 24/150 Loss: 0.4195\n",
      "Epoch 25/150 Loss: 0.4081\n",
      "Epoch 26/150 Loss: 0.4110\n",
      "Epoch 27/150 Loss: 0.4085\n",
      "Epoch 28/150 Loss: 0.4067\n",
      "Epoch 29/150 Loss: 0.3980\n",
      "Epoch 30/150 Loss: 0.4002\n",
      "Epoch 31/150 Loss: 0.3860\n",
      "Epoch 32/150 Loss: 0.3821\n",
      "Epoch 33/150 Loss: 0.3892\n",
      "Epoch 34/150 Loss: 0.3831\n",
      "Epoch 35/150 Loss: 0.3785\n",
      "Epoch 36/150 Loss: 0.3720\n",
      "Epoch 37/150 Loss: 0.3742\n",
      "Epoch 38/150 Loss: 0.3668\n",
      "Epoch 39/150 Loss: 0.3633\n",
      "Epoch 40/150 Loss: 0.3585\n",
      "Epoch 41/150 Loss: 0.3550\n",
      "Epoch 42/150 Loss: 0.3535\n",
      "Epoch 43/150 Loss: 0.3511\n",
      "Epoch 44/150 Loss: 0.3652\n",
      "Epoch 45/150 Loss: 0.3643\n",
      "Epoch 46/150 Loss: 0.3466\n",
      "Epoch 47/150 Loss: 0.3479\n",
      "Epoch 48/150 Loss: 0.3444\n",
      "Epoch 49/150 Loss: 0.3516\n",
      "Epoch 50/150 Loss: 0.3437\n",
      "Epoch 51/150 Loss: 0.3387\n",
      "Epoch 52/150 Loss: 0.3425\n",
      "Epoch 53/150 Loss: 0.3374\n",
      "Epoch 54/150 Loss: 0.3349\n",
      "Epoch 55/150 Loss: 0.3277\n",
      "Epoch 56/150 Loss: 0.3365\n",
      "Epoch 57/150 Loss: 0.3282\n",
      "Epoch 58/150 Loss: 0.3285\n",
      "Epoch 59/150 Loss: 0.3319\n",
      "Epoch 60/150 Loss: 0.3284\n",
      "Epoch 61/150 Loss: 0.3252\n",
      "Epoch 62/150 Loss: 0.3192\n",
      "Epoch 63/150 Loss: 0.3254\n",
      "Epoch 64/150 Loss: 0.3278\n",
      "Epoch 65/150 Loss: 0.3166\n",
      "Epoch 66/150 Loss: 0.3229\n",
      "Epoch 67/150 Loss: 0.3119\n",
      "Epoch 68/150 Loss: 0.3129\n",
      "Epoch 69/150 Loss: 0.3128\n",
      "Epoch 70/150 Loss: 0.3117\n",
      "Epoch 71/150 Loss: 0.3196\n",
      "Epoch 72/150 Loss: 0.3133\n",
      "Epoch 73/150 Loss: 0.3183\n",
      "Epoch 74/150 Loss: 0.3115\n",
      "Epoch 75/150 Loss: 0.3149\n",
      "Epoch 76/150 Loss: 0.3087\n",
      "Epoch 77/150 Loss: 0.3081\n",
      "Epoch 78/150 Loss: 0.3052\n",
      "Epoch 79/150 Loss: 0.2944\n",
      "Epoch 80/150 Loss: 0.2987\n",
      "Epoch 81/150 Loss: 0.3070\n",
      "Epoch 82/150 Loss: 0.2929\n",
      "Epoch 83/150 Loss: 0.3027\n",
      "Epoch 84/150 Loss: 0.2927\n",
      "Epoch 85/150 Loss: 0.2991\n",
      "Epoch 86/150 Loss: 0.2999\n",
      "Epoch 87/150 Loss: 0.2951\n",
      "Epoch 88/150 Loss: 0.3040\n",
      "Epoch 89/150 Loss: 0.2930\n",
      "Epoch 90/150 Loss: 0.2925\n",
      "Epoch 91/150 Loss: 0.2911\n",
      "Epoch 92/150 Loss: 0.2909\n",
      "Epoch 93/150 Loss: 0.2818\n",
      "Epoch 94/150 Loss: 0.2857\n",
      "Epoch 95/150 Loss: 0.2826\n",
      "Epoch 96/150 Loss: 0.2832\n",
      "Epoch 97/150 Loss: 0.2846\n",
      "Epoch 98/150 Loss: 0.2768\n",
      "Epoch 99/150 Loss: 0.2848\n",
      "Epoch 100/150 Loss: 0.2730\n",
      "Epoch 101/150 Loss: 0.2831\n",
      "Epoch 102/150 Loss: 0.2776\n",
      "Epoch 103/150 Loss: 0.2681\n",
      "Epoch 104/150 Loss: 0.2724\n",
      "Epoch 105/150 Loss: 0.2834\n",
      "Epoch 106/150 Loss: 0.2713\n",
      "Epoch 107/150 Loss: 0.2670\n",
      "Epoch 108/150 Loss: 0.2741\n",
      "Epoch 109/150 Loss: 0.2738\n",
      "Epoch 110/150 Loss: 0.2737\n",
      "Epoch 111/150 Loss: 0.2735\n",
      "Epoch 112/150 Loss: 0.2670\n",
      "Epoch 113/150 Loss: 0.2685\n",
      "Epoch 114/150 Loss: 0.2703\n",
      "Epoch 115/150 Loss: 0.2634\n",
      "Epoch 116/150 Loss: 0.2590\n",
      "Epoch 117/150 Loss: 0.2671\n",
      "Epoch 118/150 Loss: 0.2636\n",
      "Epoch 119/150 Loss: 0.2578\n",
      "Epoch 120/150 Loss: 0.2631\n",
      "Epoch 121/150 Loss: 0.2591\n",
      "Epoch 122/150 Loss: 0.2612\n",
      "Epoch 123/150 Loss: 0.2631\n",
      "Epoch 124/150 Loss: 0.2605\n",
      "Epoch 125/150 Loss: 0.2635\n",
      "Epoch 126/150 Loss: 0.2535\n",
      "Epoch 127/150 Loss: 0.2511\n",
      "Epoch 128/150 Loss: 0.2530\n",
      "Epoch 129/150 Loss: 0.2533\n",
      "Epoch 130/150 Loss: 0.2512\n",
      "Epoch 131/150 Loss: 0.2496\n",
      "Epoch 132/150 Loss: 0.2426\n",
      "Epoch 133/150 Loss: 0.2446\n",
      "Epoch 134/150 Loss: 0.2458\n",
      "Epoch 135/150 Loss: 0.2525\n",
      "Epoch 136/150 Loss: 0.2542\n",
      "Epoch 137/150 Loss: 0.2525\n",
      "Epoch 138/150 Loss: 0.2544\n",
      "Epoch 139/150 Loss: 0.2486\n",
      "Epoch 140/150 Loss: 0.2464\n",
      "Epoch 141/150 Loss: 0.2426\n",
      "Epoch 142/150 Loss: 0.2492\n",
      "Epoch 143/150 Loss: 0.2454\n",
      "Epoch 144/150 Loss: 0.2415\n",
      "Epoch 145/150 Loss: 0.2474\n",
      "Epoch 146/150 Loss: 0.2413\n",
      "Epoch 147/150 Loss: 0.2397\n",
      "Epoch 148/150 Loss: 0.2400\n",
      "Epoch 149/150 Loss: 0.2406\n",
      "Epoch 150/150 Loss: 0.2431\n",
      "Config {'w1': 0.1, 'w2': 0.6, 'lambda_eng': 0.2, 'lr': 0.001} achieved Wasserstein: 0.6592\n",
      "\n",
      "=== Training with {'w1': 0.15, 'w2': 0.5, 'lambda_eng': 0.1, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 0.9362\n",
      "Epoch 2/150 Loss: 0.6821\n",
      "Epoch 3/150 Loss: 0.6376\n",
      "Epoch 4/150 Loss: 0.6143\n",
      "Epoch 5/150 Loss: 0.5985\n",
      "Epoch 6/150 Loss: 0.5701\n",
      "Epoch 7/150 Loss: 0.5554\n",
      "Epoch 8/150 Loss: 0.5450\n",
      "Epoch 9/150 Loss: 0.5262\n",
      "Epoch 10/150 Loss: 0.5089\n",
      "Epoch 11/150 Loss: 0.4992\n",
      "Epoch 12/150 Loss: 0.4944\n",
      "Epoch 13/150 Loss: 0.4781\n",
      "Epoch 14/150 Loss: 0.4780\n",
      "Epoch 15/150 Loss: 0.4696\n",
      "Epoch 16/150 Loss: 0.4612\n",
      "Epoch 17/150 Loss: 0.4448\n",
      "Epoch 18/150 Loss: 0.4430\n",
      "Epoch 19/150 Loss: 0.4374\n",
      "Epoch 20/150 Loss: 0.4301\n",
      "Epoch 21/150 Loss: 0.4374\n",
      "Epoch 22/150 Loss: 0.4287\n",
      "Epoch 23/150 Loss: 0.4160\n",
      "Epoch 24/150 Loss: 0.4183\n",
      "Epoch 25/150 Loss: 0.4149\n",
      "Epoch 26/150 Loss: 0.4086\n",
      "Epoch 27/150 Loss: 0.4045\n",
      "Epoch 28/150 Loss: 0.4045\n",
      "Epoch 29/150 Loss: 0.3992\n",
      "Epoch 30/150 Loss: 0.3921\n",
      "Epoch 31/150 Loss: 0.3871\n",
      "Epoch 32/150 Loss: 0.3918\n",
      "Epoch 33/150 Loss: 0.3810\n",
      "Epoch 34/150 Loss: 0.3697\n",
      "Epoch 35/150 Loss: 0.3758\n",
      "Epoch 36/150 Loss: 0.3721\n",
      "Epoch 37/150 Loss: 0.3712\n",
      "Epoch 38/150 Loss: 0.3708\n",
      "Epoch 39/150 Loss: 0.3747\n",
      "Epoch 40/150 Loss: 0.3662\n",
      "Epoch 41/150 Loss: 0.3627\n",
      "Epoch 42/150 Loss: 0.3619\n",
      "Epoch 43/150 Loss: 0.3550\n",
      "Epoch 44/150 Loss: 0.3573\n",
      "Epoch 45/150 Loss: 0.3491\n",
      "Epoch 46/150 Loss: 0.3472\n",
      "Epoch 47/150 Loss: 0.3385\n",
      "Epoch 48/150 Loss: 0.3430\n",
      "Epoch 49/150 Loss: 0.3441\n",
      "Epoch 50/150 Loss: 0.3448\n",
      "Epoch 51/150 Loss: 0.3390\n",
      "Epoch 52/150 Loss: 0.3439\n",
      "Epoch 53/150 Loss: 0.3374\n",
      "Epoch 54/150 Loss: 0.3368\n",
      "Epoch 55/150 Loss: 0.3297\n",
      "Epoch 56/150 Loss: 0.3316\n",
      "Epoch 57/150 Loss: 0.3292\n",
      "Epoch 58/150 Loss: 0.3223\n",
      "Epoch 59/150 Loss: 0.3302\n",
      "Epoch 60/150 Loss: 0.3262\n",
      "Epoch 61/150 Loss: 0.3156\n",
      "Epoch 62/150 Loss: 0.3287\n",
      "Epoch 63/150 Loss: 0.3197\n",
      "Epoch 64/150 Loss: 0.3244\n",
      "Epoch 65/150 Loss: 0.3215\n",
      "Epoch 66/150 Loss: 0.3148\n",
      "Epoch 67/150 Loss: 0.3160\n",
      "Epoch 68/150 Loss: 0.3199\n",
      "Epoch 69/150 Loss: 0.3102\n",
      "Epoch 70/150 Loss: 0.3091\n",
      "Epoch 71/150 Loss: 0.3137\n",
      "Epoch 72/150 Loss: 0.3093\n",
      "Epoch 73/150 Loss: 0.3005\n",
      "Epoch 74/150 Loss: 0.3124\n",
      "Epoch 75/150 Loss: 0.3005\n",
      "Epoch 76/150 Loss: 0.3091\n",
      "Epoch 77/150 Loss: 0.2986\n",
      "Epoch 78/150 Loss: 0.3028\n",
      "Epoch 79/150 Loss: 0.2968\n",
      "Epoch 80/150 Loss: 0.3025\n",
      "Epoch 81/150 Loss: 0.2983\n",
      "Epoch 82/150 Loss: 0.2993\n",
      "Epoch 83/150 Loss: 0.3023\n",
      "Epoch 84/150 Loss: 0.2974\n",
      "Epoch 85/150 Loss: 0.3005\n",
      "Epoch 86/150 Loss: 0.2894\n",
      "Epoch 87/150 Loss: 0.2920\n",
      "Epoch 88/150 Loss: 0.2874\n",
      "Epoch 89/150 Loss: 0.2857\n",
      "Epoch 90/150 Loss: 0.2801\n",
      "Epoch 91/150 Loss: 0.2792\n",
      "Epoch 92/150 Loss: 0.2866\n",
      "Epoch 93/150 Loss: 0.2793\n",
      "Epoch 94/150 Loss: 0.2824\n",
      "Epoch 95/150 Loss: 0.2767\n",
      "Epoch 96/150 Loss: 0.2829\n",
      "Epoch 97/150 Loss: 0.2842\n",
      "Epoch 98/150 Loss: 0.2792\n",
      "Epoch 99/150 Loss: 0.2792\n",
      "Epoch 100/150 Loss: 0.2773\n",
      "Epoch 101/150 Loss: 0.2731\n",
      "Epoch 102/150 Loss: 0.2757\n",
      "Epoch 103/150 Loss: 0.2745\n",
      "Epoch 104/150 Loss: 0.2739\n",
      "Epoch 105/150 Loss: 0.2736\n",
      "Epoch 106/150 Loss: 0.2681\n",
      "Epoch 107/150 Loss: 0.2641\n",
      "Epoch 108/150 Loss: 0.2679\n",
      "Epoch 109/150 Loss: 0.2691\n",
      "Epoch 110/150 Loss: 0.2686\n",
      "Epoch 111/150 Loss: 0.2695\n",
      "Epoch 112/150 Loss: 0.2675\n",
      "Epoch 113/150 Loss: 0.2675\n",
      "Epoch 114/150 Loss: 0.2617\n",
      "Epoch 115/150 Loss: 0.2592\n",
      "Epoch 116/150 Loss: 0.2591\n",
      "Epoch 117/150 Loss: 0.2649\n",
      "Epoch 118/150 Loss: 0.2598\n",
      "Epoch 119/150 Loss: 0.2586\n",
      "Epoch 120/150 Loss: 0.2576\n",
      "Epoch 121/150 Loss: 0.2572\n",
      "Epoch 122/150 Loss: 0.2576\n",
      "Epoch 123/150 Loss: 0.2597\n",
      "Epoch 124/150 Loss: 0.2542\n",
      "Epoch 125/150 Loss: 0.2546\n",
      "Epoch 126/150 Loss: 0.2598\n",
      "Epoch 127/150 Loss: 0.2599\n",
      "Epoch 128/150 Loss: 0.2564\n",
      "Epoch 129/150 Loss: 0.2563\n",
      "Epoch 130/150 Loss: 0.2498\n",
      "Epoch 131/150 Loss: 0.2514\n",
      "Epoch 132/150 Loss: 0.2525\n",
      "Epoch 133/150 Loss: 0.2500\n",
      "Epoch 134/150 Loss: 0.2501\n",
      "Epoch 135/150 Loss: 0.2516\n",
      "Epoch 136/150 Loss: 0.2530\n",
      "Epoch 137/150 Loss: 0.2461\n",
      "Epoch 138/150 Loss: 0.2553\n",
      "Epoch 139/150 Loss: 0.2473\n",
      "Epoch 140/150 Loss: 0.2547\n",
      "Epoch 141/150 Loss: 0.2480\n",
      "Epoch 142/150 Loss: 0.2452\n",
      "Epoch 143/150 Loss: 0.2527\n",
      "Epoch 144/150 Loss: 0.2499\n",
      "Epoch 145/150 Loss: 0.2500\n",
      "Epoch 146/150 Loss: 0.2476\n",
      "Epoch 147/150 Loss: 0.2468\n",
      "Epoch 148/150 Loss: 0.2485\n",
      "Epoch 149/150 Loss: 0.2458\n",
      "Epoch 150/150 Loss: 0.2462\n",
      "Config {'w1': 0.15, 'w2': 0.5, 'lambda_eng': 0.1, 'lr': 0.001} achieved Wasserstein: 0.6915\n",
      "\n",
      "=== Training with {'w1': 0.15, 'w2': 0.5, 'lambda_eng': 0.15, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.0524\n",
      "Epoch 2/150 Loss: 0.7323\n",
      "Epoch 3/150 Loss: 0.6913\n",
      "Epoch 4/150 Loss: 0.6693\n",
      "Epoch 5/150 Loss: 0.6334\n",
      "Epoch 6/150 Loss: 0.6052\n",
      "Epoch 7/150 Loss: 0.5891\n",
      "Epoch 8/150 Loss: 0.5704\n",
      "Epoch 9/150 Loss: 0.5541\n",
      "Epoch 10/150 Loss: 0.5297\n",
      "Epoch 11/150 Loss: 0.5073\n",
      "Epoch 12/150 Loss: 0.5120\n",
      "Epoch 13/150 Loss: 0.5027\n",
      "Epoch 14/150 Loss: 0.4993\n",
      "Epoch 15/150 Loss: 0.4869\n",
      "Epoch 16/150 Loss: 0.4808\n",
      "Epoch 17/150 Loss: 0.4744\n",
      "Epoch 18/150 Loss: 0.4743\n",
      "Epoch 19/150 Loss: 0.4664\n",
      "Epoch 20/150 Loss: 0.4486\n",
      "Epoch 21/150 Loss: 0.4545\n",
      "Epoch 22/150 Loss: 0.4465\n",
      "Epoch 23/150 Loss: 0.4320\n",
      "Epoch 24/150 Loss: 0.4328\n",
      "Epoch 25/150 Loss: 0.4403\n",
      "Epoch 26/150 Loss: 0.4338\n",
      "Epoch 27/150 Loss: 0.4139\n",
      "Epoch 28/150 Loss: 0.4182\n",
      "Epoch 29/150 Loss: 0.4152\n",
      "Epoch 30/150 Loss: 0.4184\n",
      "Epoch 31/150 Loss: 0.4127\n",
      "Epoch 32/150 Loss: 0.4019\n",
      "Epoch 33/150 Loss: 0.3992\n",
      "Epoch 34/150 Loss: 0.3970\n",
      "Epoch 35/150 Loss: 0.3946\n",
      "Epoch 36/150 Loss: 0.3943\n",
      "Epoch 37/150 Loss: 0.3910\n",
      "Epoch 38/150 Loss: 0.3832\n",
      "Epoch 39/150 Loss: 0.3845\n",
      "Epoch 40/150 Loss: 0.3772\n",
      "Epoch 41/150 Loss: 0.3777\n",
      "Epoch 42/150 Loss: 0.3658\n",
      "Epoch 43/150 Loss: 0.3671\n",
      "Epoch 44/150 Loss: 0.3688\n",
      "Epoch 45/150 Loss: 0.3605\n",
      "Epoch 46/150 Loss: 0.3631\n",
      "Epoch 47/150 Loss: 0.3679\n",
      "Epoch 48/150 Loss: 0.3592\n",
      "Epoch 49/150 Loss: 0.3538\n",
      "Epoch 50/150 Loss: 0.3550\n",
      "Epoch 51/150 Loss: 0.3617\n",
      "Epoch 52/150 Loss: 0.3593\n",
      "Epoch 53/150 Loss: 0.3475\n",
      "Epoch 54/150 Loss: 0.3427\n",
      "Epoch 55/150 Loss: 0.3536\n",
      "Epoch 56/150 Loss: 0.3424\n",
      "Epoch 57/150 Loss: 0.3461\n",
      "Epoch 58/150 Loss: 0.3437\n",
      "Epoch 59/150 Loss: 0.3322\n",
      "Epoch 60/150 Loss: 0.3380\n",
      "Epoch 61/150 Loss: 0.3354\n",
      "Epoch 62/150 Loss: 0.3323\n",
      "Epoch 63/150 Loss: 0.3386\n",
      "Epoch 64/150 Loss: 0.3361\n",
      "Epoch 65/150 Loss: 0.3332\n",
      "Epoch 66/150 Loss: 0.3271\n",
      "Epoch 67/150 Loss: 0.3300\n",
      "Epoch 68/150 Loss: 0.3342\n",
      "Epoch 69/150 Loss: 0.3223\n",
      "Epoch 70/150 Loss: 0.3233\n",
      "Epoch 71/150 Loss: 0.3235\n",
      "Epoch 72/150 Loss: 0.3256\n",
      "Epoch 73/150 Loss: 0.3175\n",
      "Epoch 74/150 Loss: 0.3168\n",
      "Epoch 75/150 Loss: 0.3185\n",
      "Epoch 76/150 Loss: 0.3162\n",
      "Epoch 77/150 Loss: 0.3097\n",
      "Epoch 78/150 Loss: 0.3098\n",
      "Epoch 79/150 Loss: 0.3182\n",
      "Epoch 80/150 Loss: 0.3085\n",
      "Epoch 81/150 Loss: 0.3051\n",
      "Epoch 82/150 Loss: 0.3049\n",
      "Epoch 83/150 Loss: 0.3041\n",
      "Epoch 84/150 Loss: 0.3175\n",
      "Epoch 85/150 Loss: 0.3056\n",
      "Epoch 86/150 Loss: 0.3037\n",
      "Epoch 87/150 Loss: 0.3113\n",
      "Epoch 88/150 Loss: 0.3024\n",
      "Epoch 89/150 Loss: 0.2939\n",
      "Epoch 90/150 Loss: 0.2911\n",
      "Epoch 91/150 Loss: 0.2889\n",
      "Epoch 92/150 Loss: 0.2984\n",
      "Epoch 93/150 Loss: 0.2915\n",
      "Epoch 94/150 Loss: 0.2870\n",
      "Epoch 95/150 Loss: 0.2980\n",
      "Epoch 96/150 Loss: 0.2901\n",
      "Epoch 97/150 Loss: 0.2911\n",
      "Epoch 98/150 Loss: 0.2897\n",
      "Epoch 99/150 Loss: 0.2881\n",
      "Epoch 100/150 Loss: 0.2875\n",
      "Epoch 101/150 Loss: 0.2844\n",
      "Epoch 102/150 Loss: 0.2877\n",
      "Epoch 103/150 Loss: 0.2799\n",
      "Epoch 104/150 Loss: 0.2833\n",
      "Epoch 105/150 Loss: 0.2781\n",
      "Epoch 106/150 Loss: 0.2817\n",
      "Epoch 107/150 Loss: 0.2782\n",
      "Epoch 108/150 Loss: 0.2768\n",
      "Epoch 109/150 Loss: 0.2826\n",
      "Epoch 110/150 Loss: 0.2829\n",
      "Epoch 111/150 Loss: 0.2824\n",
      "Epoch 112/150 Loss: 0.2784\n",
      "Epoch 113/150 Loss: 0.2763\n",
      "Epoch 114/150 Loss: 0.2675\n",
      "Epoch 115/150 Loss: 0.2762\n",
      "Epoch 116/150 Loss: 0.2740\n",
      "Epoch 117/150 Loss: 0.2707\n",
      "Epoch 118/150 Loss: 0.2773\n",
      "Epoch 119/150 Loss: 0.2711\n",
      "Epoch 120/150 Loss: 0.2699\n",
      "Epoch 121/150 Loss: 0.2655\n",
      "Epoch 122/150 Loss: 0.2708\n",
      "Epoch 123/150 Loss: 0.2673\n",
      "Epoch 124/150 Loss: 0.2641\n",
      "Epoch 125/150 Loss: 0.2660\n",
      "Epoch 126/150 Loss: 0.2716\n",
      "Epoch 127/150 Loss: 0.2682\n",
      "Epoch 128/150 Loss: 0.2610\n",
      "Epoch 129/150 Loss: 0.2611\n",
      "Epoch 130/150 Loss: 0.2680\n",
      "Epoch 131/150 Loss: 0.2665\n",
      "Epoch 132/150 Loss: 0.2685\n",
      "Epoch 133/150 Loss: 0.2645\n",
      "Epoch 134/150 Loss: 0.2592\n",
      "Epoch 135/150 Loss: 0.2556\n",
      "Epoch 136/150 Loss: 0.2611\n",
      "Epoch 137/150 Loss: 0.2578\n",
      "Epoch 138/150 Loss: 0.2608\n",
      "Epoch 139/150 Loss: 0.2630\n",
      "Epoch 140/150 Loss: 0.2564\n",
      "Epoch 141/150 Loss: 0.2577\n",
      "Epoch 142/150 Loss: 0.2551\n",
      "Epoch 143/150 Loss: 0.2534\n",
      "Epoch 144/150 Loss: 0.2546\n",
      "Epoch 145/150 Loss: 0.2533\n",
      "Epoch 146/150 Loss: 0.2528\n",
      "Epoch 147/150 Loss: 0.2512\n",
      "Epoch 148/150 Loss: 0.2540\n",
      "Epoch 149/150 Loss: 0.2529\n",
      "Epoch 150/150 Loss: 0.2563\n",
      "Config {'w1': 0.15, 'w2': 0.5, 'lambda_eng': 0.15, 'lr': 0.001} achieved Wasserstein: 0.8640\n",
      "\n",
      "=== Training with {'w1': 0.15, 'w2': 0.5, 'lambda_eng': 0.2, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.0889\n",
      "Epoch 2/150 Loss: 0.7281\n",
      "Epoch 3/150 Loss: 0.7015\n",
      "Epoch 4/150 Loss: 0.6680\n",
      "Epoch 5/150 Loss: 0.6409\n",
      "Epoch 6/150 Loss: 0.6129\n",
      "Epoch 7/150 Loss: 0.5947\n",
      "Epoch 8/150 Loss: 0.5802\n",
      "Epoch 9/150 Loss: 0.5484\n",
      "Epoch 10/150 Loss: 0.5560\n",
      "Epoch 11/150 Loss: 0.5443\n",
      "Epoch 12/150 Loss: 0.5227\n",
      "Epoch 13/150 Loss: 0.5159\n",
      "Epoch 14/150 Loss: 0.5108\n",
      "Epoch 15/150 Loss: 0.5030\n",
      "Epoch 16/150 Loss: 0.5044\n",
      "Epoch 17/150 Loss: 0.4812\n",
      "Epoch 18/150 Loss: 0.4828\n",
      "Epoch 19/150 Loss: 0.4661\n",
      "Epoch 20/150 Loss: 0.4666\n",
      "Epoch 21/150 Loss: 0.4571\n",
      "Epoch 22/150 Loss: 0.4507\n",
      "Epoch 23/150 Loss: 0.4344\n",
      "Epoch 24/150 Loss: 0.4382\n",
      "Epoch 25/150 Loss: 0.4238\n",
      "Epoch 26/150 Loss: 0.4291\n",
      "Epoch 27/150 Loss: 0.4202\n",
      "Epoch 28/150 Loss: 0.4166\n",
      "Epoch 29/150 Loss: 0.4109\n",
      "Epoch 30/150 Loss: 0.4156\n",
      "Epoch 31/150 Loss: 0.4004\n",
      "Epoch 32/150 Loss: 0.3970\n",
      "Epoch 33/150 Loss: 0.3945\n",
      "Epoch 34/150 Loss: 0.3911\n",
      "Epoch 35/150 Loss: 0.3836\n",
      "Epoch 36/150 Loss: 0.3877\n",
      "Epoch 37/150 Loss: 0.3836\n",
      "Epoch 38/150 Loss: 0.3817\n",
      "Epoch 39/150 Loss: 0.3725\n",
      "Epoch 40/150 Loss: 0.3796\n",
      "Epoch 41/150 Loss: 0.3679\n",
      "Epoch 42/150 Loss: 0.3622\n",
      "Epoch 43/150 Loss: 0.3702\n",
      "Epoch 44/150 Loss: 0.3702\n",
      "Epoch 45/150 Loss: 0.3700\n",
      "Epoch 46/150 Loss: 0.3588\n",
      "Epoch 47/150 Loss: 0.3556\n",
      "Epoch 48/150 Loss: 0.3629\n",
      "Epoch 49/150 Loss: 0.3496\n",
      "Epoch 50/150 Loss: 0.3534\n",
      "Epoch 51/150 Loss: 0.3482\n",
      "Epoch 52/150 Loss: 0.3500\n",
      "Epoch 53/150 Loss: 0.3435\n",
      "Epoch 54/150 Loss: 0.3464\n",
      "Epoch 55/150 Loss: 0.3417\n",
      "Epoch 56/150 Loss: 0.3467\n",
      "Epoch 57/150 Loss: 0.3365\n",
      "Epoch 58/150 Loss: 0.3404\n",
      "Epoch 59/150 Loss: 0.3388\n",
      "Epoch 60/150 Loss: 0.3478\n",
      "Epoch 61/150 Loss: 0.3462\n",
      "Epoch 62/150 Loss: 0.3355\n",
      "Epoch 63/150 Loss: 0.3253\n",
      "Epoch 64/150 Loss: 0.3285\n",
      "Epoch 65/150 Loss: 0.3278\n",
      "Epoch 66/150 Loss: 0.3287\n",
      "Epoch 67/150 Loss: 0.3276\n",
      "Epoch 68/150 Loss: 0.3252\n",
      "Epoch 69/150 Loss: 0.3207\n",
      "Epoch 70/150 Loss: 0.3159\n",
      "Epoch 71/150 Loss: 0.3209\n",
      "Epoch 72/150 Loss: 0.3223\n",
      "Epoch 73/150 Loss: 0.3265\n",
      "Epoch 74/150 Loss: 0.3199\n",
      "Epoch 75/150 Loss: 0.3174\n",
      "Epoch 76/150 Loss: 0.3095\n",
      "Epoch 77/150 Loss: 0.3112\n",
      "Epoch 78/150 Loss: 0.3059\n",
      "Epoch 79/150 Loss: 0.3134\n",
      "Epoch 80/150 Loss: 0.3128\n",
      "Epoch 81/150 Loss: 0.3151\n",
      "Epoch 82/150 Loss: 0.3066\n",
      "Epoch 83/150 Loss: 0.3010\n",
      "Epoch 84/150 Loss: 0.3014\n",
      "Epoch 85/150 Loss: 0.3112\n",
      "Epoch 86/150 Loss: 0.3106\n",
      "Epoch 87/150 Loss: 0.2989\n",
      "Epoch 88/150 Loss: 0.3004\n",
      "Epoch 89/150 Loss: 0.2962\n",
      "Epoch 90/150 Loss: 0.2958\n",
      "Epoch 91/150 Loss: 0.2980\n",
      "Epoch 92/150 Loss: 0.2971\n",
      "Epoch 93/150 Loss: 0.2989\n",
      "Epoch 94/150 Loss: 0.2941\n",
      "Epoch 95/150 Loss: 0.2915\n",
      "Epoch 96/150 Loss: 0.3007\n",
      "Epoch 97/150 Loss: 0.2926\n",
      "Epoch 98/150 Loss: 0.2838\n",
      "Epoch 99/150 Loss: 0.2870\n",
      "Epoch 100/150 Loss: 0.2850\n",
      "Epoch 101/150 Loss: 0.2815\n",
      "Epoch 102/150 Loss: 0.2808\n",
      "Epoch 103/150 Loss: 0.2891\n",
      "Epoch 104/150 Loss: 0.2881\n",
      "Epoch 105/150 Loss: 0.2815\n",
      "Epoch 106/150 Loss: 0.2849\n",
      "Epoch 107/150 Loss: 0.2794\n",
      "Epoch 108/150 Loss: 0.2815\n",
      "Epoch 109/150 Loss: 0.2825\n",
      "Epoch 110/150 Loss: 0.2779\n",
      "Epoch 111/150 Loss: 0.2778\n",
      "Epoch 112/150 Loss: 0.2828\n",
      "Epoch 113/150 Loss: 0.2724\n",
      "Epoch 114/150 Loss: 0.2832\n",
      "Epoch 115/150 Loss: 0.2696\n",
      "Epoch 116/150 Loss: 0.2742\n",
      "Epoch 117/150 Loss: 0.2767\n",
      "Epoch 118/150 Loss: 0.2777\n",
      "Epoch 119/150 Loss: 0.2707\n",
      "Epoch 120/150 Loss: 0.2784\n",
      "Epoch 121/150 Loss: 0.2732\n",
      "Epoch 122/150 Loss: 0.2667\n",
      "Epoch 123/150 Loss: 0.2686\n",
      "Epoch 124/150 Loss: 0.2640\n",
      "Epoch 125/150 Loss: 0.2702\n",
      "Epoch 126/150 Loss: 0.2637\n",
      "Epoch 127/150 Loss: 0.2605\n",
      "Epoch 128/150 Loss: 0.2681\n",
      "Epoch 129/150 Loss: 0.2675\n",
      "Epoch 130/150 Loss: 0.2650\n",
      "Epoch 131/150 Loss: 0.2566\n",
      "Epoch 132/150 Loss: 0.2641\n",
      "Epoch 133/150 Loss: 0.2608\n",
      "Epoch 134/150 Loss: 0.2674\n",
      "Epoch 135/150 Loss: 0.2643\n",
      "Epoch 136/150 Loss: 0.2633\n",
      "Epoch 137/150 Loss: 0.2597\n",
      "Epoch 138/150 Loss: 0.2633\n",
      "Epoch 139/150 Loss: 0.2621\n",
      "Epoch 140/150 Loss: 0.2556\n",
      "Epoch 141/150 Loss: 0.2552\n",
      "Epoch 142/150 Loss: 0.2520\n",
      "Epoch 143/150 Loss: 0.2585\n",
      "Epoch 144/150 Loss: 0.2582\n",
      "Epoch 145/150 Loss: 0.2503\n",
      "Epoch 146/150 Loss: 0.2583\n",
      "Epoch 147/150 Loss: 0.2555\n",
      "Epoch 148/150 Loss: 0.2609\n",
      "Epoch 149/150 Loss: 0.2577\n",
      "Epoch 150/150 Loss: 0.2622\n",
      "Config {'w1': 0.15, 'w2': 0.5, 'lambda_eng': 0.2, 'lr': 0.001} achieved Wasserstein: 0.8001\n",
      "\n",
      "=== Training with {'w1': 0.15, 'w2': 0.55, 'lambda_eng': 0.1, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 0.9433\n",
      "Epoch 2/150 Loss: 0.6978\n",
      "Epoch 3/150 Loss: 0.6652\n",
      "Epoch 4/150 Loss: 0.6256\n",
      "Epoch 5/150 Loss: 0.5994\n",
      "Epoch 6/150 Loss: 0.5789\n",
      "Epoch 7/150 Loss: 0.5491\n",
      "Epoch 8/150 Loss: 0.5495\n",
      "Epoch 9/150 Loss: 0.5238\n",
      "Epoch 10/150 Loss: 0.5098\n",
      "Epoch 11/150 Loss: 0.5024\n",
      "Epoch 12/150 Loss: 0.4984\n",
      "Epoch 13/150 Loss: 0.4991\n",
      "Epoch 14/150 Loss: 0.4822\n",
      "Epoch 15/150 Loss: 0.4707\n",
      "Epoch 16/150 Loss: 0.4568\n",
      "Epoch 17/150 Loss: 0.4537\n",
      "Epoch 18/150 Loss: 0.4598\n",
      "Epoch 19/150 Loss: 0.4502\n",
      "Epoch 20/150 Loss: 0.4449\n",
      "Epoch 21/150 Loss: 0.4413\n",
      "Epoch 22/150 Loss: 0.4344\n",
      "Epoch 23/150 Loss: 0.4188\n",
      "Epoch 24/150 Loss: 0.4279\n",
      "Epoch 25/150 Loss: 0.4204\n",
      "Epoch 26/150 Loss: 0.4189\n",
      "Epoch 27/150 Loss: 0.4116\n",
      "Epoch 28/150 Loss: 0.4168\n",
      "Epoch 29/150 Loss: 0.4176\n",
      "Epoch 30/150 Loss: 0.3982\n",
      "Epoch 31/150 Loss: 0.4026\n",
      "Epoch 32/150 Loss: 0.4036\n",
      "Epoch 33/150 Loss: 0.3997\n",
      "Epoch 34/150 Loss: 0.3964\n",
      "Epoch 35/150 Loss: 0.3883\n",
      "Epoch 36/150 Loss: 0.3912\n",
      "Epoch 37/150 Loss: 0.3909\n",
      "Epoch 38/150 Loss: 0.3870\n",
      "Epoch 39/150 Loss: 0.3771\n",
      "Epoch 40/150 Loss: 0.3825\n",
      "Epoch 41/150 Loss: 0.3718\n",
      "Epoch 42/150 Loss: 0.3767\n",
      "Epoch 43/150 Loss: 0.3728\n",
      "Epoch 44/150 Loss: 0.3703\n",
      "Epoch 45/150 Loss: 0.3630\n",
      "Epoch 46/150 Loss: 0.3627\n",
      "Epoch 47/150 Loss: 0.3591\n",
      "Epoch 48/150 Loss: 0.3549\n",
      "Epoch 49/150 Loss: 0.3579\n",
      "Epoch 50/150 Loss: 0.3542\n",
      "Epoch 51/150 Loss: 0.3621\n",
      "Epoch 52/150 Loss: 0.3511\n",
      "Epoch 53/150 Loss: 0.3476\n",
      "Epoch 54/150 Loss: 0.3368\n",
      "Epoch 55/150 Loss: 0.3398\n",
      "Epoch 56/150 Loss: 0.3453\n",
      "Epoch 57/150 Loss: 0.3516\n",
      "Epoch 58/150 Loss: 0.3428\n",
      "Epoch 59/150 Loss: 0.3394\n",
      "Epoch 60/150 Loss: 0.3353\n",
      "Epoch 61/150 Loss: 0.3371\n",
      "Epoch 62/150 Loss: 0.3357\n",
      "Epoch 63/150 Loss: 0.3331\n",
      "Epoch 64/150 Loss: 0.3287\n",
      "Epoch 65/150 Loss: 0.3354\n",
      "Epoch 66/150 Loss: 0.3269\n",
      "Epoch 67/150 Loss: 0.3285\n",
      "Epoch 68/150 Loss: 0.3306\n",
      "Epoch 69/150 Loss: 0.3194\n",
      "Epoch 70/150 Loss: 0.3215\n",
      "Epoch 71/150 Loss: 0.3170\n",
      "Epoch 72/150 Loss: 0.3174\n",
      "Epoch 73/150 Loss: 0.3271\n",
      "Epoch 74/150 Loss: 0.3063\n",
      "Epoch 75/150 Loss: 0.3168\n",
      "Epoch 76/150 Loss: 0.3190\n",
      "Epoch 77/150 Loss: 0.3184\n",
      "Epoch 78/150 Loss: 0.3086\n",
      "Epoch 79/150 Loss: 0.3072\n",
      "Epoch 80/150 Loss: 0.3108\n",
      "Epoch 81/150 Loss: 0.3077\n",
      "Epoch 82/150 Loss: 0.3084\n",
      "Epoch 83/150 Loss: 0.3012\n",
      "Epoch 84/150 Loss: 0.3041\n",
      "Epoch 85/150 Loss: 0.3086\n",
      "Epoch 86/150 Loss: 0.3045\n",
      "Epoch 87/150 Loss: 0.3073\n",
      "Epoch 88/150 Loss: 0.2966\n",
      "Epoch 89/150 Loss: 0.2963\n",
      "Epoch 90/150 Loss: 0.2965\n",
      "Epoch 91/150 Loss: 0.2849\n",
      "Epoch 92/150 Loss: 0.2947\n",
      "Epoch 93/150 Loss: 0.2957\n",
      "Epoch 94/150 Loss: 0.2991\n",
      "Epoch 95/150 Loss: 0.2931\n",
      "Epoch 96/150 Loss: 0.2994\n",
      "Epoch 97/150 Loss: 0.2850\n",
      "Epoch 98/150 Loss: 0.2895\n",
      "Epoch 99/150 Loss: 0.2898\n",
      "Epoch 100/150 Loss: 0.2849\n",
      "Epoch 101/150 Loss: 0.2867\n",
      "Epoch 102/150 Loss: 0.2797\n",
      "Epoch 103/150 Loss: 0.2845\n",
      "Epoch 104/150 Loss: 0.2881\n",
      "Epoch 105/150 Loss: 0.2892\n",
      "Epoch 106/150 Loss: 0.2789\n",
      "Epoch 107/150 Loss: 0.2853\n",
      "Epoch 108/150 Loss: 0.2799\n",
      "Epoch 109/150 Loss: 0.2789\n",
      "Epoch 110/150 Loss: 0.2842\n",
      "Epoch 111/150 Loss: 0.2781\n",
      "Epoch 112/150 Loss: 0.2813\n",
      "Epoch 113/150 Loss: 0.2756\n",
      "Epoch 114/150 Loss: 0.2773\n",
      "Epoch 115/150 Loss: 0.2705\n",
      "Epoch 116/150 Loss: 0.2659\n",
      "Epoch 117/150 Loss: 0.2609\n",
      "Epoch 118/150 Loss: 0.2711\n",
      "Epoch 119/150 Loss: 0.2735\n",
      "Epoch 120/150 Loss: 0.2667\n",
      "Epoch 121/150 Loss: 0.2660\n",
      "Epoch 122/150 Loss: 0.2646\n",
      "Epoch 123/150 Loss: 0.2750\n",
      "Epoch 124/150 Loss: 0.2649\n",
      "Epoch 125/150 Loss: 0.2636\n",
      "Epoch 126/150 Loss: 0.2622\n",
      "Epoch 127/150 Loss: 0.2622\n",
      "Epoch 128/150 Loss: 0.2630\n",
      "Epoch 129/150 Loss: 0.2697\n",
      "Epoch 130/150 Loss: 0.2648\n",
      "Epoch 131/150 Loss: 0.2620\n",
      "Epoch 132/150 Loss: 0.2580\n",
      "Epoch 133/150 Loss: 0.2584\n",
      "Epoch 134/150 Loss: 0.2590\n",
      "Epoch 135/150 Loss: 0.2587\n",
      "Epoch 136/150 Loss: 0.2601\n",
      "Epoch 137/150 Loss: 0.2606\n",
      "Epoch 138/150 Loss: 0.2581\n",
      "Epoch 139/150 Loss: 0.2571\n",
      "Epoch 140/150 Loss: 0.2540\n",
      "Epoch 141/150 Loss: 0.2601\n",
      "Epoch 142/150 Loss: 0.2576\n",
      "Epoch 143/150 Loss: 0.2581\n",
      "Epoch 144/150 Loss: 0.2533\n",
      "Epoch 145/150 Loss: 0.2538\n",
      "Epoch 146/150 Loss: 0.2583\n",
      "Epoch 147/150 Loss: 0.2520\n",
      "Epoch 148/150 Loss: 0.2527\n",
      "Epoch 149/150 Loss: 0.2487\n",
      "Epoch 150/150 Loss: 0.2545\n",
      "Config {'w1': 0.15, 'w2': 0.55, 'lambda_eng': 0.1, 'lr': 0.001} achieved Wasserstein: 0.8263\n",
      "\n",
      "=== Training with {'w1': 0.15, 'w2': 0.55, 'lambda_eng': 0.15, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.0404\n",
      "Epoch 2/150 Loss: 0.7673\n",
      "Epoch 3/150 Loss: 0.7269\n",
      "Epoch 4/150 Loss: 0.7024\n",
      "Epoch 5/150 Loss: 0.6571\n",
      "Epoch 6/150 Loss: 0.6360\n",
      "Epoch 7/150 Loss: 0.6123\n",
      "Epoch 8/150 Loss: 0.5855\n",
      "Epoch 9/150 Loss: 0.5713\n",
      "Epoch 10/150 Loss: 0.5481\n",
      "Epoch 11/150 Loss: 0.5350\n",
      "Epoch 12/150 Loss: 0.5250\n",
      "Epoch 13/150 Loss: 0.5119\n",
      "Epoch 14/150 Loss: 0.5014\n",
      "Epoch 15/150 Loss: 0.4916\n",
      "Epoch 16/150 Loss: 0.4880\n",
      "Epoch 17/150 Loss: 0.4799\n",
      "Epoch 18/150 Loss: 0.4660\n",
      "Epoch 19/150 Loss: 0.4655\n",
      "Epoch 20/150 Loss: 0.4533\n",
      "Epoch 21/150 Loss: 0.4566\n",
      "Epoch 22/150 Loss: 0.4452\n",
      "Epoch 23/150 Loss: 0.4424\n",
      "Epoch 24/150 Loss: 0.4437\n",
      "Epoch 25/150 Loss: 0.4301\n",
      "Epoch 26/150 Loss: 0.4288\n",
      "Epoch 27/150 Loss: 0.4231\n",
      "Epoch 28/150 Loss: 0.4204\n",
      "Epoch 29/150 Loss: 0.4166\n",
      "Epoch 30/150 Loss: 0.4062\n",
      "Epoch 31/150 Loss: 0.4023\n",
      "Epoch 32/150 Loss: 0.3937\n",
      "Epoch 33/150 Loss: 0.3917\n",
      "Epoch 34/150 Loss: 0.3867\n",
      "Epoch 35/150 Loss: 0.3857\n",
      "Epoch 36/150 Loss: 0.3921\n",
      "Epoch 37/150 Loss: 0.3797\n",
      "Epoch 38/150 Loss: 0.3884\n",
      "Epoch 39/150 Loss: 0.3838\n",
      "Epoch 40/150 Loss: 0.3762\n",
      "Epoch 41/150 Loss: 0.3656\n",
      "Epoch 42/150 Loss: 0.3669\n",
      "Epoch 43/150 Loss: 0.3611\n",
      "Epoch 44/150 Loss: 0.3622\n",
      "Epoch 45/150 Loss: 0.3594\n",
      "Epoch 46/150 Loss: 0.3597\n",
      "Epoch 47/150 Loss: 0.3590\n",
      "Epoch 48/150 Loss: 0.3576\n",
      "Epoch 49/150 Loss: 0.3566\n",
      "Epoch 50/150 Loss: 0.3490\n",
      "Epoch 51/150 Loss: 0.3458\n",
      "Epoch 52/150 Loss: 0.3468\n",
      "Epoch 53/150 Loss: 0.3307\n",
      "Epoch 54/150 Loss: 0.3318\n",
      "Epoch 55/150 Loss: 0.3446\n",
      "Epoch 56/150 Loss: 0.3392\n",
      "Epoch 57/150 Loss: 0.3304\n",
      "Epoch 58/150 Loss: 0.3339\n",
      "Epoch 59/150 Loss: 0.3349\n",
      "Epoch 60/150 Loss: 0.3365\n",
      "Epoch 61/150 Loss: 0.3272\n",
      "Epoch 62/150 Loss: 0.3340\n",
      "Epoch 63/150 Loss: 0.3282\n",
      "Epoch 64/150 Loss: 0.3301\n",
      "Epoch 65/150 Loss: 0.3248\n",
      "Epoch 66/150 Loss: 0.3138\n",
      "Epoch 67/150 Loss: 0.3186\n",
      "Epoch 68/150 Loss: 0.3179\n",
      "Epoch 69/150 Loss: 0.3196\n",
      "Epoch 70/150 Loss: 0.3152\n",
      "Epoch 71/150 Loss: 0.3102\n",
      "Epoch 72/150 Loss: 0.3155\n",
      "Epoch 73/150 Loss: 0.3261\n",
      "Epoch 74/150 Loss: 0.3126\n",
      "Epoch 75/150 Loss: 0.3139\n",
      "Epoch 76/150 Loss: 0.3099\n",
      "Epoch 77/150 Loss: 0.3097\n",
      "Epoch 78/150 Loss: 0.3065\n",
      "Epoch 79/150 Loss: 0.3089\n",
      "Epoch 80/150 Loss: 0.3015\n",
      "Epoch 81/150 Loss: 0.3045\n",
      "Epoch 82/150 Loss: 0.2978\n",
      "Epoch 83/150 Loss: 0.2988\n",
      "Epoch 84/150 Loss: 0.2956\n",
      "Epoch 85/150 Loss: 0.3049\n",
      "Epoch 86/150 Loss: 0.3023\n",
      "Epoch 87/150 Loss: 0.2949\n",
      "Epoch 88/150 Loss: 0.2944\n",
      "Epoch 89/150 Loss: 0.2957\n",
      "Epoch 90/150 Loss: 0.2955\n",
      "Epoch 91/150 Loss: 0.2922\n",
      "Epoch 92/150 Loss: 0.2914\n",
      "Epoch 93/150 Loss: 0.2897\n",
      "Epoch 94/150 Loss: 0.2939\n",
      "Epoch 95/150 Loss: 0.2811\n",
      "Epoch 96/150 Loss: 0.2901\n",
      "Epoch 97/150 Loss: 0.2858\n",
      "Epoch 98/150 Loss: 0.2868\n",
      "Epoch 99/150 Loss: 0.2829\n",
      "Epoch 100/150 Loss: 0.2893\n",
      "Epoch 101/150 Loss: 0.2872\n",
      "Epoch 102/150 Loss: 0.2845\n",
      "Epoch 103/150 Loss: 0.2832\n",
      "Epoch 104/150 Loss: 0.2829\n",
      "Epoch 105/150 Loss: 0.2785\n",
      "Epoch 106/150 Loss: 0.2828\n",
      "Epoch 107/150 Loss: 0.2777\n",
      "Epoch 108/150 Loss: 0.2826\n",
      "Epoch 109/150 Loss: 0.2778\n",
      "Epoch 110/150 Loss: 0.2753\n",
      "Epoch 111/150 Loss: 0.2705\n",
      "Epoch 112/150 Loss: 0.2729\n",
      "Epoch 113/150 Loss: 0.2722\n",
      "Epoch 114/150 Loss: 0.2746\n",
      "Epoch 115/150 Loss: 0.2775\n",
      "Epoch 116/150 Loss: 0.2714\n",
      "Epoch 117/150 Loss: 0.2716\n",
      "Epoch 118/150 Loss: 0.2724\n",
      "Epoch 119/150 Loss: 0.2702\n",
      "Epoch 120/150 Loss: 0.2720\n",
      "Epoch 121/150 Loss: 0.2698\n",
      "Epoch 122/150 Loss: 0.2672\n",
      "Epoch 123/150 Loss: 0.2649\n",
      "Epoch 124/150 Loss: 0.2627\n",
      "Epoch 125/150 Loss: 0.2742\n",
      "Epoch 126/150 Loss: 0.2721\n",
      "Epoch 127/150 Loss: 0.2712\n",
      "Epoch 128/150 Loss: 0.2634\n",
      "Epoch 129/150 Loss: 0.2648\n",
      "Epoch 130/150 Loss: 0.2643\n",
      "Epoch 131/150 Loss: 0.2644\n",
      "Epoch 132/150 Loss: 0.2613\n",
      "Epoch 133/150 Loss: 0.2633\n",
      "Epoch 134/150 Loss: 0.2603\n",
      "Epoch 135/150 Loss: 0.2608\n",
      "Epoch 136/150 Loss: 0.2565\n",
      "Epoch 137/150 Loss: 0.2614\n",
      "Epoch 138/150 Loss: 0.2630\n",
      "Epoch 139/150 Loss: 0.2573\n",
      "Epoch 140/150 Loss: 0.2560\n",
      "Epoch 141/150 Loss: 0.2612\n",
      "Epoch 142/150 Loss: 0.2561\n",
      "Epoch 143/150 Loss: 0.2591\n",
      "Epoch 144/150 Loss: 0.2560\n",
      "Epoch 145/150 Loss: 0.2561\n",
      "Epoch 146/150 Loss: 0.2546\n",
      "Epoch 147/150 Loss: 0.2600\n",
      "Epoch 148/150 Loss: 0.2598\n",
      "Epoch 149/150 Loss: 0.2577\n",
      "Epoch 150/150 Loss: 0.2531\n",
      "Config {'w1': 0.15, 'w2': 0.55, 'lambda_eng': 0.15, 'lr': 0.001} achieved Wasserstein: 0.7822\n",
      "\n",
      "=== Training with {'w1': 0.15, 'w2': 0.55, 'lambda_eng': 0.2, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.1162\n",
      "Epoch 2/150 Loss: 0.7861\n",
      "Epoch 3/150 Loss: 0.7316\n",
      "Epoch 4/150 Loss: 0.6982\n",
      "Epoch 5/150 Loss: 0.6594\n",
      "Epoch 6/150 Loss: 0.6275\n",
      "Epoch 7/150 Loss: 0.6150\n",
      "Epoch 8/150 Loss: 0.5844\n",
      "Epoch 9/150 Loss: 0.5674\n",
      "Epoch 10/150 Loss: 0.5616\n",
      "Epoch 11/150 Loss: 0.5470\n",
      "Epoch 12/150 Loss: 0.5363\n",
      "Epoch 13/150 Loss: 0.5309\n",
      "Epoch 14/150 Loss: 0.5196\n",
      "Epoch 15/150 Loss: 0.5110\n",
      "Epoch 16/150 Loss: 0.5000\n",
      "Epoch 17/150 Loss: 0.4978\n",
      "Epoch 18/150 Loss: 0.4912\n",
      "Epoch 19/150 Loss: 0.4987\n",
      "Epoch 20/150 Loss: 0.4709\n",
      "Epoch 21/150 Loss: 0.4647\n",
      "Epoch 22/150 Loss: 0.4668\n",
      "Epoch 23/150 Loss: 0.4555\n",
      "Epoch 24/150 Loss: 0.4535\n",
      "Epoch 25/150 Loss: 0.4663\n",
      "Epoch 26/150 Loss: 0.4490\n",
      "Epoch 27/150 Loss: 0.4423\n",
      "Epoch 28/150 Loss: 0.4409\n",
      "Epoch 29/150 Loss: 0.4459\n",
      "Epoch 30/150 Loss: 0.4239\n",
      "Epoch 31/150 Loss: 0.4281\n",
      "Epoch 32/150 Loss: 0.4238\n",
      "Epoch 33/150 Loss: 0.4165\n",
      "Epoch 34/150 Loss: 0.4231\n",
      "Epoch 35/150 Loss: 0.4138\n",
      "Epoch 36/150 Loss: 0.4051\n",
      "Epoch 37/150 Loss: 0.4066\n",
      "Epoch 38/150 Loss: 0.4033\n",
      "Epoch 39/150 Loss: 0.4042\n",
      "Epoch 40/150 Loss: 0.3891\n",
      "Epoch 41/150 Loss: 0.3987\n",
      "Epoch 42/150 Loss: 0.3945\n",
      "Epoch 43/150 Loss: 0.3851\n",
      "Epoch 44/150 Loss: 0.3864\n",
      "Epoch 45/150 Loss: 0.3838\n",
      "Epoch 46/150 Loss: 0.3921\n",
      "Epoch 47/150 Loss: 0.3834\n",
      "Epoch 48/150 Loss: 0.3772\n",
      "Epoch 49/150 Loss: 0.3818\n",
      "Epoch 50/150 Loss: 0.3709\n",
      "Epoch 51/150 Loss: 0.3704\n",
      "Epoch 52/150 Loss: 0.3745\n",
      "Epoch 53/150 Loss: 0.3735\n",
      "Epoch 54/150 Loss: 0.3738\n",
      "Epoch 55/150 Loss: 0.3549\n",
      "Epoch 56/150 Loss: 0.3615\n",
      "Epoch 57/150 Loss: 0.3587\n",
      "Epoch 58/150 Loss: 0.3564\n",
      "Epoch 59/150 Loss: 0.3600\n",
      "Epoch 60/150 Loss: 0.3567\n",
      "Epoch 61/150 Loss: 0.3580\n",
      "Epoch 62/150 Loss: 0.3539\n",
      "Epoch 63/150 Loss: 0.3588\n",
      "Epoch 64/150 Loss: 0.3451\n",
      "Epoch 65/150 Loss: 0.3546\n",
      "Epoch 66/150 Loss: 0.3393\n",
      "Epoch 67/150 Loss: 0.3457\n",
      "Epoch 68/150 Loss: 0.3559\n",
      "Epoch 69/150 Loss: 0.3354\n",
      "Epoch 70/150 Loss: 0.3407\n",
      "Epoch 71/150 Loss: 0.3347\n",
      "Epoch 72/150 Loss: 0.3362\n",
      "Epoch 73/150 Loss: 0.3334\n",
      "Epoch 74/150 Loss: 0.3282\n",
      "Epoch 75/150 Loss: 0.3309\n",
      "Epoch 76/150 Loss: 0.3353\n",
      "Epoch 77/150 Loss: 0.3290\n",
      "Epoch 78/150 Loss: 0.3261\n",
      "Epoch 79/150 Loss: 0.3347\n",
      "Epoch 80/150 Loss: 0.3239\n",
      "Epoch 81/150 Loss: 0.3190\n",
      "Epoch 82/150 Loss: 0.3249\n",
      "Epoch 83/150 Loss: 0.3233\n",
      "Epoch 84/150 Loss: 0.3162\n",
      "Epoch 85/150 Loss: 0.3178\n",
      "Epoch 86/150 Loss: 0.3173\n",
      "Epoch 87/150 Loss: 0.3069\n",
      "Epoch 88/150 Loss: 0.3102\n",
      "Epoch 89/150 Loss: 0.3134\n",
      "Epoch 90/150 Loss: 0.3097\n",
      "Epoch 91/150 Loss: 0.3085\n",
      "Epoch 92/150 Loss: 0.3045\n",
      "Epoch 93/150 Loss: 0.3138\n",
      "Epoch 94/150 Loss: 0.3042\n",
      "Epoch 95/150 Loss: 0.3061\n",
      "Epoch 96/150 Loss: 0.2999\n",
      "Epoch 97/150 Loss: 0.3044\n",
      "Epoch 98/150 Loss: 0.3002\n",
      "Epoch 99/150 Loss: 0.3005\n",
      "Epoch 100/150 Loss: 0.3041\n",
      "Epoch 101/150 Loss: 0.2970\n",
      "Epoch 102/150 Loss: 0.3010\n",
      "Epoch 103/150 Loss: 0.2949\n",
      "Epoch 104/150 Loss: 0.2936\n",
      "Epoch 105/150 Loss: 0.2856\n",
      "Epoch 106/150 Loss: 0.2964\n",
      "Epoch 107/150 Loss: 0.2915\n",
      "Epoch 108/150 Loss: 0.2916\n",
      "Epoch 109/150 Loss: 0.2861\n",
      "Epoch 110/150 Loss: 0.2938\n",
      "Epoch 111/150 Loss: 0.2894\n",
      "Epoch 112/150 Loss: 0.2891\n",
      "Epoch 113/150 Loss: 0.2865\n",
      "Epoch 114/150 Loss: 0.2790\n",
      "Epoch 115/150 Loss: 0.2789\n",
      "Epoch 116/150 Loss: 0.2817\n",
      "Epoch 117/150 Loss: 0.2799\n",
      "Epoch 118/150 Loss: 0.2765\n",
      "Epoch 119/150 Loss: 0.2853\n",
      "Epoch 120/150 Loss: 0.2851\n",
      "Epoch 121/150 Loss: 0.2820\n",
      "Epoch 122/150 Loss: 0.2826\n",
      "Epoch 123/150 Loss: 0.2727\n",
      "Epoch 124/150 Loss: 0.2762\n",
      "Epoch 125/150 Loss: 0.2763\n",
      "Epoch 126/150 Loss: 0.2707\n",
      "Epoch 127/150 Loss: 0.2785\n",
      "Epoch 128/150 Loss: 0.2792\n",
      "Epoch 129/150 Loss: 0.2742\n",
      "Epoch 130/150 Loss: 0.2733\n",
      "Epoch 131/150 Loss: 0.2777\n",
      "Epoch 132/150 Loss: 0.2728\n",
      "Epoch 133/150 Loss: 0.2732\n",
      "Epoch 134/150 Loss: 0.2669\n",
      "Epoch 135/150 Loss: 0.2729\n",
      "Epoch 136/150 Loss: 0.2767\n",
      "Epoch 137/150 Loss: 0.2808\n",
      "Epoch 138/150 Loss: 0.2748\n",
      "Epoch 139/150 Loss: 0.2677\n",
      "Epoch 140/150 Loss: 0.2687\n",
      "Epoch 141/150 Loss: 0.2655\n",
      "Epoch 142/150 Loss: 0.2695\n",
      "Epoch 143/150 Loss: 0.2665\n",
      "Epoch 144/150 Loss: 0.2678\n",
      "Epoch 145/150 Loss: 0.2631\n",
      "Epoch 146/150 Loss: 0.2627\n",
      "Epoch 147/150 Loss: 0.2646\n",
      "Epoch 148/150 Loss: 0.2628\n",
      "Epoch 149/150 Loss: 0.2591\n",
      "Epoch 150/150 Loss: 0.2648\n",
      "Config {'w1': 0.15, 'w2': 0.55, 'lambda_eng': 0.2, 'lr': 0.001} achieved Wasserstein: 0.7309\n",
      "\n",
      "=== Training with {'w1': 0.15, 'w2': 0.6, 'lambda_eng': 0.1, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 0.9806\n",
      "Epoch 2/150 Loss: 0.7300\n",
      "Epoch 3/150 Loss: 0.7122\n",
      "Epoch 4/150 Loss: 0.6741\n",
      "Epoch 5/150 Loss: 0.6307\n",
      "Epoch 6/150 Loss: 0.6218\n",
      "Epoch 7/150 Loss: 0.6055\n",
      "Epoch 8/150 Loss: 0.5728\n",
      "Epoch 9/150 Loss: 0.5693\n",
      "Epoch 10/150 Loss: 0.5466\n",
      "Epoch 11/150 Loss: 0.5483\n",
      "Epoch 12/150 Loss: 0.5284\n",
      "Epoch 13/150 Loss: 0.5148\n",
      "Epoch 14/150 Loss: 0.5125\n",
      "Epoch 15/150 Loss: 0.5013\n",
      "Epoch 16/150 Loss: 0.4996\n",
      "Epoch 17/150 Loss: 0.4869\n",
      "Epoch 18/150 Loss: 0.4721\n",
      "Epoch 19/150 Loss: 0.4691\n",
      "Epoch 20/150 Loss: 0.4657\n",
      "Epoch 21/150 Loss: 0.4599\n",
      "Epoch 22/150 Loss: 0.4460\n",
      "Epoch 23/150 Loss: 0.4483\n",
      "Epoch 24/150 Loss: 0.4303\n",
      "Epoch 25/150 Loss: 0.4393\n",
      "Epoch 26/150 Loss: 0.4216\n",
      "Epoch 27/150 Loss: 0.4237\n",
      "Epoch 28/150 Loss: 0.4153\n",
      "Epoch 29/150 Loss: 0.4084\n",
      "Epoch 30/150 Loss: 0.4030\n",
      "Epoch 31/150 Loss: 0.4109\n",
      "Epoch 32/150 Loss: 0.3956\n",
      "Epoch 33/150 Loss: 0.3907\n",
      "Epoch 34/150 Loss: 0.3887\n",
      "Epoch 35/150 Loss: 0.3907\n",
      "Epoch 36/150 Loss: 0.3883\n",
      "Epoch 37/150 Loss: 0.3803\n",
      "Epoch 38/150 Loss: 0.3784\n",
      "Epoch 39/150 Loss: 0.3812\n",
      "Epoch 40/150 Loss: 0.3714\n",
      "Epoch 41/150 Loss: 0.3738\n",
      "Epoch 42/150 Loss: 0.3627\n",
      "Epoch 43/150 Loss: 0.3670\n",
      "Epoch 44/150 Loss: 0.3683\n",
      "Epoch 45/150 Loss: 0.3663\n",
      "Epoch 46/150 Loss: 0.3600\n",
      "Epoch 47/150 Loss: 0.3603\n",
      "Epoch 48/150 Loss: 0.3531\n",
      "Epoch 49/150 Loss: 0.3649\n",
      "Epoch 50/150 Loss: 0.3568\n",
      "Epoch 51/150 Loss: 0.3574\n",
      "Epoch 52/150 Loss: 0.3479\n",
      "Epoch 53/150 Loss: 0.3494\n",
      "Epoch 54/150 Loss: 0.3446\n",
      "Epoch 55/150 Loss: 0.3392\n",
      "Epoch 56/150 Loss: 0.3437\n",
      "Epoch 57/150 Loss: 0.3344\n",
      "Epoch 58/150 Loss: 0.3424\n",
      "Epoch 59/150 Loss: 0.3346\n",
      "Epoch 60/150 Loss: 0.3351\n",
      "Epoch 61/150 Loss: 0.3345\n",
      "Epoch 62/150 Loss: 0.3295\n",
      "Epoch 63/150 Loss: 0.3306\n",
      "Epoch 64/150 Loss: 0.3286\n",
      "Epoch 65/150 Loss: 0.3251\n",
      "Epoch 66/150 Loss: 0.3178\n",
      "Epoch 67/150 Loss: 0.3231\n",
      "Epoch 68/150 Loss: 0.3164\n",
      "Epoch 69/150 Loss: 0.3232\n",
      "Epoch 70/150 Loss: 0.3146\n",
      "Epoch 71/150 Loss: 0.3174\n",
      "Epoch 72/150 Loss: 0.3185\n",
      "Epoch 73/150 Loss: 0.3155\n",
      "Epoch 74/150 Loss: 0.3152\n",
      "Epoch 75/150 Loss: 0.3073\n",
      "Epoch 76/150 Loss: 0.3146\n",
      "Epoch 77/150 Loss: 0.3107\n",
      "Epoch 78/150 Loss: 0.3095\n",
      "Epoch 79/150 Loss: 0.3066\n",
      "Epoch 80/150 Loss: 0.3070\n",
      "Epoch 81/150 Loss: 0.3015\n",
      "Epoch 82/150 Loss: 0.3029\n",
      "Epoch 83/150 Loss: 0.3017\n",
      "Epoch 84/150 Loss: 0.2965\n",
      "Epoch 85/150 Loss: 0.2962\n",
      "Epoch 86/150 Loss: 0.2956\n",
      "Epoch 87/150 Loss: 0.3007\n",
      "Epoch 88/150 Loss: 0.2933\n",
      "Epoch 89/150 Loss: 0.2910\n",
      "Epoch 90/150 Loss: 0.2996\n",
      "Epoch 91/150 Loss: 0.2902\n",
      "Epoch 92/150 Loss: 0.2858\n",
      "Epoch 93/150 Loss: 0.2891\n",
      "Epoch 94/150 Loss: 0.2963\n",
      "Epoch 95/150 Loss: 0.2894\n",
      "Epoch 96/150 Loss: 0.2920\n",
      "Epoch 97/150 Loss: 0.2835\n",
      "Epoch 98/150 Loss: 0.2772\n",
      "Epoch 99/150 Loss: 0.2848\n",
      "Epoch 100/150 Loss: 0.2795\n",
      "Epoch 101/150 Loss: 0.2836\n",
      "Epoch 102/150 Loss: 0.2789\n",
      "Epoch 103/150 Loss: 0.2772\n",
      "Epoch 104/150 Loss: 0.2825\n",
      "Epoch 105/150 Loss: 0.2794\n",
      "Epoch 106/150 Loss: 0.2750\n",
      "Epoch 107/150 Loss: 0.2763\n",
      "Epoch 108/150 Loss: 0.2776\n",
      "Epoch 109/150 Loss: 0.2756\n",
      "Epoch 110/150 Loss: 0.2732\n",
      "Epoch 111/150 Loss: 0.2739\n",
      "Epoch 112/150 Loss: 0.2744\n",
      "Epoch 113/150 Loss: 0.2634\n",
      "Epoch 114/150 Loss: 0.2774\n",
      "Epoch 115/150 Loss: 0.2750\n",
      "Epoch 116/150 Loss: 0.2706\n",
      "Epoch 117/150 Loss: 0.2656\n",
      "Epoch 118/150 Loss: 0.2710\n",
      "Epoch 119/150 Loss: 0.2652\n",
      "Epoch 120/150 Loss: 0.2689\n",
      "Epoch 121/150 Loss: 0.2644\n",
      "Epoch 122/150 Loss: 0.2657\n",
      "Epoch 123/150 Loss: 0.2597\n",
      "Epoch 124/150 Loss: 0.2667\n",
      "Epoch 125/150 Loss: 0.2633\n",
      "Epoch 126/150 Loss: 0.2650\n",
      "Epoch 127/150 Loss: 0.2587\n",
      "Epoch 128/150 Loss: 0.2668\n",
      "Epoch 129/150 Loss: 0.2584\n",
      "Epoch 130/150 Loss: 0.2560\n",
      "Epoch 131/150 Loss: 0.2518\n",
      "Epoch 132/150 Loss: 0.2544\n",
      "Epoch 133/150 Loss: 0.2553\n",
      "Epoch 134/150 Loss: 0.2602\n",
      "Epoch 135/150 Loss: 0.2578\n",
      "Epoch 136/150 Loss: 0.2553\n",
      "Epoch 137/150 Loss: 0.2589\n",
      "Epoch 138/150 Loss: 0.2460\n",
      "Epoch 139/150 Loss: 0.2568\n",
      "Epoch 140/150 Loss: 0.2550\n",
      "Epoch 141/150 Loss: 0.2559\n",
      "Epoch 142/150 Loss: 0.2535\n",
      "Epoch 143/150 Loss: 0.2545\n",
      "Epoch 144/150 Loss: 0.2503\n",
      "Epoch 145/150 Loss: 0.2499\n",
      "Epoch 146/150 Loss: 0.2501\n",
      "Epoch 147/150 Loss: 0.2549\n",
      "Epoch 148/150 Loss: 0.2499\n",
      "Epoch 149/150 Loss: 0.2454\n",
      "Epoch 150/150 Loss: 0.2488\n",
      "Config {'w1': 0.15, 'w2': 0.6, 'lambda_eng': 0.1, 'lr': 0.001} achieved Wasserstein: 0.7374\n",
      "\n",
      "=== Training with {'w1': 0.15, 'w2': 0.6, 'lambda_eng': 0.15, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.0506\n",
      "Epoch 2/150 Loss: 0.7556\n",
      "Epoch 3/150 Loss: 0.6795\n",
      "Epoch 4/150 Loss: 0.6584\n",
      "Epoch 5/150 Loss: 0.6205\n",
      "Epoch 6/150 Loss: 0.6179\n",
      "Epoch 7/150 Loss: 0.5861\n",
      "Epoch 8/150 Loss: 0.5700\n",
      "Epoch 9/150 Loss: 0.5542\n",
      "Epoch 10/150 Loss: 0.5486\n",
      "Epoch 11/150 Loss: 0.5361\n",
      "Epoch 12/150 Loss: 0.5247\n",
      "Epoch 13/150 Loss: 0.5238\n",
      "Epoch 14/150 Loss: 0.5097\n",
      "Epoch 15/150 Loss: 0.4945\n",
      "Epoch 16/150 Loss: 0.4850\n",
      "Epoch 17/150 Loss: 0.4916\n",
      "Epoch 18/150 Loss: 0.4775\n",
      "Epoch 19/150 Loss: 0.4726\n",
      "Epoch 20/150 Loss: 0.4613\n",
      "Epoch 21/150 Loss: 0.4660\n",
      "Epoch 22/150 Loss: 0.4467\n",
      "Epoch 23/150 Loss: 0.4579\n",
      "Epoch 24/150 Loss: 0.4416\n",
      "Epoch 25/150 Loss: 0.4431\n",
      "Epoch 26/150 Loss: 0.4286\n",
      "Epoch 27/150 Loss: 0.4263\n",
      "Epoch 28/150 Loss: 0.4294\n",
      "Epoch 29/150 Loss: 0.4170\n",
      "Epoch 30/150 Loss: 0.4086\n",
      "Epoch 31/150 Loss: 0.4126\n",
      "Epoch 32/150 Loss: 0.4107\n",
      "Epoch 33/150 Loss: 0.4121\n",
      "Epoch 34/150 Loss: 0.4040\n",
      "Epoch 35/150 Loss: 0.4067\n",
      "Epoch 36/150 Loss: 0.4038\n",
      "Epoch 37/150 Loss: 0.3921\n",
      "Epoch 38/150 Loss: 0.3953\n",
      "Epoch 39/150 Loss: 0.3897\n",
      "Epoch 40/150 Loss: 0.3895\n",
      "Epoch 41/150 Loss: 0.3840\n",
      "Epoch 42/150 Loss: 0.3792\n",
      "Epoch 43/150 Loss: 0.3740\n",
      "Epoch 44/150 Loss: 0.3711\n",
      "Epoch 45/150 Loss: 0.3718\n",
      "Epoch 46/150 Loss: 0.3699\n",
      "Epoch 47/150 Loss: 0.3765\n",
      "Epoch 48/150 Loss: 0.3692\n",
      "Epoch 49/150 Loss: 0.3605\n",
      "Epoch 50/150 Loss: 0.3633\n",
      "Epoch 51/150 Loss: 0.3529\n",
      "Epoch 52/150 Loss: 0.3573\n",
      "Epoch 53/150 Loss: 0.3594\n",
      "Epoch 54/150 Loss: 0.3516\n",
      "Epoch 55/150 Loss: 0.3503\n",
      "Epoch 56/150 Loss: 0.3446\n",
      "Epoch 57/150 Loss: 0.3531\n",
      "Epoch 58/150 Loss: 0.3421\n",
      "Epoch 59/150 Loss: 0.3483\n",
      "Epoch 60/150 Loss: 0.3487\n",
      "Epoch 61/150 Loss: 0.3401\n",
      "Epoch 62/150 Loss: 0.3372\n",
      "Epoch 63/150 Loss: 0.3395\n",
      "Epoch 64/150 Loss: 0.3355\n",
      "Epoch 65/150 Loss: 0.3322\n",
      "Epoch 66/150 Loss: 0.3336\n",
      "Epoch 67/150 Loss: 0.3369\n",
      "Epoch 68/150 Loss: 0.3243\n",
      "Epoch 69/150 Loss: 0.3313\n",
      "Epoch 70/150 Loss: 0.3231\n",
      "Epoch 71/150 Loss: 0.3277\n",
      "Epoch 72/150 Loss: 0.3233\n",
      "Epoch 73/150 Loss: 0.3303\n",
      "Epoch 74/150 Loss: 0.3252\n",
      "Epoch 75/150 Loss: 0.3194\n",
      "Epoch 76/150 Loss: 0.3231\n",
      "Epoch 77/150 Loss: 0.3211\n",
      "Epoch 78/150 Loss: 0.3274\n",
      "Epoch 79/150 Loss: 0.3189\n",
      "Epoch 80/150 Loss: 0.3223\n",
      "Epoch 81/150 Loss: 0.3169\n",
      "Epoch 82/150 Loss: 0.3156\n",
      "Epoch 83/150 Loss: 0.3171\n",
      "Epoch 84/150 Loss: 0.3124\n",
      "Epoch 85/150 Loss: 0.3027\n",
      "Epoch 86/150 Loss: 0.3018\n",
      "Epoch 87/150 Loss: 0.3100\n",
      "Epoch 88/150 Loss: 0.3020\n",
      "Epoch 89/150 Loss: 0.3057\n",
      "Epoch 90/150 Loss: 0.3105\n",
      "Epoch 91/150 Loss: 0.3064\n",
      "Epoch 92/150 Loss: 0.3067\n",
      "Epoch 93/150 Loss: 0.3023\n",
      "Epoch 94/150 Loss: 0.2987\n",
      "Epoch 95/150 Loss: 0.3007\n",
      "Epoch 96/150 Loss: 0.3016\n",
      "Epoch 97/150 Loss: 0.2994\n",
      "Epoch 98/150 Loss: 0.2939\n",
      "Epoch 99/150 Loss: 0.2929\n",
      "Epoch 100/150 Loss: 0.2878\n",
      "Epoch 101/150 Loss: 0.2858\n",
      "Epoch 102/150 Loss: 0.2875\n",
      "Epoch 103/150 Loss: 0.2900\n",
      "Epoch 104/150 Loss: 0.2894\n",
      "Epoch 105/150 Loss: 0.2961\n",
      "Epoch 106/150 Loss: 0.2795\n",
      "Epoch 107/150 Loss: 0.2879\n",
      "Epoch 108/150 Loss: 0.2896\n",
      "Epoch 109/150 Loss: 0.2894\n",
      "Epoch 110/150 Loss: 0.2861\n",
      "Epoch 111/150 Loss: 0.2850\n",
      "Epoch 112/150 Loss: 0.2907\n",
      "Epoch 113/150 Loss: 0.2844\n",
      "Epoch 114/150 Loss: 0.2786\n",
      "Epoch 115/150 Loss: 0.2856\n",
      "Epoch 116/150 Loss: 0.2759\n",
      "Epoch 117/150 Loss: 0.2772\n",
      "Epoch 118/150 Loss: 0.2813\n",
      "Epoch 119/150 Loss: 0.2753\n",
      "Epoch 120/150 Loss: 0.2761\n",
      "Epoch 121/150 Loss: 0.2751\n",
      "Epoch 122/150 Loss: 0.2733\n",
      "Epoch 123/150 Loss: 0.2764\n",
      "Epoch 124/150 Loss: 0.2759\n",
      "Epoch 125/150 Loss: 0.2727\n",
      "Epoch 126/150 Loss: 0.2707\n",
      "Epoch 127/150 Loss: 0.2672\n",
      "Epoch 128/150 Loss: 0.2655\n",
      "Epoch 129/150 Loss: 0.2734\n",
      "Epoch 130/150 Loss: 0.2693\n",
      "Epoch 131/150 Loss: 0.2663\n",
      "Epoch 132/150 Loss: 0.2693\n",
      "Epoch 133/150 Loss: 0.2700\n",
      "Epoch 134/150 Loss: 0.2652\n",
      "Epoch 135/150 Loss: 0.2615\n",
      "Epoch 136/150 Loss: 0.2655\n",
      "Epoch 137/150 Loss: 0.2627\n",
      "Epoch 138/150 Loss: 0.2639\n",
      "Epoch 139/150 Loss: 0.2639\n",
      "Epoch 140/150 Loss: 0.2722\n",
      "Epoch 141/150 Loss: 0.2650\n",
      "Epoch 142/150 Loss: 0.2674\n",
      "Epoch 143/150 Loss: 0.2626\n",
      "Epoch 144/150 Loss: 0.2564\n",
      "Epoch 145/150 Loss: 0.2619\n",
      "Epoch 146/150 Loss: 0.2618\n",
      "Epoch 147/150 Loss: 0.2584\n",
      "Epoch 148/150 Loss: 0.2571\n",
      "Epoch 149/150 Loss: 0.2538\n",
      "Epoch 150/150 Loss: 0.2675\n",
      "Config {'w1': 0.15, 'w2': 0.6, 'lambda_eng': 0.15, 'lr': 0.001} achieved Wasserstein: 0.6890\n",
      "\n",
      "=== Training with {'w1': 0.15, 'w2': 0.6, 'lambda_eng': 0.2, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.1500\n",
      "Epoch 2/150 Loss: 0.7885\n",
      "Epoch 3/150 Loss: 0.7196\n",
      "Epoch 4/150 Loss: 0.6940\n",
      "Epoch 5/150 Loss: 0.6669\n",
      "Epoch 6/150 Loss: 0.6339\n",
      "Epoch 7/150 Loss: 0.6128\n",
      "Epoch 8/150 Loss: 0.6109\n",
      "Epoch 9/150 Loss: 0.5836\n",
      "Epoch 10/150 Loss: 0.5724\n",
      "Epoch 11/150 Loss: 0.5638\n",
      "Epoch 12/150 Loss: 0.5546\n",
      "Epoch 13/150 Loss: 0.5410\n",
      "Epoch 14/150 Loss: 0.5350\n",
      "Epoch 15/150 Loss: 0.5259\n",
      "Epoch 16/150 Loss: 0.5069\n",
      "Epoch 17/150 Loss: 0.5000\n",
      "Epoch 18/150 Loss: 0.4963\n",
      "Epoch 19/150 Loss: 0.4804\n",
      "Epoch 20/150 Loss: 0.4876\n",
      "Epoch 21/150 Loss: 0.4669\n",
      "Epoch 22/150 Loss: 0.4759\n",
      "Epoch 23/150 Loss: 0.4634\n",
      "Epoch 24/150 Loss: 0.4459\n",
      "Epoch 25/150 Loss: 0.4554\n",
      "Epoch 26/150 Loss: 0.4450\n",
      "Epoch 27/150 Loss: 0.4514\n",
      "Epoch 28/150 Loss: 0.4381\n",
      "Epoch 29/150 Loss: 0.4394\n",
      "Epoch 30/150 Loss: 0.4258\n",
      "Epoch 31/150 Loss: 0.4266\n",
      "Epoch 32/150 Loss: 0.4239\n",
      "Epoch 33/150 Loss: 0.4194\n",
      "Epoch 34/150 Loss: 0.4235\n",
      "Epoch 35/150 Loss: 0.4087\n",
      "Epoch 36/150 Loss: 0.4120\n",
      "Epoch 37/150 Loss: 0.3979\n",
      "Epoch 38/150 Loss: 0.3944\n",
      "Epoch 39/150 Loss: 0.3985\n",
      "Epoch 40/150 Loss: 0.3940\n",
      "Epoch 41/150 Loss: 0.4017\n",
      "Epoch 42/150 Loss: 0.3932\n",
      "Epoch 43/150 Loss: 0.3875\n",
      "Epoch 44/150 Loss: 0.3914\n",
      "Epoch 45/150 Loss: 0.3849\n",
      "Epoch 46/150 Loss: 0.3767\n",
      "Epoch 47/150 Loss: 0.3810\n",
      "Epoch 48/150 Loss: 0.3878\n",
      "Epoch 49/150 Loss: 0.3845\n",
      "Epoch 50/150 Loss: 0.3746\n",
      "Epoch 51/150 Loss: 0.3776\n",
      "Epoch 52/150 Loss: 0.3706\n",
      "Epoch 53/150 Loss: 0.3722\n",
      "Epoch 54/150 Loss: 0.3659\n",
      "Epoch 55/150 Loss: 0.3660\n",
      "Epoch 56/150 Loss: 0.3581\n",
      "Epoch 57/150 Loss: 0.3587\n",
      "Epoch 58/150 Loss: 0.3563\n",
      "Epoch 59/150 Loss: 0.3599\n",
      "Epoch 60/150 Loss: 0.3504\n",
      "Epoch 61/150 Loss: 0.3486\n",
      "Epoch 62/150 Loss: 0.3595\n",
      "Epoch 63/150 Loss: 0.3607\n",
      "Epoch 64/150 Loss: 0.3509\n",
      "Epoch 65/150 Loss: 0.3482\n",
      "Epoch 66/150 Loss: 0.3443\n",
      "Epoch 67/150 Loss: 0.3438\n",
      "Epoch 68/150 Loss: 0.3335\n",
      "Epoch 69/150 Loss: 0.3384\n",
      "Epoch 70/150 Loss: 0.3394\n",
      "Epoch 71/150 Loss: 0.3312\n",
      "Epoch 72/150 Loss: 0.3303\n",
      "Epoch 73/150 Loss: 0.3357\n",
      "Epoch 74/150 Loss: 0.3350\n",
      "Epoch 75/150 Loss: 0.3295\n",
      "Epoch 76/150 Loss: 0.3294\n",
      "Epoch 77/150 Loss: 0.3222\n",
      "Epoch 78/150 Loss: 0.3254\n",
      "Epoch 79/150 Loss: 0.3170\n",
      "Epoch 80/150 Loss: 0.3243\n",
      "Epoch 81/150 Loss: 0.3212\n",
      "Epoch 82/150 Loss: 0.3207\n",
      "Epoch 83/150 Loss: 0.3231\n",
      "Epoch 84/150 Loss: 0.3154\n",
      "Epoch 85/150 Loss: 0.3285\n",
      "Epoch 86/150 Loss: 0.3153\n",
      "Epoch 87/150 Loss: 0.3101\n",
      "Epoch 88/150 Loss: 0.3128\n",
      "Epoch 89/150 Loss: 0.3067\n",
      "Epoch 90/150 Loss: 0.3117\n",
      "Epoch 91/150 Loss: 0.3153\n",
      "Epoch 92/150 Loss: 0.3123\n",
      "Epoch 93/150 Loss: 0.3031\n",
      "Epoch 94/150 Loss: 0.2972\n",
      "Epoch 95/150 Loss: 0.3080\n",
      "Epoch 96/150 Loss: 0.3066\n",
      "Epoch 97/150 Loss: 0.3042\n",
      "Epoch 98/150 Loss: 0.3056\n",
      "Epoch 99/150 Loss: 0.2940\n",
      "Epoch 100/150 Loss: 0.2985\n",
      "Epoch 101/150 Loss: 0.3019\n",
      "Epoch 102/150 Loss: 0.3025\n",
      "Epoch 103/150 Loss: 0.2935\n",
      "Epoch 104/150 Loss: 0.3048\n",
      "Epoch 105/150 Loss: 0.2975\n",
      "Epoch 106/150 Loss: 0.2990\n",
      "Epoch 107/150 Loss: 0.2932\n",
      "Epoch 108/150 Loss: 0.2913\n",
      "Epoch 109/150 Loss: 0.2839\n",
      "Epoch 110/150 Loss: 0.2920\n",
      "Epoch 111/150 Loss: 0.2878\n",
      "Epoch 112/150 Loss: 0.2878\n",
      "Epoch 113/150 Loss: 0.2871\n",
      "Epoch 114/150 Loss: 0.2821\n",
      "Epoch 115/150 Loss: 0.2808\n",
      "Epoch 116/150 Loss: 0.2841\n",
      "Epoch 117/150 Loss: 0.2757\n",
      "Epoch 118/150 Loss: 0.2780\n",
      "Epoch 119/150 Loss: 0.2842\n",
      "Epoch 120/150 Loss: 0.2829\n",
      "Epoch 121/150 Loss: 0.2821\n",
      "Epoch 122/150 Loss: 0.2761\n",
      "Epoch 123/150 Loss: 0.2799\n",
      "Epoch 124/150 Loss: 0.2715\n",
      "Epoch 125/150 Loss: 0.2714\n",
      "Epoch 126/150 Loss: 0.2713\n",
      "Epoch 127/150 Loss: 0.2730\n",
      "Epoch 128/150 Loss: 0.2781\n",
      "Epoch 129/150 Loss: 0.2723\n",
      "Epoch 130/150 Loss: 0.2810\n",
      "Epoch 131/150 Loss: 0.2760\n",
      "Epoch 132/150 Loss: 0.2753\n",
      "Epoch 133/150 Loss: 0.2737\n",
      "Epoch 134/150 Loss: 0.2650\n",
      "Epoch 135/150 Loss: 0.2639\n",
      "Epoch 136/150 Loss: 0.2681\n",
      "Epoch 137/150 Loss: 0.2672\n",
      "Epoch 138/150 Loss: 0.2630\n",
      "Epoch 139/150 Loss: 0.2671\n",
      "Epoch 140/150 Loss: 0.2776\n",
      "Epoch 141/150 Loss: 0.2563\n",
      "Epoch 142/150 Loss: 0.2668\n",
      "Epoch 143/150 Loss: 0.2650\n",
      "Epoch 144/150 Loss: 0.2638\n",
      "Epoch 145/150 Loss: 0.2606\n",
      "Epoch 146/150 Loss: 0.2577\n",
      "Epoch 147/150 Loss: 0.2628\n",
      "Epoch 148/150 Loss: 0.2598\n",
      "Epoch 149/150 Loss: 0.2623\n",
      "Epoch 150/150 Loss: 0.2583\n",
      "Config {'w1': 0.15, 'w2': 0.6, 'lambda_eng': 0.2, 'lr': 0.001} achieved Wasserstein: 0.7449\n",
      "\n",
      "=== Training with {'w1': 0.2, 'w2': 0.5, 'lambda_eng': 0.1, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 0.9444\n",
      "Epoch 2/150 Loss: 0.7200\n",
      "Epoch 3/150 Loss: 0.6756\n",
      "Epoch 4/150 Loss: 0.6349\n",
      "Epoch 5/150 Loss: 0.5961\n",
      "Epoch 6/150 Loss: 0.5875\n",
      "Epoch 7/150 Loss: 0.5567\n",
      "Epoch 8/150 Loss: 0.5533\n",
      "Epoch 9/150 Loss: 0.5433\n",
      "Epoch 10/150 Loss: 0.5255\n",
      "Epoch 11/150 Loss: 0.5191\n",
      "Epoch 12/150 Loss: 0.5058\n",
      "Epoch 13/150 Loss: 0.4900\n",
      "Epoch 14/150 Loss: 0.4842\n",
      "Epoch 15/150 Loss: 0.4904\n",
      "Epoch 16/150 Loss: 0.4766\n",
      "Epoch 17/150 Loss: 0.4694\n",
      "Epoch 18/150 Loss: 0.4611\n",
      "Epoch 19/150 Loss: 0.4495\n",
      "Epoch 20/150 Loss: 0.4513\n",
      "Epoch 21/150 Loss: 0.4457\n",
      "Epoch 22/150 Loss: 0.4378\n",
      "Epoch 23/150 Loss: 0.4402\n",
      "Epoch 24/150 Loss: 0.4255\n",
      "Epoch 25/150 Loss: 0.4257\n",
      "Epoch 26/150 Loss: 0.4216\n",
      "Epoch 27/150 Loss: 0.4126\n",
      "Epoch 28/150 Loss: 0.4140\n",
      "Epoch 29/150 Loss: 0.4116\n",
      "Epoch 30/150 Loss: 0.4074\n",
      "Epoch 31/150 Loss: 0.4110\n",
      "Epoch 32/150 Loss: 0.4037\n",
      "Epoch 33/150 Loss: 0.3950\n",
      "Epoch 34/150 Loss: 0.3938\n",
      "Epoch 35/150 Loss: 0.3829\n",
      "Epoch 36/150 Loss: 0.3864\n",
      "Epoch 37/150 Loss: 0.3799\n",
      "Epoch 38/150 Loss: 0.3847\n",
      "Epoch 39/150 Loss: 0.3849\n",
      "Epoch 40/150 Loss: 0.3817\n",
      "Epoch 41/150 Loss: 0.3757\n",
      "Epoch 42/150 Loss: 0.3706\n",
      "Epoch 43/150 Loss: 0.3685\n",
      "Epoch 44/150 Loss: 0.3730\n",
      "Epoch 45/150 Loss: 0.3691\n",
      "Epoch 46/150 Loss: 0.3656\n",
      "Epoch 47/150 Loss: 0.3512\n",
      "Epoch 48/150 Loss: 0.3606\n",
      "Epoch 49/150 Loss: 0.3739\n",
      "Epoch 50/150 Loss: 0.3658\n",
      "Epoch 51/150 Loss: 0.3635\n",
      "Epoch 52/150 Loss: 0.3561\n",
      "Epoch 53/150 Loss: 0.3619\n",
      "Epoch 54/150 Loss: 0.3526\n",
      "Epoch 55/150 Loss: 0.3504\n",
      "Epoch 56/150 Loss: 0.3526\n",
      "Epoch 57/150 Loss: 0.3493\n",
      "Epoch 58/150 Loss: 0.3496\n",
      "Epoch 59/150 Loss: 0.3486\n",
      "Epoch 60/150 Loss: 0.3478\n",
      "Epoch 61/150 Loss: 0.3439\n",
      "Epoch 62/150 Loss: 0.3386\n",
      "Epoch 63/150 Loss: 0.3377\n",
      "Epoch 64/150 Loss: 0.3381\n",
      "Epoch 65/150 Loss: 0.3359\n",
      "Epoch 66/150 Loss: 0.3415\n",
      "Epoch 67/150 Loss: 0.3365\n",
      "Epoch 68/150 Loss: 0.3372\n",
      "Epoch 69/150 Loss: 0.3417\n",
      "Epoch 70/150 Loss: 0.3283\n",
      "Epoch 71/150 Loss: 0.3310\n",
      "Epoch 72/150 Loss: 0.3266\n",
      "Epoch 73/150 Loss: 0.3253\n",
      "Epoch 74/150 Loss: 0.3257\n",
      "Epoch 75/150 Loss: 0.3245\n",
      "Epoch 76/150 Loss: 0.3199\n",
      "Epoch 77/150 Loss: 0.3237\n",
      "Epoch 78/150 Loss: 0.3248\n",
      "Epoch 79/150 Loss: 0.3226\n",
      "Epoch 80/150 Loss: 0.3187\n",
      "Epoch 81/150 Loss: 0.3221\n",
      "Epoch 82/150 Loss: 0.3131\n",
      "Epoch 83/150 Loss: 0.3111\n",
      "Epoch 84/150 Loss: 0.3145\n",
      "Epoch 85/150 Loss: 0.3088\n",
      "Epoch 86/150 Loss: 0.3129\n",
      "Epoch 87/150 Loss: 0.3129\n",
      "Epoch 88/150 Loss: 0.3147\n",
      "Epoch 89/150 Loss: 0.3128\n",
      "Epoch 90/150 Loss: 0.3083\n",
      "Epoch 91/150 Loss: 0.3035\n",
      "Epoch 92/150 Loss: 0.3081\n",
      "Epoch 93/150 Loss: 0.3095\n",
      "Epoch 94/150 Loss: 0.3154\n",
      "Epoch 95/150 Loss: 0.3035\n",
      "Epoch 96/150 Loss: 0.3052\n",
      "Epoch 97/150 Loss: 0.3000\n",
      "Epoch 98/150 Loss: 0.3027\n",
      "Epoch 99/150 Loss: 0.3059\n",
      "Epoch 100/150 Loss: 0.2993\n",
      "Epoch 101/150 Loss: 0.3022\n",
      "Epoch 102/150 Loss: 0.2936\n",
      "Epoch 103/150 Loss: 0.2976\n",
      "Epoch 104/150 Loss: 0.2975\n",
      "Epoch 105/150 Loss: 0.2982\n",
      "Epoch 106/150 Loss: 0.2901\n",
      "Epoch 107/150 Loss: 0.2959\n",
      "Epoch 108/150 Loss: 0.2921\n",
      "Epoch 109/150 Loss: 0.2871\n",
      "Epoch 110/150 Loss: 0.2888\n",
      "Epoch 111/150 Loss: 0.2855\n",
      "Epoch 112/150 Loss: 0.2877\n",
      "Epoch 113/150 Loss: 0.2873\n",
      "Epoch 114/150 Loss: 0.2909\n",
      "Epoch 115/150 Loss: 0.2853\n",
      "Epoch 116/150 Loss: 0.2796\n",
      "Epoch 117/150 Loss: 0.2839\n",
      "Epoch 118/150 Loss: 0.2859\n",
      "Epoch 119/150 Loss: 0.2807\n",
      "Epoch 120/150 Loss: 0.2855\n",
      "Epoch 121/150 Loss: 0.2860\n",
      "Epoch 122/150 Loss: 0.2813\n",
      "Epoch 123/150 Loss: 0.2819\n",
      "Epoch 124/150 Loss: 0.2848\n",
      "Epoch 125/150 Loss: 0.2818\n",
      "Epoch 126/150 Loss: 0.2770\n",
      "Epoch 127/150 Loss: 0.2759\n",
      "Epoch 128/150 Loss: 0.2837\n",
      "Epoch 129/150 Loss: 0.2777\n",
      "Epoch 130/150 Loss: 0.2824\n",
      "Epoch 131/150 Loss: 0.2790\n",
      "Epoch 132/150 Loss: 0.2809\n",
      "Epoch 133/150 Loss: 0.2756\n",
      "Epoch 134/150 Loss: 0.2769\n",
      "Epoch 135/150 Loss: 0.2772\n",
      "Epoch 136/150 Loss: 0.2748\n",
      "Epoch 137/150 Loss: 0.2749\n",
      "Epoch 138/150 Loss: 0.2734\n",
      "Epoch 139/150 Loss: 0.2793\n",
      "Epoch 140/150 Loss: 0.2740\n",
      "Epoch 141/150 Loss: 0.2719\n",
      "Epoch 142/150 Loss: 0.2756\n",
      "Epoch 143/150 Loss: 0.2698\n",
      "Epoch 144/150 Loss: 0.2705\n",
      "Epoch 145/150 Loss: 0.2701\n",
      "Epoch 146/150 Loss: 0.2716\n",
      "Epoch 147/150 Loss: 0.2714\n",
      "Epoch 148/150 Loss: 0.2672\n",
      "Epoch 149/150 Loss: 0.2662\n",
      "Epoch 150/150 Loss: 0.2618\n",
      "Config {'w1': 0.2, 'w2': 0.5, 'lambda_eng': 0.1, 'lr': 0.001} achieved Wasserstein: 0.6617\n",
      "\n",
      "=== Training with {'w1': 0.2, 'w2': 0.5, 'lambda_eng': 0.15, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.0360\n",
      "Epoch 2/150 Loss: 0.7406\n",
      "Epoch 3/150 Loss: 0.6910\n",
      "Epoch 4/150 Loss: 0.6661\n",
      "Epoch 5/150 Loss: 0.6186\n",
      "Epoch 6/150 Loss: 0.6026\n",
      "Epoch 7/150 Loss: 0.5892\n",
      "Epoch 8/150 Loss: 0.5764\n",
      "Epoch 9/150 Loss: 0.5542\n",
      "Epoch 10/150 Loss: 0.5437\n",
      "Epoch 11/150 Loss: 0.5363\n",
      "Epoch 12/150 Loss: 0.5223\n",
      "Epoch 13/150 Loss: 0.5142\n",
      "Epoch 14/150 Loss: 0.5012\n",
      "Epoch 15/150 Loss: 0.4986\n",
      "Epoch 16/150 Loss: 0.4891\n",
      "Epoch 17/150 Loss: 0.4780\n",
      "Epoch 18/150 Loss: 0.4795\n",
      "Epoch 19/150 Loss: 0.4693\n",
      "Epoch 20/150 Loss: 0.4774\n",
      "Epoch 21/150 Loss: 0.4634\n",
      "Epoch 22/150 Loss: 0.4553\n",
      "Epoch 23/150 Loss: 0.4504\n",
      "Epoch 24/150 Loss: 0.4411\n",
      "Epoch 25/150 Loss: 0.4388\n",
      "Epoch 26/150 Loss: 0.4420\n",
      "Epoch 27/150 Loss: 0.4280\n",
      "Epoch 28/150 Loss: 0.4272\n",
      "Epoch 29/150 Loss: 0.4196\n",
      "Epoch 30/150 Loss: 0.4198\n",
      "Epoch 31/150 Loss: 0.4192\n",
      "Epoch 32/150 Loss: 0.4167\n",
      "Epoch 33/150 Loss: 0.4050\n",
      "Epoch 34/150 Loss: 0.4061\n",
      "Epoch 35/150 Loss: 0.4017\n",
      "Epoch 36/150 Loss: 0.4009\n",
      "Epoch 37/150 Loss: 0.3976\n",
      "Epoch 38/150 Loss: 0.3957\n",
      "Epoch 39/150 Loss: 0.3942\n",
      "Epoch 40/150 Loss: 0.3856\n",
      "Epoch 41/150 Loss: 0.3936\n",
      "Epoch 42/150 Loss: 0.3812\n",
      "Epoch 43/150 Loss: 0.3777\n",
      "Epoch 44/150 Loss: 0.3737\n",
      "Epoch 45/150 Loss: 0.3820\n",
      "Epoch 46/150 Loss: 0.3731\n",
      "Epoch 47/150 Loss: 0.3660\n",
      "Epoch 48/150 Loss: 0.3746\n",
      "Epoch 49/150 Loss: 0.3763\n",
      "Epoch 50/150 Loss: 0.3646\n",
      "Epoch 51/150 Loss: 0.3569\n",
      "Epoch 52/150 Loss: 0.3623\n",
      "Epoch 53/150 Loss: 0.3620\n",
      "Epoch 54/150 Loss: 0.3645\n",
      "Epoch 55/150 Loss: 0.3544\n",
      "Epoch 56/150 Loss: 0.3575\n",
      "Epoch 57/150 Loss: 0.3562\n",
      "Epoch 58/150 Loss: 0.3543\n",
      "Epoch 59/150 Loss: 0.3538\n",
      "Epoch 60/150 Loss: 0.3474\n",
      "Epoch 61/150 Loss: 0.3490\n",
      "Epoch 62/150 Loss: 0.3484\n",
      "Epoch 63/150 Loss: 0.3523\n",
      "Epoch 64/150 Loss: 0.3402\n",
      "Epoch 65/150 Loss: 0.3447\n",
      "Epoch 66/150 Loss: 0.3391\n",
      "Epoch 67/150 Loss: 0.3424\n",
      "Epoch 68/150 Loss: 0.3376\n",
      "Epoch 69/150 Loss: 0.3384\n",
      "Epoch 70/150 Loss: 0.3386\n",
      "Epoch 71/150 Loss: 0.3298\n",
      "Epoch 72/150 Loss: 0.3349\n",
      "Epoch 73/150 Loss: 0.3341\n",
      "Epoch 74/150 Loss: 0.3302\n",
      "Epoch 75/150 Loss: 0.3340\n",
      "Epoch 76/150 Loss: 0.3252\n",
      "Epoch 77/150 Loss: 0.3252\n",
      "Epoch 78/150 Loss: 0.3306\n",
      "Epoch 79/150 Loss: 0.3292\n",
      "Epoch 80/150 Loss: 0.3267\n",
      "Epoch 81/150 Loss: 0.3267\n",
      "Epoch 82/150 Loss: 0.3229\n",
      "Epoch 83/150 Loss: 0.3155\n",
      "Epoch 84/150 Loss: 0.3197\n",
      "Epoch 85/150 Loss: 0.3188\n",
      "Epoch 86/150 Loss: 0.3233\n",
      "Epoch 87/150 Loss: 0.3127\n",
      "Epoch 88/150 Loss: 0.3188\n",
      "Epoch 89/150 Loss: 0.3184\n",
      "Epoch 90/150 Loss: 0.3152\n",
      "Epoch 91/150 Loss: 0.3145\n",
      "Epoch 92/150 Loss: 0.3078\n",
      "Epoch 93/150 Loss: 0.3085\n",
      "Epoch 94/150 Loss: 0.3088\n",
      "Epoch 95/150 Loss: 0.3050\n",
      "Epoch 96/150 Loss: 0.3026\n",
      "Epoch 97/150 Loss: 0.3099\n",
      "Epoch 98/150 Loss: 0.3067\n",
      "Epoch 99/150 Loss: 0.3043\n",
      "Epoch 100/150 Loss: 0.3052\n",
      "Epoch 101/150 Loss: 0.2966\n",
      "Epoch 102/150 Loss: 0.3047\n",
      "Epoch 103/150 Loss: 0.2976\n",
      "Epoch 104/150 Loss: 0.3038\n",
      "Epoch 105/150 Loss: 0.2905\n",
      "Epoch 106/150 Loss: 0.2997\n",
      "Epoch 107/150 Loss: 0.2967\n",
      "Epoch 108/150 Loss: 0.2939\n",
      "Epoch 109/150 Loss: 0.2929\n",
      "Epoch 110/150 Loss: 0.2873\n",
      "Epoch 111/150 Loss: 0.2865\n",
      "Epoch 112/150 Loss: 0.2991\n",
      "Epoch 113/150 Loss: 0.2909\n",
      "Epoch 114/150 Loss: 0.2951\n",
      "Epoch 115/150 Loss: 0.2899\n",
      "Epoch 116/150 Loss: 0.2942\n",
      "Epoch 117/150 Loss: 0.2861\n",
      "Epoch 118/150 Loss: 0.2925\n",
      "Epoch 119/150 Loss: 0.2871\n",
      "Epoch 120/150 Loss: 0.2898\n",
      "Epoch 121/150 Loss: 0.2837\n",
      "Epoch 122/150 Loss: 0.2863\n",
      "Epoch 123/150 Loss: 0.2845\n",
      "Epoch 124/150 Loss: 0.2815\n",
      "Epoch 125/150 Loss: 0.2840\n",
      "Epoch 126/150 Loss: 0.2860\n",
      "Epoch 127/150 Loss: 0.2845\n",
      "Epoch 128/150 Loss: 0.2818\n",
      "Epoch 129/150 Loss: 0.2757\n",
      "Epoch 130/150 Loss: 0.2833\n",
      "Epoch 131/150 Loss: 0.2824\n",
      "Epoch 132/150 Loss: 0.2805\n",
      "Epoch 133/150 Loss: 0.2764\n",
      "Epoch 134/150 Loss: 0.2790\n",
      "Epoch 135/150 Loss: 0.2779\n",
      "Epoch 136/150 Loss: 0.2797\n",
      "Epoch 137/150 Loss: 0.2756\n",
      "Epoch 138/150 Loss: 0.2815\n",
      "Epoch 139/150 Loss: 0.2774\n",
      "Epoch 140/150 Loss: 0.2690\n",
      "Epoch 141/150 Loss: 0.2781\n",
      "Epoch 142/150 Loss: 0.2803\n",
      "Epoch 143/150 Loss: 0.2767\n",
      "Epoch 144/150 Loss: 0.2720\n",
      "Epoch 145/150 Loss: 0.2770\n",
      "Epoch 146/150 Loss: 0.2776\n",
      "Epoch 147/150 Loss: 0.2741\n",
      "Epoch 148/150 Loss: 0.2686\n",
      "Epoch 149/150 Loss: 0.2705\n",
      "Epoch 150/150 Loss: 0.2719\n",
      "Config {'w1': 0.2, 'w2': 0.5, 'lambda_eng': 0.15, 'lr': 0.001} achieved Wasserstein: 0.7611\n",
      "\n",
      "=== Training with {'w1': 0.2, 'w2': 0.5, 'lambda_eng': 0.2, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.1357\n",
      "Epoch 2/150 Loss: 0.8038\n",
      "Epoch 3/150 Loss: 0.7581\n",
      "Epoch 4/150 Loss: 0.7255\n",
      "Epoch 5/150 Loss: 0.6917\n",
      "Epoch 6/150 Loss: 0.6596\n",
      "Epoch 7/150 Loss: 0.6303\n",
      "Epoch 8/150 Loss: 0.6135\n",
      "Epoch 9/150 Loss: 0.5948\n",
      "Epoch 10/150 Loss: 0.5823\n",
      "Epoch 11/150 Loss: 0.5738\n",
      "Epoch 12/150 Loss: 0.5544\n",
      "Epoch 13/150 Loss: 0.5253\n",
      "Epoch 14/150 Loss: 0.5209\n",
      "Epoch 15/150 Loss: 0.5117\n",
      "Epoch 16/150 Loss: 0.5063\n",
      "Epoch 17/150 Loss: 0.5075\n",
      "Epoch 18/150 Loss: 0.4973\n",
      "Epoch 19/150 Loss: 0.4876\n",
      "Epoch 20/150 Loss: 0.4713\n",
      "Epoch 21/150 Loss: 0.4701\n",
      "Epoch 22/150 Loss: 0.4669\n",
      "Epoch 23/150 Loss: 0.4545\n",
      "Epoch 24/150 Loss: 0.4508\n",
      "Epoch 25/150 Loss: 0.4521\n",
      "Epoch 26/150 Loss: 0.4480\n",
      "Epoch 27/150 Loss: 0.4446\n",
      "Epoch 28/150 Loss: 0.4336\n",
      "Epoch 29/150 Loss: 0.4381\n",
      "Epoch 30/150 Loss: 0.4397\n",
      "Epoch 31/150 Loss: 0.4187\n",
      "Epoch 32/150 Loss: 0.4218\n",
      "Epoch 33/150 Loss: 0.4192\n",
      "Epoch 34/150 Loss: 0.4156\n",
      "Epoch 35/150 Loss: 0.4180\n",
      "Epoch 36/150 Loss: 0.4165\n",
      "Epoch 37/150 Loss: 0.4163\n",
      "Epoch 38/150 Loss: 0.3965\n",
      "Epoch 39/150 Loss: 0.4031\n",
      "Epoch 40/150 Loss: 0.4033\n",
      "Epoch 41/150 Loss: 0.3934\n",
      "Epoch 42/150 Loss: 0.3997\n",
      "Epoch 43/150 Loss: 0.3976\n",
      "Epoch 44/150 Loss: 0.3987\n",
      "Epoch 45/150 Loss: 0.3925\n",
      "Epoch 46/150 Loss: 0.3956\n",
      "Epoch 47/150 Loss: 0.3841\n",
      "Epoch 48/150 Loss: 0.3817\n",
      "Epoch 49/150 Loss: 0.3759\n",
      "Epoch 50/150 Loss: 0.3817\n",
      "Epoch 51/150 Loss: 0.3768\n",
      "Epoch 52/150 Loss: 0.3769\n",
      "Epoch 53/150 Loss: 0.3817\n",
      "Epoch 54/150 Loss: 0.3698\n",
      "Epoch 55/150 Loss: 0.3777\n",
      "Epoch 56/150 Loss: 0.3753\n",
      "Epoch 57/150 Loss: 0.3747\n",
      "Epoch 58/150 Loss: 0.3632\n",
      "Epoch 59/150 Loss: 0.3740\n",
      "Epoch 60/150 Loss: 0.3632\n",
      "Epoch 61/150 Loss: 0.3676\n",
      "Epoch 62/150 Loss: 0.3600\n",
      "Epoch 63/150 Loss: 0.3522\n",
      "Epoch 64/150 Loss: 0.3570\n",
      "Epoch 65/150 Loss: 0.3451\n",
      "Epoch 66/150 Loss: 0.3519\n",
      "Epoch 67/150 Loss: 0.3497\n",
      "Epoch 68/150 Loss: 0.3489\n",
      "Epoch 69/150 Loss: 0.3465\n",
      "Epoch 70/150 Loss: 0.3448\n",
      "Epoch 71/150 Loss: 0.3475\n",
      "Epoch 72/150 Loss: 0.3363\n",
      "Epoch 73/150 Loss: 0.3433\n",
      "Epoch 74/150 Loss: 0.3375\n",
      "Epoch 75/150 Loss: 0.3471\n",
      "Epoch 76/150 Loss: 0.3377\n",
      "Epoch 77/150 Loss: 0.3303\n",
      "Epoch 78/150 Loss: 0.3317\n",
      "Epoch 79/150 Loss: 0.3309\n",
      "Epoch 80/150 Loss: 0.3290\n",
      "Epoch 81/150 Loss: 0.3269\n",
      "Epoch 82/150 Loss: 0.3331\n",
      "Epoch 83/150 Loss: 0.3347\n",
      "Epoch 84/150 Loss: 0.3159\n",
      "Epoch 85/150 Loss: 0.3253\n",
      "Epoch 86/150 Loss: 0.3278\n",
      "Epoch 87/150 Loss: 0.3276\n",
      "Epoch 88/150 Loss: 0.3211\n",
      "Epoch 89/150 Loss: 0.3223\n",
      "Epoch 90/150 Loss: 0.3161\n",
      "Epoch 91/150 Loss: 0.3184\n",
      "Epoch 92/150 Loss: 0.3160\n",
      "Epoch 93/150 Loss: 0.3175\n",
      "Epoch 94/150 Loss: 0.3159\n",
      "Epoch 95/150 Loss: 0.3107\n",
      "Epoch 96/150 Loss: 0.3094\n",
      "Epoch 97/150 Loss: 0.3074\n",
      "Epoch 98/150 Loss: 0.3135\n",
      "Epoch 99/150 Loss: 0.3113\n",
      "Epoch 100/150 Loss: 0.3073\n",
      "Epoch 101/150 Loss: 0.2996\n",
      "Epoch 102/150 Loss: 0.3116\n",
      "Epoch 103/150 Loss: 0.3067\n",
      "Epoch 104/150 Loss: 0.3084\n",
      "Epoch 105/150 Loss: 0.3070\n",
      "Epoch 106/150 Loss: 0.3092\n",
      "Epoch 107/150 Loss: 0.3040\n",
      "Epoch 108/150 Loss: 0.3052\n",
      "Epoch 109/150 Loss: 0.2993\n",
      "Epoch 110/150 Loss: 0.2999\n",
      "Epoch 111/150 Loss: 0.2940\n",
      "Epoch 112/150 Loss: 0.3008\n",
      "Epoch 113/150 Loss: 0.3018\n",
      "Epoch 114/150 Loss: 0.2938\n",
      "Epoch 115/150 Loss: 0.2966\n",
      "Epoch 116/150 Loss: 0.2958\n",
      "Epoch 117/150 Loss: 0.2969\n",
      "Epoch 118/150 Loss: 0.2994\n",
      "Epoch 119/150 Loss: 0.2960\n",
      "Epoch 120/150 Loss: 0.2926\n",
      "Epoch 121/150 Loss: 0.2934\n",
      "Epoch 122/150 Loss: 0.2910\n",
      "Epoch 123/150 Loss: 0.2933\n",
      "Epoch 124/150 Loss: 0.2974\n",
      "Epoch 125/150 Loss: 0.2929\n",
      "Epoch 126/150 Loss: 0.2904\n",
      "Epoch 127/150 Loss: 0.2895\n",
      "Epoch 128/150 Loss: 0.2903\n",
      "Epoch 129/150 Loss: 0.2836\n",
      "Epoch 130/150 Loss: 0.2884\n",
      "Epoch 131/150 Loss: 0.2836\n",
      "Epoch 132/150 Loss: 0.2902\n",
      "Epoch 133/150 Loss: 0.2952\n",
      "Epoch 134/150 Loss: 0.2853\n",
      "Epoch 135/150 Loss: 0.2874\n",
      "Epoch 136/150 Loss: 0.2857\n",
      "Epoch 137/150 Loss: 0.2780\n",
      "Epoch 138/150 Loss: 0.2846\n",
      "Epoch 139/150 Loss: 0.2847\n",
      "Epoch 140/150 Loss: 0.2801\n",
      "Epoch 141/150 Loss: 0.2826\n",
      "Epoch 142/150 Loss: 0.2823\n",
      "Epoch 143/150 Loss: 0.2824\n",
      "Epoch 144/150 Loss: 0.2750\n",
      "Epoch 145/150 Loss: 0.2791\n",
      "Epoch 146/150 Loss: 0.2801\n",
      "Epoch 147/150 Loss: 0.2793\n",
      "Epoch 148/150 Loss: 0.2756\n",
      "Epoch 149/150 Loss: 0.2763\n",
      "Epoch 150/150 Loss: 0.2780\n",
      "Config {'w1': 0.2, 'w2': 0.5, 'lambda_eng': 0.2, 'lr': 0.001} achieved Wasserstein: 0.6563\n",
      "\n",
      "=== Training with {'w1': 0.2, 'w2': 0.55, 'lambda_eng': 0.1, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 0.9721\n",
      "Epoch 2/150 Loss: 0.7393\n",
      "Epoch 3/150 Loss: 0.6897\n",
      "Epoch 4/150 Loss: 0.6781\n",
      "Epoch 5/150 Loss: 0.6537\n",
      "Epoch 6/150 Loss: 0.6251\n",
      "Epoch 7/150 Loss: 0.5941\n",
      "Epoch 8/150 Loss: 0.5591\n",
      "Epoch 9/150 Loss: 0.5615\n",
      "Epoch 10/150 Loss: 0.5420\n",
      "Epoch 11/150 Loss: 0.5377\n",
      "Epoch 12/150 Loss: 0.5179\n",
      "Epoch 13/150 Loss: 0.5120\n",
      "Epoch 14/150 Loss: 0.4982\n",
      "Epoch 15/150 Loss: 0.4959\n",
      "Epoch 16/150 Loss: 0.4816\n",
      "Epoch 17/150 Loss: 0.4723\n",
      "Epoch 18/150 Loss: 0.4672\n",
      "Epoch 19/150 Loss: 0.4609\n",
      "Epoch 20/150 Loss: 0.4618\n",
      "Epoch 21/150 Loss: 0.4528\n",
      "Epoch 22/150 Loss: 0.4515\n",
      "Epoch 23/150 Loss: 0.4439\n",
      "Epoch 24/150 Loss: 0.4445\n",
      "Epoch 25/150 Loss: 0.4499\n",
      "Epoch 26/150 Loss: 0.4389\n",
      "Epoch 27/150 Loss: 0.4272\n",
      "Epoch 28/150 Loss: 0.4291\n",
      "Epoch 29/150 Loss: 0.4249\n",
      "Epoch 30/150 Loss: 0.4221\n",
      "Epoch 31/150 Loss: 0.4243\n",
      "Epoch 32/150 Loss: 0.4107\n",
      "Epoch 33/150 Loss: 0.4104\n",
      "Epoch 34/150 Loss: 0.4087\n",
      "Epoch 35/150 Loss: 0.4080\n",
      "Epoch 36/150 Loss: 0.4067\n",
      "Epoch 37/150 Loss: 0.3989\n",
      "Epoch 38/150 Loss: 0.3890\n",
      "Epoch 39/150 Loss: 0.3927\n",
      "Epoch 40/150 Loss: 0.3866\n",
      "Epoch 41/150 Loss: 0.3850\n",
      "Epoch 42/150 Loss: 0.3808\n",
      "Epoch 43/150 Loss: 0.3778\n",
      "Epoch 44/150 Loss: 0.3882\n",
      "Epoch 45/150 Loss: 0.3811\n",
      "Epoch 46/150 Loss: 0.3782\n",
      "Epoch 47/150 Loss: 0.3722\n",
      "Epoch 48/150 Loss: 0.3790\n",
      "Epoch 49/150 Loss: 0.3740\n",
      "Epoch 50/150 Loss: 0.3750\n",
      "Epoch 51/150 Loss: 0.3703\n",
      "Epoch 52/150 Loss: 0.3621\n",
      "Epoch 53/150 Loss: 0.3628\n",
      "Epoch 54/150 Loss: 0.3601\n",
      "Epoch 55/150 Loss: 0.3556\n",
      "Epoch 56/150 Loss: 0.3575\n",
      "Epoch 57/150 Loss: 0.3627\n",
      "Epoch 58/150 Loss: 0.3556\n",
      "Epoch 59/150 Loss: 0.3542\n",
      "Epoch 60/150 Loss: 0.3545\n",
      "Epoch 61/150 Loss: 0.3415\n",
      "Epoch 62/150 Loss: 0.3504\n",
      "Epoch 63/150 Loss: 0.3476\n",
      "Epoch 64/150 Loss: 0.3473\n",
      "Epoch 65/150 Loss: 0.3433\n",
      "Epoch 66/150 Loss: 0.3452\n",
      "Epoch 67/150 Loss: 0.3439\n",
      "Epoch 68/150 Loss: 0.3441\n",
      "Epoch 69/150 Loss: 0.3416\n",
      "Epoch 70/150 Loss: 0.3398\n",
      "Epoch 71/150 Loss: 0.3481\n",
      "Epoch 72/150 Loss: 0.3370\n",
      "Epoch 73/150 Loss: 0.3337\n",
      "Epoch 74/150 Loss: 0.3345\n",
      "Epoch 75/150 Loss: 0.3336\n",
      "Epoch 76/150 Loss: 0.3321\n",
      "Epoch 77/150 Loss: 0.3286\n",
      "Epoch 78/150 Loss: 0.3280\n",
      "Epoch 79/150 Loss: 0.3319\n",
      "Epoch 80/150 Loss: 0.3205\n",
      "Epoch 81/150 Loss: 0.3241\n",
      "Epoch 82/150 Loss: 0.3251\n",
      "Epoch 83/150 Loss: 0.3240\n",
      "Epoch 84/150 Loss: 0.3174\n",
      "Epoch 85/150 Loss: 0.3241\n",
      "Epoch 86/150 Loss: 0.3249\n",
      "Epoch 87/150 Loss: 0.3171\n",
      "Epoch 88/150 Loss: 0.3153\n",
      "Epoch 89/150 Loss: 0.3165\n",
      "Epoch 90/150 Loss: 0.3192\n",
      "Epoch 91/150 Loss: 0.3166\n",
      "Epoch 92/150 Loss: 0.3127\n",
      "Epoch 93/150 Loss: 0.3153\n",
      "Epoch 94/150 Loss: 0.3091\n",
      "Epoch 95/150 Loss: 0.3133\n",
      "Epoch 96/150 Loss: 0.3051\n",
      "Epoch 97/150 Loss: 0.3042\n",
      "Epoch 98/150 Loss: 0.3072\n",
      "Epoch 99/150 Loss: 0.3039\n",
      "Epoch 100/150 Loss: 0.3065\n",
      "Epoch 101/150 Loss: 0.3034\n",
      "Epoch 102/150 Loss: 0.3034\n",
      "Epoch 103/150 Loss: 0.3035\n",
      "Epoch 104/150 Loss: 0.3006\n",
      "Epoch 105/150 Loss: 0.2948\n",
      "Epoch 106/150 Loss: 0.3012\n",
      "Epoch 107/150 Loss: 0.3011\n",
      "Epoch 108/150 Loss: 0.2966\n",
      "Epoch 109/150 Loss: 0.3013\n",
      "Epoch 110/150 Loss: 0.2982\n",
      "Epoch 111/150 Loss: 0.2955\n",
      "Epoch 112/150 Loss: 0.2990\n",
      "Epoch 113/150 Loss: 0.2969\n",
      "Epoch 114/150 Loss: 0.2960\n",
      "Epoch 115/150 Loss: 0.2950\n",
      "Epoch 116/150 Loss: 0.2885\n",
      "Epoch 117/150 Loss: 0.2956\n",
      "Epoch 118/150 Loss: 0.2970\n",
      "Epoch 119/150 Loss: 0.2982\n",
      "Epoch 120/150 Loss: 0.2875\n",
      "Epoch 121/150 Loss: 0.2901\n",
      "Epoch 122/150 Loss: 0.2893\n",
      "Epoch 123/150 Loss: 0.2849\n",
      "Epoch 124/150 Loss: 0.2907\n",
      "Epoch 125/150 Loss: 0.2886\n",
      "Epoch 126/150 Loss: 0.2836\n",
      "Epoch 127/150 Loss: 0.2811\n",
      "Epoch 128/150 Loss: 0.2885\n",
      "Epoch 129/150 Loss: 0.2850\n",
      "Epoch 130/150 Loss: 0.2853\n",
      "Epoch 131/150 Loss: 0.2876\n",
      "Epoch 132/150 Loss: 0.2923\n",
      "Epoch 133/150 Loss: 0.2794\n",
      "Epoch 134/150 Loss: 0.2824\n",
      "Epoch 135/150 Loss: 0.2825\n",
      "Epoch 136/150 Loss: 0.2802\n",
      "Epoch 137/150 Loss: 0.2790\n",
      "Epoch 138/150 Loss: 0.2784\n",
      "Epoch 139/150 Loss: 0.2788\n",
      "Epoch 140/150 Loss: 0.2773\n",
      "Epoch 141/150 Loss: 0.2810\n",
      "Epoch 142/150 Loss: 0.2792\n",
      "Epoch 143/150 Loss: 0.2835\n",
      "Epoch 144/150 Loss: 0.2770\n",
      "Epoch 145/150 Loss: 0.2772\n",
      "Epoch 146/150 Loss: 0.2740\n",
      "Epoch 147/150 Loss: 0.2791\n",
      "Epoch 148/150 Loss: 0.2792\n",
      "Epoch 149/150 Loss: 0.2723\n",
      "Epoch 150/150 Loss: 0.2722\n",
      "Config {'w1': 0.2, 'w2': 0.55, 'lambda_eng': 0.1, 'lr': 0.001} achieved Wasserstein: 0.7806\n",
      "\n",
      "=== Training with {'w1': 0.2, 'w2': 0.55, 'lambda_eng': 0.15, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.0342\n",
      "Epoch 2/150 Loss: 0.7448\n",
      "Epoch 3/150 Loss: 0.7038\n",
      "Epoch 4/150 Loss: 0.6829\n",
      "Epoch 5/150 Loss: 0.6505\n",
      "Epoch 6/150 Loss: 0.6246\n",
      "Epoch 7/150 Loss: 0.6079\n",
      "Epoch 8/150 Loss: 0.5923\n",
      "Epoch 9/150 Loss: 0.5863\n",
      "Epoch 10/150 Loss: 0.5652\n",
      "Epoch 11/150 Loss: 0.5415\n",
      "Epoch 12/150 Loss: 0.5378\n",
      "Epoch 13/150 Loss: 0.5314\n",
      "Epoch 14/150 Loss: 0.5207\n",
      "Epoch 15/150 Loss: 0.5203\n",
      "Epoch 16/150 Loss: 0.5021\n",
      "Epoch 17/150 Loss: 0.5029\n",
      "Epoch 18/150 Loss: 0.4871\n",
      "Epoch 19/150 Loss: 0.4908\n",
      "Epoch 20/150 Loss: 0.4799\n",
      "Epoch 21/150 Loss: 0.4639\n",
      "Epoch 22/150 Loss: 0.4710\n",
      "Epoch 23/150 Loss: 0.4596\n",
      "Epoch 24/150 Loss: 0.4547\n",
      "Epoch 25/150 Loss: 0.4555\n",
      "Epoch 26/150 Loss: 0.4449\n",
      "Epoch 27/150 Loss: 0.4395\n",
      "Epoch 28/150 Loss: 0.4381\n",
      "Epoch 29/150 Loss: 0.4312\n",
      "Epoch 30/150 Loss: 0.4257\n",
      "Epoch 31/150 Loss: 0.4255\n",
      "Epoch 32/150 Loss: 0.4207\n",
      "Epoch 33/150 Loss: 0.4223\n",
      "Epoch 34/150 Loss: 0.4142\n",
      "Epoch 35/150 Loss: 0.4055\n",
      "Epoch 36/150 Loss: 0.4041\n",
      "Epoch 37/150 Loss: 0.4056\n",
      "Epoch 38/150 Loss: 0.4032\n",
      "Epoch 39/150 Loss: 0.4032\n",
      "Epoch 40/150 Loss: 0.4013\n",
      "Epoch 41/150 Loss: 0.3905\n",
      "Epoch 42/150 Loss: 0.3951\n",
      "Epoch 43/150 Loss: 0.3948\n",
      "Epoch 44/150 Loss: 0.3912\n",
      "Epoch 45/150 Loss: 0.3880\n",
      "Epoch 46/150 Loss: 0.3841\n",
      "Epoch 47/150 Loss: 0.3803\n",
      "Epoch 48/150 Loss: 0.3814\n",
      "Epoch 49/150 Loss: 0.3780\n",
      "Epoch 50/150 Loss: 0.3724\n",
      "Epoch 51/150 Loss: 0.3771\n",
      "Epoch 52/150 Loss: 0.3665\n",
      "Epoch 53/150 Loss: 0.3677\n",
      "Epoch 54/150 Loss: 0.3628\n",
      "Epoch 55/150 Loss: 0.3578\n",
      "Epoch 56/150 Loss: 0.3695\n",
      "Epoch 57/150 Loss: 0.3633\n",
      "Epoch 58/150 Loss: 0.3613\n",
      "Epoch 59/150 Loss: 0.3563\n",
      "Epoch 60/150 Loss: 0.3559\n",
      "Epoch 61/150 Loss: 0.3517\n",
      "Epoch 62/150 Loss: 0.3535\n",
      "Epoch 63/150 Loss: 0.3537\n",
      "Epoch 64/150 Loss: 0.3528\n",
      "Epoch 65/150 Loss: 0.3503\n",
      "Epoch 66/150 Loss: 0.3428\n",
      "Epoch 67/150 Loss: 0.3474\n",
      "Epoch 68/150 Loss: 0.3479\n",
      "Epoch 69/150 Loss: 0.3394\n",
      "Epoch 70/150 Loss: 0.3418\n",
      "Epoch 71/150 Loss: 0.3446\n",
      "Epoch 72/150 Loss: 0.3423\n",
      "Epoch 73/150 Loss: 0.3407\n",
      "Epoch 74/150 Loss: 0.3371\n",
      "Epoch 75/150 Loss: 0.3336\n",
      "Epoch 76/150 Loss: 0.3412\n",
      "Epoch 77/150 Loss: 0.3341\n",
      "Epoch 78/150 Loss: 0.3361\n",
      "Epoch 79/150 Loss: 0.3345\n",
      "Epoch 80/150 Loss: 0.3407\n",
      "Epoch 81/150 Loss: 0.3261\n",
      "Epoch 82/150 Loss: 0.3255\n",
      "Epoch 83/150 Loss: 0.3258\n",
      "Epoch 84/150 Loss: 0.3271\n",
      "Epoch 85/150 Loss: 0.3235\n",
      "Epoch 86/150 Loss: 0.3181\n",
      "Epoch 87/150 Loss: 0.3245\n",
      "Epoch 88/150 Loss: 0.3184\n",
      "Epoch 89/150 Loss: 0.3187\n",
      "Epoch 90/150 Loss: 0.3193\n",
      "Epoch 91/150 Loss: 0.3164\n",
      "Epoch 92/150 Loss: 0.3102\n",
      "Epoch 93/150 Loss: 0.3169\n",
      "Epoch 94/150 Loss: 0.3109\n",
      "Epoch 95/150 Loss: 0.3099\n",
      "Epoch 96/150 Loss: 0.3104\n",
      "Epoch 97/150 Loss: 0.3043\n",
      "Epoch 98/150 Loss: 0.3094\n",
      "Epoch 99/150 Loss: 0.3132\n",
      "Epoch 100/150 Loss: 0.3094\n",
      "Epoch 101/150 Loss: 0.3040\n",
      "Epoch 102/150 Loss: 0.3099\n",
      "Epoch 103/150 Loss: 0.3040\n",
      "Epoch 104/150 Loss: 0.3104\n",
      "Epoch 105/150 Loss: 0.3067\n",
      "Epoch 106/150 Loss: 0.3031\n",
      "Epoch 107/150 Loss: 0.3061\n",
      "Epoch 108/150 Loss: 0.2947\n",
      "Epoch 109/150 Loss: 0.3063\n",
      "Epoch 110/150 Loss: 0.3068\n",
      "Epoch 111/150 Loss: 0.2997\n",
      "Epoch 112/150 Loss: 0.2919\n",
      "Epoch 113/150 Loss: 0.2976\n",
      "Epoch 114/150 Loss: 0.2953\n",
      "Epoch 115/150 Loss: 0.2981\n",
      "Epoch 116/150 Loss: 0.2979\n",
      "Epoch 117/150 Loss: 0.2948\n",
      "Epoch 118/150 Loss: 0.2938\n",
      "Epoch 119/150 Loss: 0.2933\n",
      "Epoch 120/150 Loss: 0.2934\n",
      "Epoch 121/150 Loss: 0.2859\n",
      "Epoch 122/150 Loss: 0.2848\n",
      "Epoch 123/150 Loss: 0.2927\n",
      "Epoch 124/150 Loss: 0.2885\n",
      "Epoch 125/150 Loss: 0.2892\n",
      "Epoch 126/150 Loss: 0.2809\n",
      "Epoch 127/150 Loss: 0.2851\n",
      "Epoch 128/150 Loss: 0.2885\n",
      "Epoch 129/150 Loss: 0.2906\n",
      "Epoch 130/150 Loss: 0.2843\n",
      "Epoch 131/150 Loss: 0.2865\n",
      "Epoch 132/150 Loss: 0.2867\n",
      "Epoch 133/150 Loss: 0.2886\n",
      "Epoch 134/150 Loss: 0.2814\n",
      "Epoch 135/150 Loss: 0.2847\n",
      "Epoch 136/150 Loss: 0.2781\n",
      "Epoch 137/150 Loss: 0.2819\n",
      "Epoch 138/150 Loss: 0.2799\n",
      "Epoch 139/150 Loss: 0.2780\n",
      "Epoch 140/150 Loss: 0.2794\n",
      "Epoch 141/150 Loss: 0.2780\n",
      "Epoch 142/150 Loss: 0.2813\n",
      "Epoch 143/150 Loss: 0.2816\n",
      "Epoch 144/150 Loss: 0.2770\n",
      "Epoch 145/150 Loss: 0.2859\n",
      "Epoch 146/150 Loss: 0.2824\n",
      "Epoch 147/150 Loss: 0.2701\n",
      "Epoch 148/150 Loss: 0.2688\n",
      "Epoch 149/150 Loss: 0.2769\n",
      "Epoch 150/150 Loss: 0.2724\n",
      "Config {'w1': 0.2, 'w2': 0.55, 'lambda_eng': 0.15, 'lr': 0.001} achieved Wasserstein: 0.5633\n",
      "\n",
      "=== Training with {'w1': 0.2, 'w2': 0.55, 'lambda_eng': 0.2, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.1381\n",
      "Epoch 2/150 Loss: 0.7952\n",
      "Epoch 3/150 Loss: 0.7368\n",
      "Epoch 4/150 Loss: 0.7048\n",
      "Epoch 5/150 Loss: 0.6696\n",
      "Epoch 6/150 Loss: 0.6528\n",
      "Epoch 7/150 Loss: 0.6184\n",
      "Epoch 8/150 Loss: 0.6185\n",
      "Epoch 9/150 Loss: 0.5950\n",
      "Epoch 10/150 Loss: 0.5773\n",
      "Epoch 11/150 Loss: 0.5771\n",
      "Epoch 12/150 Loss: 0.5561\n",
      "Epoch 13/150 Loss: 0.5550\n",
      "Epoch 14/150 Loss: 0.5337\n",
      "Epoch 15/150 Loss: 0.5306\n",
      "Epoch 16/150 Loss: 0.5136\n",
      "Epoch 17/150 Loss: 0.5246\n",
      "Epoch 18/150 Loss: 0.5058\n",
      "Epoch 19/150 Loss: 0.5030\n",
      "Epoch 20/150 Loss: 0.4881\n",
      "Epoch 21/150 Loss: 0.4908\n",
      "Epoch 22/150 Loss: 0.4759\n",
      "Epoch 23/150 Loss: 0.4841\n",
      "Epoch 24/150 Loss: 0.4778\n",
      "Epoch 25/150 Loss: 0.4769\n",
      "Epoch 26/150 Loss: 0.4597\n",
      "Epoch 27/150 Loss: 0.4728\n",
      "Epoch 28/150 Loss: 0.4685\n",
      "Epoch 29/150 Loss: 0.4513\n",
      "Epoch 30/150 Loss: 0.4494\n",
      "Epoch 31/150 Loss: 0.4547\n",
      "Epoch 32/150 Loss: 0.4472\n",
      "Epoch 33/150 Loss: 0.4332\n",
      "Epoch 34/150 Loss: 0.4249\n",
      "Epoch 35/150 Loss: 0.4381\n",
      "Epoch 36/150 Loss: 0.4314\n",
      "Epoch 37/150 Loss: 0.4228\n",
      "Epoch 38/150 Loss: 0.4159\n",
      "Epoch 39/150 Loss: 0.4179\n",
      "Epoch 40/150 Loss: 0.4135\n",
      "Epoch 41/150 Loss: 0.4150\n",
      "Epoch 42/150 Loss: 0.4097\n",
      "Epoch 43/150 Loss: 0.4105\n",
      "Epoch 44/150 Loss: 0.4045\n",
      "Epoch 45/150 Loss: 0.3975\n",
      "Epoch 46/150 Loss: 0.3965\n",
      "Epoch 47/150 Loss: 0.4010\n",
      "Epoch 48/150 Loss: 0.3898\n",
      "Epoch 49/150 Loss: 0.4009\n",
      "Epoch 50/150 Loss: 0.3898\n",
      "Epoch 51/150 Loss: 0.3853\n",
      "Epoch 52/150 Loss: 0.3975\n",
      "Epoch 53/150 Loss: 0.3836\n",
      "Epoch 54/150 Loss: 0.3783\n",
      "Epoch 55/150 Loss: 0.3866\n",
      "Epoch 56/150 Loss: 0.3799\n",
      "Epoch 57/150 Loss: 0.3820\n",
      "Epoch 58/150 Loss: 0.3727\n",
      "Epoch 59/150 Loss: 0.3675\n",
      "Epoch 60/150 Loss: 0.3692\n",
      "Epoch 61/150 Loss: 0.3774\n",
      "Epoch 62/150 Loss: 0.3732\n",
      "Epoch 63/150 Loss: 0.3702\n",
      "Epoch 64/150 Loss: 0.3651\n",
      "Epoch 65/150 Loss: 0.3651\n",
      "Epoch 66/150 Loss: 0.3618\n",
      "Epoch 67/150 Loss: 0.3640\n",
      "Epoch 68/150 Loss: 0.3576\n",
      "Epoch 69/150 Loss: 0.3567\n",
      "Epoch 70/150 Loss: 0.3579\n",
      "Epoch 71/150 Loss: 0.3539\n",
      "Epoch 72/150 Loss: 0.3556\n",
      "Epoch 73/150 Loss: 0.3457\n",
      "Epoch 74/150 Loss: 0.3424\n",
      "Epoch 75/150 Loss: 0.3520\n",
      "Epoch 76/150 Loss: 0.3486\n",
      "Epoch 77/150 Loss: 0.3503\n",
      "Epoch 78/150 Loss: 0.3394\n",
      "Epoch 79/150 Loss: 0.3450\n",
      "Epoch 80/150 Loss: 0.3415\n",
      "Epoch 81/150 Loss: 0.3502\n",
      "Epoch 82/150 Loss: 0.3355\n",
      "Epoch 83/150 Loss: 0.3342\n",
      "Epoch 84/150 Loss: 0.3358\n",
      "Epoch 85/150 Loss: 0.3375\n",
      "Epoch 86/150 Loss: 0.3382\n",
      "Epoch 87/150 Loss: 0.3319\n",
      "Epoch 88/150 Loss: 0.3231\n",
      "Epoch 89/150 Loss: 0.3288\n",
      "Epoch 90/150 Loss: 0.3288\n",
      "Epoch 91/150 Loss: 0.3293\n",
      "Epoch 92/150 Loss: 0.3239\n",
      "Epoch 93/150 Loss: 0.3269\n",
      "Epoch 94/150 Loss: 0.3232\n",
      "Epoch 95/150 Loss: 0.3263\n",
      "Epoch 96/150 Loss: 0.3252\n",
      "Epoch 97/150 Loss: 0.3139\n",
      "Epoch 98/150 Loss: 0.3190\n",
      "Epoch 99/150 Loss: 0.3099\n",
      "Epoch 100/150 Loss: 0.3235\n",
      "Epoch 101/150 Loss: 0.3191\n",
      "Epoch 102/150 Loss: 0.3168\n",
      "Epoch 103/150 Loss: 0.3092\n",
      "Epoch 104/150 Loss: 0.3146\n",
      "Epoch 105/150 Loss: 0.3095\n",
      "Epoch 106/150 Loss: 0.3035\n",
      "Epoch 107/150 Loss: 0.3054\n",
      "Epoch 108/150 Loss: 0.3094\n",
      "Epoch 109/150 Loss: 0.3105\n",
      "Epoch 110/150 Loss: 0.3077\n",
      "Epoch 111/150 Loss: 0.3096\n",
      "Epoch 112/150 Loss: 0.3059\n",
      "Epoch 113/150 Loss: 0.3029\n",
      "Epoch 114/150 Loss: 0.3042\n",
      "Epoch 115/150 Loss: 0.3053\n",
      "Epoch 116/150 Loss: 0.2980\n",
      "Epoch 117/150 Loss: 0.3002\n",
      "Epoch 118/150 Loss: 0.3076\n",
      "Epoch 119/150 Loss: 0.3023\n",
      "Epoch 120/150 Loss: 0.2990\n",
      "Epoch 121/150 Loss: 0.2979\n",
      "Epoch 122/150 Loss: 0.2985\n",
      "Epoch 123/150 Loss: 0.2976\n",
      "Epoch 124/150 Loss: 0.2962\n",
      "Epoch 125/150 Loss: 0.2941\n",
      "Epoch 126/150 Loss: 0.2949\n",
      "Epoch 127/150 Loss: 0.2907\n",
      "Epoch 128/150 Loss: 0.2942\n",
      "Epoch 129/150 Loss: 0.2925\n",
      "Epoch 130/150 Loss: 0.2894\n",
      "Epoch 131/150 Loss: 0.2901\n",
      "Epoch 132/150 Loss: 0.2868\n",
      "Epoch 133/150 Loss: 0.2913\n",
      "Epoch 134/150 Loss: 0.2900\n",
      "Epoch 135/150 Loss: 0.2909\n",
      "Epoch 136/150 Loss: 0.2857\n",
      "Epoch 137/150 Loss: 0.2931\n",
      "Epoch 138/150 Loss: 0.2902\n",
      "Epoch 139/150 Loss: 0.2913\n",
      "Epoch 140/150 Loss: 0.2844\n",
      "Epoch 141/150 Loss: 0.2894\n",
      "Epoch 142/150 Loss: 0.2903\n",
      "Epoch 143/150 Loss: 0.2822\n",
      "Epoch 144/150 Loss: 0.2800\n",
      "Epoch 145/150 Loss: 0.2894\n",
      "Epoch 146/150 Loss: 0.2849\n",
      "Epoch 147/150 Loss: 0.2821\n",
      "Epoch 148/150 Loss: 0.2839\n",
      "Epoch 149/150 Loss: 0.2824\n",
      "Epoch 150/150 Loss: 0.2775\n",
      "Config {'w1': 0.2, 'w2': 0.55, 'lambda_eng': 0.2, 'lr': 0.001} achieved Wasserstein: 0.7358\n",
      "\n",
      "=== Training with {'w1': 0.2, 'w2': 0.6, 'lambda_eng': 0.1, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 0.9962\n",
      "Epoch 2/150 Loss: 0.7414\n",
      "Epoch 3/150 Loss: 0.6982\n",
      "Epoch 4/150 Loss: 0.6614\n",
      "Epoch 5/150 Loss: 0.6338\n",
      "Epoch 6/150 Loss: 0.6154\n",
      "Epoch 7/150 Loss: 0.5921\n",
      "Epoch 8/150 Loss: 0.5739\n",
      "Epoch 9/150 Loss: 0.5556\n",
      "Epoch 10/150 Loss: 0.5573\n",
      "Epoch 11/150 Loss: 0.5358\n",
      "Epoch 12/150 Loss: 0.5308\n",
      "Epoch 13/150 Loss: 0.5105\n",
      "Epoch 14/150 Loss: 0.5021\n",
      "Epoch 15/150 Loss: 0.5058\n",
      "Epoch 16/150 Loss: 0.4960\n",
      "Epoch 17/150 Loss: 0.4866\n",
      "Epoch 18/150 Loss: 0.4748\n",
      "Epoch 19/150 Loss: 0.4737\n",
      "Epoch 20/150 Loss: 0.4642\n",
      "Epoch 21/150 Loss: 0.4609\n",
      "Epoch 22/150 Loss: 0.4562\n",
      "Epoch 23/150 Loss: 0.4488\n",
      "Epoch 24/150 Loss: 0.4599\n",
      "Epoch 25/150 Loss: 0.4344\n",
      "Epoch 26/150 Loss: 0.4359\n",
      "Epoch 27/150 Loss: 0.4274\n",
      "Epoch 28/150 Loss: 0.4279\n",
      "Epoch 29/150 Loss: 0.4170\n",
      "Epoch 30/150 Loss: 0.4234\n",
      "Epoch 31/150 Loss: 0.4212\n",
      "Epoch 32/150 Loss: 0.4133\n",
      "Epoch 33/150 Loss: 0.4052\n",
      "Epoch 34/150 Loss: 0.4024\n",
      "Epoch 35/150 Loss: 0.3902\n",
      "Epoch 36/150 Loss: 0.3945\n",
      "Epoch 37/150 Loss: 0.3930\n",
      "Epoch 38/150 Loss: 0.3907\n",
      "Epoch 39/150 Loss: 0.3997\n",
      "Epoch 40/150 Loss: 0.3922\n",
      "Epoch 41/150 Loss: 0.3826\n",
      "Epoch 42/150 Loss: 0.3820\n",
      "Epoch 43/150 Loss: 0.3876\n",
      "Epoch 44/150 Loss: 0.3836\n",
      "Epoch 45/150 Loss: 0.3778\n",
      "Epoch 46/150 Loss: 0.3792\n",
      "Epoch 47/150 Loss: 0.3831\n",
      "Epoch 48/150 Loss: 0.3711\n",
      "Epoch 49/150 Loss: 0.3750\n",
      "Epoch 50/150 Loss: 0.3657\n",
      "Epoch 51/150 Loss: 0.3741\n",
      "Epoch 52/150 Loss: 0.3628\n",
      "Epoch 53/150 Loss: 0.3652\n",
      "Epoch 54/150 Loss: 0.3634\n",
      "Epoch 55/150 Loss: 0.3609\n",
      "Epoch 56/150 Loss: 0.3589\n",
      "Epoch 57/150 Loss: 0.3634\n",
      "Epoch 58/150 Loss: 0.3609\n",
      "Epoch 59/150 Loss: 0.3520\n",
      "Epoch 60/150 Loss: 0.3554\n",
      "Epoch 61/150 Loss: 0.3553\n",
      "Epoch 62/150 Loss: 0.3452\n",
      "Epoch 63/150 Loss: 0.3533\n",
      "Epoch 64/150 Loss: 0.3459\n",
      "Epoch 65/150 Loss: 0.3547\n",
      "Epoch 66/150 Loss: 0.3516\n",
      "Epoch 67/150 Loss: 0.3423\n",
      "Epoch 68/150 Loss: 0.3412\n",
      "Epoch 69/150 Loss: 0.3409\n",
      "Epoch 70/150 Loss: 0.3414\n",
      "Epoch 71/150 Loss: 0.3445\n",
      "Epoch 72/150 Loss: 0.3365\n",
      "Epoch 73/150 Loss: 0.3313\n",
      "Epoch 74/150 Loss: 0.3447\n",
      "Epoch 75/150 Loss: 0.3362\n",
      "Epoch 76/150 Loss: 0.3377\n",
      "Epoch 77/150 Loss: 0.3397\n",
      "Epoch 78/150 Loss: 0.3313\n",
      "Epoch 79/150 Loss: 0.3282\n",
      "Epoch 80/150 Loss: 0.3343\n",
      "Epoch 81/150 Loss: 0.3284\n",
      "Epoch 82/150 Loss: 0.3251\n",
      "Epoch 83/150 Loss: 0.3309\n",
      "Epoch 84/150 Loss: 0.3253\n",
      "Epoch 85/150 Loss: 0.3253\n",
      "Epoch 86/150 Loss: 0.3261\n",
      "Epoch 87/150 Loss: 0.3225\n",
      "Epoch 88/150 Loss: 0.3247\n",
      "Epoch 89/150 Loss: 0.3271\n",
      "Epoch 90/150 Loss: 0.3167\n",
      "Epoch 91/150 Loss: 0.3258\n",
      "Epoch 92/150 Loss: 0.3235\n",
      "Epoch 93/150 Loss: 0.3088\n",
      "Epoch 94/150 Loss: 0.3170\n",
      "Epoch 95/150 Loss: 0.3120\n",
      "Epoch 96/150 Loss: 0.3145\n",
      "Epoch 97/150 Loss: 0.3191\n",
      "Epoch 98/150 Loss: 0.3140\n",
      "Epoch 99/150 Loss: 0.3114\n",
      "Epoch 100/150 Loss: 0.3084\n",
      "Epoch 101/150 Loss: 0.3083\n",
      "Epoch 102/150 Loss: 0.3086\n",
      "Epoch 103/150 Loss: 0.3058\n",
      "Epoch 104/150 Loss: 0.2999\n",
      "Epoch 105/150 Loss: 0.3030\n",
      "Epoch 106/150 Loss: 0.3071\n",
      "Epoch 107/150 Loss: 0.2988\n",
      "Epoch 108/150 Loss: 0.3043\n",
      "Epoch 109/150 Loss: 0.3027\n",
      "Epoch 110/150 Loss: 0.2984\n",
      "Epoch 111/150 Loss: 0.2992\n",
      "Epoch 112/150 Loss: 0.3023\n",
      "Epoch 113/150 Loss: 0.2953\n",
      "Epoch 114/150 Loss: 0.2946\n",
      "Epoch 115/150 Loss: 0.2944\n",
      "Epoch 116/150 Loss: 0.2935\n",
      "Epoch 117/150 Loss: 0.2896\n",
      "Epoch 118/150 Loss: 0.2924\n",
      "Epoch 119/150 Loss: 0.2898\n",
      "Epoch 120/150 Loss: 0.2954\n",
      "Epoch 121/150 Loss: 0.2932\n",
      "Epoch 122/150 Loss: 0.3008\n",
      "Epoch 123/150 Loss: 0.2868\n",
      "Epoch 124/150 Loss: 0.2911\n",
      "Epoch 125/150 Loss: 0.2953\n",
      "Epoch 126/150 Loss: 0.2908\n",
      "Epoch 127/150 Loss: 0.2866\n",
      "Epoch 128/150 Loss: 0.2932\n",
      "Epoch 129/150 Loss: 0.2860\n",
      "Epoch 130/150 Loss: 0.2839\n",
      "Epoch 131/150 Loss: 0.2837\n",
      "Epoch 132/150 Loss: 0.2845\n",
      "Epoch 133/150 Loss: 0.2862\n",
      "Epoch 134/150 Loss: 0.2836\n",
      "Epoch 135/150 Loss: 0.2825\n",
      "Epoch 136/150 Loss: 0.2879\n",
      "Epoch 137/150 Loss: 0.2784\n",
      "Epoch 138/150 Loss: 0.2785\n",
      "Epoch 139/150 Loss: 0.2764\n",
      "Epoch 140/150 Loss: 0.2828\n",
      "Epoch 141/150 Loss: 0.2821\n",
      "Epoch 142/150 Loss: 0.2745\n",
      "Epoch 143/150 Loss: 0.2791\n",
      "Epoch 144/150 Loss: 0.2782\n",
      "Epoch 145/150 Loss: 0.2799\n",
      "Epoch 146/150 Loss: 0.2813\n",
      "Epoch 147/150 Loss: 0.2781\n",
      "Epoch 148/150 Loss: 0.2731\n",
      "Epoch 149/150 Loss: 0.2737\n",
      "Epoch 150/150 Loss: 0.2749\n",
      "Config {'w1': 0.2, 'w2': 0.6, 'lambda_eng': 0.1, 'lr': 0.001} achieved Wasserstein: 0.8479\n",
      "\n",
      "=== Training with {'w1': 0.2, 'w2': 0.6, 'lambda_eng': 0.15, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.0862\n",
      "Epoch 2/150 Loss: 0.7782\n",
      "Epoch 3/150 Loss: 0.7213\n",
      "Epoch 4/150 Loss: 0.6754\n",
      "Epoch 5/150 Loss: 0.6389\n",
      "Epoch 6/150 Loss: 0.6209\n",
      "Epoch 7/150 Loss: 0.5892\n",
      "Epoch 8/150 Loss: 0.5795\n",
      "Epoch 9/150 Loss: 0.5629\n",
      "Epoch 10/150 Loss: 0.5535\n",
      "Epoch 11/150 Loss: 0.5477\n",
      "Epoch 12/150 Loss: 0.5295\n",
      "Epoch 13/150 Loss: 0.5215\n",
      "Epoch 14/150 Loss: 0.5083\n",
      "Epoch 15/150 Loss: 0.5117\n",
      "Epoch 16/150 Loss: 0.5059\n",
      "Epoch 17/150 Loss: 0.4990\n",
      "Epoch 18/150 Loss: 0.4880\n",
      "Epoch 19/150 Loss: 0.4884\n",
      "Epoch 20/150 Loss: 0.4830\n",
      "Epoch 21/150 Loss: 0.4699\n",
      "Epoch 22/150 Loss: 0.4633\n",
      "Epoch 23/150 Loss: 0.4619\n",
      "Epoch 24/150 Loss: 0.4537\n",
      "Epoch 25/150 Loss: 0.4480\n",
      "Epoch 26/150 Loss: 0.4540\n",
      "Epoch 27/150 Loss: 0.4423\n",
      "Epoch 28/150 Loss: 0.4338\n",
      "Epoch 29/150 Loss: 0.4397\n",
      "Epoch 30/150 Loss: 0.4241\n",
      "Epoch 31/150 Loss: 0.4299\n",
      "Epoch 32/150 Loss: 0.4328\n",
      "Epoch 33/150 Loss: 0.4191\n",
      "Epoch 34/150 Loss: 0.4148\n",
      "Epoch 35/150 Loss: 0.4159\n",
      "Epoch 36/150 Loss: 0.4132\n",
      "Epoch 37/150 Loss: 0.4066\n",
      "Epoch 38/150 Loss: 0.4088\n",
      "Epoch 39/150 Loss: 0.4003\n",
      "Epoch 40/150 Loss: 0.3960\n",
      "Epoch 41/150 Loss: 0.3955\n",
      "Epoch 42/150 Loss: 0.3992\n",
      "Epoch 43/150 Loss: 0.3919\n",
      "Epoch 44/150 Loss: 0.3883\n",
      "Epoch 45/150 Loss: 0.3853\n",
      "Epoch 46/150 Loss: 0.3858\n",
      "Epoch 47/150 Loss: 0.3869\n",
      "Epoch 48/150 Loss: 0.3791\n",
      "Epoch 49/150 Loss: 0.3825\n",
      "Epoch 50/150 Loss: 0.3722\n",
      "Epoch 51/150 Loss: 0.3698\n",
      "Epoch 52/150 Loss: 0.3736\n",
      "Epoch 53/150 Loss: 0.3700\n",
      "Epoch 54/150 Loss: 0.3752\n",
      "Epoch 55/150 Loss: 0.3704\n",
      "Epoch 56/150 Loss: 0.3657\n",
      "Epoch 57/150 Loss: 0.3718\n",
      "Epoch 58/150 Loss: 0.3637\n",
      "Epoch 59/150 Loss: 0.3644\n",
      "Epoch 60/150 Loss: 0.3585\n",
      "Epoch 61/150 Loss: 0.3549\n",
      "Epoch 62/150 Loss: 0.3530\n",
      "Epoch 63/150 Loss: 0.3476\n",
      "Epoch 64/150 Loss: 0.3447\n",
      "Epoch 65/150 Loss: 0.3525\n",
      "Epoch 66/150 Loss: 0.3512\n",
      "Epoch 67/150 Loss: 0.3463\n",
      "Epoch 68/150 Loss: 0.3456\n",
      "Epoch 69/150 Loss: 0.3481\n",
      "Epoch 70/150 Loss: 0.3410\n",
      "Epoch 71/150 Loss: 0.3388\n",
      "Epoch 72/150 Loss: 0.3400\n",
      "Epoch 73/150 Loss: 0.3433\n",
      "Epoch 74/150 Loss: 0.3365\n",
      "Epoch 75/150 Loss: 0.3314\n",
      "Epoch 76/150 Loss: 0.3346\n",
      "Epoch 77/150 Loss: 0.3286\n",
      "Epoch 78/150 Loss: 0.3355\n",
      "Epoch 79/150 Loss: 0.3375\n",
      "Epoch 80/150 Loss: 0.3304\n",
      "Epoch 81/150 Loss: 0.3394\n",
      "Epoch 82/150 Loss: 0.3243\n",
      "Epoch 83/150 Loss: 0.3213\n",
      "Epoch 84/150 Loss: 0.3231\n",
      "Epoch 85/150 Loss: 0.3230\n",
      "Epoch 86/150 Loss: 0.3179\n",
      "Epoch 87/150 Loss: 0.3248\n",
      "Epoch 88/150 Loss: 0.3312\n",
      "Epoch 89/150 Loss: 0.3221\n",
      "Epoch 90/150 Loss: 0.3210\n",
      "Epoch 91/150 Loss: 0.3184\n",
      "Epoch 92/150 Loss: 0.3181\n",
      "Epoch 93/150 Loss: 0.3139\n",
      "Epoch 94/150 Loss: 0.3101\n",
      "Epoch 95/150 Loss: 0.3159\n",
      "Epoch 96/150 Loss: 0.3144\n",
      "Epoch 97/150 Loss: 0.3121\n",
      "Epoch 98/150 Loss: 0.3155\n",
      "Epoch 99/150 Loss: 0.3098\n",
      "Epoch 100/150 Loss: 0.3097\n",
      "Epoch 101/150 Loss: 0.3042\n",
      "Epoch 102/150 Loss: 0.3089\n",
      "Epoch 103/150 Loss: 0.3032\n",
      "Epoch 104/150 Loss: 0.3095\n",
      "Epoch 105/150 Loss: 0.3144\n",
      "Epoch 106/150 Loss: 0.2980\n",
      "Epoch 107/150 Loss: 0.2946\n",
      "Epoch 108/150 Loss: 0.3018\n",
      "Epoch 109/150 Loss: 0.3086\n",
      "Epoch 110/150 Loss: 0.3036\n",
      "Epoch 111/150 Loss: 0.3119\n",
      "Epoch 112/150 Loss: 0.3014\n",
      "Epoch 113/150 Loss: 0.2994\n",
      "Epoch 114/150 Loss: 0.3001\n",
      "Epoch 115/150 Loss: 0.2994\n",
      "Epoch 116/150 Loss: 0.2910\n",
      "Epoch 117/150 Loss: 0.3010\n",
      "Epoch 118/150 Loss: 0.2929\n",
      "Epoch 119/150 Loss: 0.2955\n",
      "Epoch 120/150 Loss: 0.2950\n",
      "Epoch 121/150 Loss: 0.2922\n",
      "Epoch 122/150 Loss: 0.2882\n",
      "Epoch 123/150 Loss: 0.2909\n",
      "Epoch 124/150 Loss: 0.2944\n",
      "Epoch 125/150 Loss: 0.2945\n",
      "Epoch 126/150 Loss: 0.2843\n",
      "Epoch 127/150 Loss: 0.2862\n",
      "Epoch 128/150 Loss: 0.2902\n",
      "Epoch 129/150 Loss: 0.2925\n",
      "Epoch 130/150 Loss: 0.2942\n",
      "Epoch 131/150 Loss: 0.2889\n",
      "Epoch 132/150 Loss: 0.2844\n",
      "Epoch 133/150 Loss: 0.2930\n",
      "Epoch 134/150 Loss: 0.2902\n",
      "Epoch 135/150 Loss: 0.2916\n",
      "Epoch 136/150 Loss: 0.2899\n",
      "Epoch 137/150 Loss: 0.2824\n",
      "Epoch 138/150 Loss: 0.2861\n",
      "Epoch 139/150 Loss: 0.2877\n",
      "Epoch 140/150 Loss: 0.2860\n",
      "Epoch 141/150 Loss: 0.2853\n",
      "Epoch 142/150 Loss: 0.2826\n",
      "Epoch 143/150 Loss: 0.2834\n",
      "Epoch 144/150 Loss: 0.2833\n",
      "Epoch 145/150 Loss: 0.2865\n",
      "Epoch 146/150 Loss: 0.2798\n",
      "Epoch 147/150 Loss: 0.2828\n",
      "Epoch 148/150 Loss: 0.2773\n",
      "Epoch 149/150 Loss: 0.2841\n",
      "Epoch 150/150 Loss: 0.2776\n",
      "Config {'w1': 0.2, 'w2': 0.6, 'lambda_eng': 0.15, 'lr': 0.001} achieved Wasserstein: 0.8052\n",
      "\n",
      "=== Training with {'w1': 0.2, 'w2': 0.6, 'lambda_eng': 0.2, 'lr': 0.001} ===\n",
      "Epoch 1/150 Loss: 1.1615\n",
      "Epoch 2/150 Loss: 0.8128\n",
      "Epoch 3/150 Loss: 0.7526\n",
      "Epoch 4/150 Loss: 0.7331\n",
      "Epoch 5/150 Loss: 0.6690\n",
      "Epoch 6/150 Loss: 0.6652\n",
      "Epoch 7/150 Loss: 0.6416\n",
      "Epoch 8/150 Loss: 0.6207\n",
      "Epoch 9/150 Loss: 0.6098\n",
      "Epoch 10/150 Loss: 0.6127\n",
      "Epoch 11/150 Loss: 0.5885\n",
      "Epoch 12/150 Loss: 0.5793\n",
      "Epoch 13/150 Loss: 0.5610\n",
      "Epoch 14/150 Loss: 0.5431\n",
      "Epoch 15/150 Loss: 0.5461\n",
      "Epoch 16/150 Loss: 0.5320\n",
      "Epoch 17/150 Loss: 0.5275\n",
      "Epoch 18/150 Loss: 0.5125\n",
      "Epoch 19/150 Loss: 0.5168\n",
      "Epoch 20/150 Loss: 0.5138\n",
      "Epoch 21/150 Loss: 0.4987\n",
      "Epoch 22/150 Loss: 0.5000\n",
      "Epoch 23/150 Loss: 0.4956\n",
      "Epoch 24/150 Loss: 0.4894\n",
      "Epoch 25/150 Loss: 0.4818\n",
      "Epoch 26/150 Loss: 0.4764\n",
      "Epoch 27/150 Loss: 0.4763\n",
      "Epoch 28/150 Loss: 0.4740\n",
      "Epoch 29/150 Loss: 0.4727\n",
      "Epoch 30/150 Loss: 0.4578\n",
      "Epoch 31/150 Loss: 0.4481\n",
      "Epoch 32/150 Loss: 0.4526\n",
      "Epoch 33/150 Loss: 0.4410\n",
      "Epoch 34/150 Loss: 0.4527\n",
      "Epoch 35/150 Loss: 0.4370\n",
      "Epoch 36/150 Loss: 0.4378\n",
      "Epoch 37/150 Loss: 0.4366\n",
      "Epoch 38/150 Loss: 0.4310\n",
      "Epoch 39/150 Loss: 0.4310\n",
      "Epoch 40/150 Loss: 0.4303\n",
      "Epoch 41/150 Loss: 0.4286\n",
      "Epoch 42/150 Loss: 0.4238\n",
      "Epoch 43/150 Loss: 0.4174\n",
      "Epoch 44/150 Loss: 0.4127\n",
      "Epoch 45/150 Loss: 0.4116\n",
      "Epoch 46/150 Loss: 0.4176\n",
      "Epoch 47/150 Loss: 0.4055\n",
      "Epoch 48/150 Loss: 0.4051\n",
      "Epoch 49/150 Loss: 0.4091\n",
      "Epoch 50/150 Loss: 0.3995\n",
      "Epoch 51/150 Loss: 0.3984\n",
      "Epoch 52/150 Loss: 0.3970\n",
      "Epoch 53/150 Loss: 0.3943\n",
      "Epoch 54/150 Loss: 0.3919\n",
      "Epoch 55/150 Loss: 0.3929\n",
      "Epoch 56/150 Loss: 0.3901\n",
      "Epoch 57/150 Loss: 0.3940\n",
      "Epoch 58/150 Loss: 0.3886\n",
      "Epoch 59/150 Loss: 0.3801\n",
      "Epoch 60/150 Loss: 0.3776\n",
      "Epoch 61/150 Loss: 0.3779\n",
      "Epoch 62/150 Loss: 0.3771\n",
      "Epoch 63/150 Loss: 0.3821\n",
      "Epoch 64/150 Loss: 0.3747\n",
      "Epoch 65/150 Loss: 0.3724\n",
      "Epoch 66/150 Loss: 0.3716\n",
      "Epoch 67/150 Loss: 0.3641\n",
      "Epoch 68/150 Loss: 0.3677\n",
      "Epoch 69/150 Loss: 0.3709\n",
      "Epoch 70/150 Loss: 0.3558\n",
      "Epoch 71/150 Loss: 0.3630\n",
      "Epoch 72/150 Loss: 0.3581\n",
      "Epoch 73/150 Loss: 0.3624\n",
      "Epoch 74/150 Loss: 0.3556\n",
      "Epoch 75/150 Loss: 0.3596\n",
      "Epoch 76/150 Loss: 0.3526\n",
      "Epoch 77/150 Loss: 0.3605\n",
      "Epoch 78/150 Loss: 0.3550\n",
      "Epoch 79/150 Loss: 0.3569\n",
      "Epoch 80/150 Loss: 0.3486\n",
      "Epoch 81/150 Loss: 0.3493\n",
      "Epoch 82/150 Loss: 0.3449\n",
      "Epoch 83/150 Loss: 0.3349\n",
      "Epoch 84/150 Loss: 0.3373\n",
      "Epoch 85/150 Loss: 0.3377\n",
      "Epoch 86/150 Loss: 0.3468\n",
      "Epoch 87/150 Loss: 0.3464\n",
      "Epoch 88/150 Loss: 0.3384\n",
      "Epoch 89/150 Loss: 0.3404\n",
      "Epoch 90/150 Loss: 0.3377\n",
      "Epoch 91/150 Loss: 0.3407\n",
      "Epoch 92/150 Loss: 0.3377\n",
      "Epoch 93/150 Loss: 0.3301\n",
      "Epoch 94/150 Loss: 0.3289\n",
      "Epoch 95/150 Loss: 0.3292\n",
      "Epoch 96/150 Loss: 0.3191\n",
      "Epoch 97/150 Loss: 0.3221\n",
      "Epoch 98/150 Loss: 0.3242\n",
      "Epoch 99/150 Loss: 0.3328\n",
      "Epoch 100/150 Loss: 0.3258\n",
      "Epoch 101/150 Loss: 0.3273\n",
      "Epoch 102/150 Loss: 0.3249\n",
      "Epoch 103/150 Loss: 0.3241\n",
      "Epoch 104/150 Loss: 0.3183\n",
      "Epoch 105/150 Loss: 0.3221\n",
      "Epoch 106/150 Loss: 0.3226\n",
      "Epoch 107/150 Loss: 0.3185\n",
      "Epoch 108/150 Loss: 0.3150\n",
      "Epoch 109/150 Loss: 0.3238\n",
      "Epoch 110/150 Loss: 0.3071\n",
      "Epoch 111/150 Loss: 0.3176\n",
      "Epoch 112/150 Loss: 0.3097\n",
      "Epoch 113/150 Loss: 0.3098\n",
      "Epoch 114/150 Loss: 0.3110\n",
      "Epoch 115/150 Loss: 0.3095\n",
      "Epoch 116/150 Loss: 0.3049\n",
      "Epoch 117/150 Loss: 0.3082\n",
      "Epoch 118/150 Loss: 0.3088\n",
      "Epoch 119/150 Loss: 0.3013\n",
      "Epoch 120/150 Loss: 0.3081\n",
      "Epoch 121/150 Loss: 0.3083\n",
      "Epoch 122/150 Loss: 0.3088\n",
      "Epoch 123/150 Loss: 0.3015\n",
      "Epoch 124/150 Loss: 0.3059\n",
      "Epoch 125/150 Loss: 0.3066\n",
      "Epoch 126/150 Loss: 0.3068\n",
      "Epoch 127/150 Loss: 0.3041\n",
      "Epoch 128/150 Loss: 0.2977\n",
      "Epoch 129/150 Loss: 0.3016\n",
      "Epoch 130/150 Loss: 0.3019\n",
      "Epoch 131/150 Loss: 0.2962\n",
      "Epoch 132/150 Loss: 0.3058\n",
      "Epoch 133/150 Loss: 0.2978\n",
      "Epoch 134/150 Loss: 0.2978\n",
      "Epoch 135/150 Loss: 0.2930\n",
      "Epoch 136/150 Loss: 0.2905\n",
      "Epoch 137/150 Loss: 0.2926\n",
      "Epoch 138/150 Loss: 0.2981\n",
      "Epoch 139/150 Loss: 0.2896\n",
      "Epoch 140/150 Loss: 0.2926\n",
      "Epoch 141/150 Loss: 0.2925\n",
      "Epoch 142/150 Loss: 0.2889\n",
      "Epoch 143/150 Loss: 0.2916\n",
      "Epoch 144/150 Loss: 0.2976\n",
      "Epoch 145/150 Loss: 0.2948\n",
      "Epoch 146/150 Loss: 0.2942\n",
      "Epoch 147/150 Loss: 0.2859\n",
      "Epoch 148/150 Loss: 0.3015\n",
      "Epoch 149/150 Loss: 0.2862\n",
      "Epoch 150/150 Loss: 0.2849\n",
      "Config {'w1': 0.2, 'w2': 0.6, 'lambda_eng': 0.2, 'lr': 0.001} achieved Wasserstein: 0.7664\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Step 6: Hyperparameter Tuning\n",
    "#############################################\n",
    "search_space = {\n",
    "    'w1': [0.1, 0.15, 0.2],    # Prior loss weight\n",
    "    'w2': [0.5, 0.55, 0.6],    # Triplet loss weight\n",
    "    'lambda_eng': [0.1, 0.15, 0.2],  # Engineered loss weight\n",
    "    'lr': [0.001]\n",
    "}\n",
    "\n",
    "results = []\n",
    "train_loader = DataLoader(FraudDataset(X_train_num, X_train_cat), batch_size=40, shuffle=True)\n",
    "\n",
    "for config in (dict(zip(search_space.keys(), vals)) for vals in product(*search_space.values())):\n",
    "    print(f\"\\n=== Training with {config} ===\")\n",
    "    model = train_model(config, train_loader)\n",
    "    ws_dist = evaluate_model(model)\n",
    "    results.append({**config, 'wasserstein': ws_dist})\n",
    "    print(f\"Config {config} achieved Wasserstein: {ws_dist:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best configuration:\n",
      "w1             0.200000\n",
      "w2             0.550000\n",
      "lambda_eng     0.150000\n",
      "lr             0.001000\n",
      "wasserstein    0.563277\n",
      "Name: 22, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Save and visualize results\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(r\"D:\\DS Northeastern\\DS 5500 - Capstone\\FraudFusion\\Data\\processed\\tuning_results.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(pd.pivot_table(results_df, values='wasserstein', index='w1', columns='w2'), \n",
    "            annot=True, fmt=\".3f\", cmap=\"viridis_r\")\n",
    "plt.title(\"Wasserstein Distance by Loss Weights\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBest configuration:\")\n",
    "print(results_df.loc[results_df['wasserstein'].idxmin()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying best configuration with multiple trials:\n",
      "\n",
      "=== Trial 1/3 ===\n",
      "Epoch 1/150 Loss: 1.0770\n",
      "Epoch 2/150 Loss: 0.7823\n",
      "Epoch 3/150 Loss: 0.7342\n",
      "Epoch 4/150 Loss: 0.7000\n",
      "Epoch 5/150 Loss: 0.6764\n",
      "Epoch 6/150 Loss: 0.6601\n",
      "Epoch 7/150 Loss: 0.6346\n",
      "Epoch 8/150 Loss: 0.6102\n",
      "Epoch 9/150 Loss: 0.5916\n",
      "Epoch 10/150 Loss: 0.5733\n",
      "Epoch 11/150 Loss: 0.5498\n",
      "Epoch 12/150 Loss: 0.5417\n",
      "Epoch 13/150 Loss: 0.5349\n",
      "Epoch 14/150 Loss: 0.5275\n",
      "Epoch 15/150 Loss: 0.5259\n",
      "Epoch 16/150 Loss: 0.5142\n",
      "Epoch 17/150 Loss: 0.4913\n",
      "Epoch 18/150 Loss: 0.4892\n",
      "Epoch 19/150 Loss: 0.4909\n",
      "Epoch 20/150 Loss: 0.4750\n",
      "Epoch 21/150 Loss: 0.4621\n",
      "Epoch 22/150 Loss: 0.4718\n",
      "Epoch 23/150 Loss: 0.4563\n",
      "Epoch 24/150 Loss: 0.4545\n",
      "Epoch 25/150 Loss: 0.4596\n",
      "Epoch 26/150 Loss: 0.4439\n",
      "Epoch 27/150 Loss: 0.4456\n",
      "Epoch 28/150 Loss: 0.4357\n",
      "Epoch 29/150 Loss: 0.4339\n",
      "Epoch 30/150 Loss: 0.4366\n",
      "Epoch 31/150 Loss: 0.4203\n",
      "Epoch 32/150 Loss: 0.4176\n",
      "Epoch 33/150 Loss: 0.4137\n",
      "Epoch 34/150 Loss: 0.4114\n",
      "Epoch 35/150 Loss: 0.4061\n",
      "Epoch 36/150 Loss: 0.4122\n",
      "Epoch 37/150 Loss: 0.4080\n",
      "Epoch 38/150 Loss: 0.3929\n",
      "Epoch 39/150 Loss: 0.3917\n",
      "Epoch 40/150 Loss: 0.3904\n",
      "Epoch 41/150 Loss: 0.3831\n",
      "Epoch 42/150 Loss: 0.3913\n",
      "Epoch 43/150 Loss: 0.3840\n",
      "Epoch 44/150 Loss: 0.3741\n",
      "Epoch 45/150 Loss: 0.3789\n",
      "Epoch 46/150 Loss: 0.3832\n",
      "Epoch 47/150 Loss: 0.3729\n",
      "Epoch 48/150 Loss: 0.3829\n",
      "Epoch 49/150 Loss: 0.3669\n",
      "Epoch 50/150 Loss: 0.3748\n",
      "Epoch 51/150 Loss: 0.3689\n",
      "Epoch 52/150 Loss: 0.3693\n",
      "Epoch 53/150 Loss: 0.3617\n",
      "Epoch 54/150 Loss: 0.3606\n",
      "Epoch 55/150 Loss: 0.3646\n",
      "Epoch 56/150 Loss: 0.3540\n",
      "Epoch 57/150 Loss: 0.3569\n",
      "Epoch 58/150 Loss: 0.3561\n",
      "Epoch 59/150 Loss: 0.3527\n",
      "Epoch 60/150 Loss: 0.3560\n",
      "Epoch 61/150 Loss: 0.3604\n",
      "Epoch 62/150 Loss: 0.3464\n",
      "Epoch 63/150 Loss: 0.3478\n",
      "Epoch 64/150 Loss: 0.3486\n",
      "Epoch 65/150 Loss: 0.3415\n",
      "Epoch 66/150 Loss: 0.3454\n",
      "Epoch 67/150 Loss: 0.3423\n",
      "Epoch 68/150 Loss: 0.3474\n",
      "Epoch 69/150 Loss: 0.3451\n",
      "Epoch 70/150 Loss: 0.3387\n",
      "Epoch 71/150 Loss: 0.3346\n",
      "Epoch 72/150 Loss: 0.3379\n",
      "Epoch 73/150 Loss: 0.3420\n",
      "Epoch 74/150 Loss: 0.3391\n",
      "Epoch 75/150 Loss: 0.3340\n",
      "Epoch 76/150 Loss: 0.3300\n",
      "Epoch 77/150 Loss: 0.3322\n",
      "Epoch 78/150 Loss: 0.3364\n",
      "Epoch 79/150 Loss: 0.3320\n",
      "Epoch 80/150 Loss: 0.3290\n",
      "Epoch 81/150 Loss: 0.3280\n",
      "Epoch 82/150 Loss: 0.3220\n",
      "Epoch 83/150 Loss: 0.3243\n",
      "Epoch 84/150 Loss: 0.3180\n",
      "Epoch 85/150 Loss: 0.3253\n",
      "Epoch 86/150 Loss: 0.3268\n",
      "Epoch 87/150 Loss: 0.3173\n",
      "Epoch 88/150 Loss: 0.3214\n",
      "Epoch 89/150 Loss: 0.3208\n",
      "Epoch 90/150 Loss: 0.3262\n",
      "Epoch 91/150 Loss: 0.3208\n",
      "Epoch 92/150 Loss: 0.3163\n",
      "Epoch 93/150 Loss: 0.3145\n",
      "Epoch 94/150 Loss: 0.3172\n",
      "Epoch 95/150 Loss: 0.3154\n",
      "Epoch 96/150 Loss: 0.3145\n",
      "Epoch 97/150 Loss: 0.3084\n",
      "Epoch 98/150 Loss: 0.3088\n",
      "Epoch 99/150 Loss: 0.3122\n",
      "Epoch 100/150 Loss: 0.3081\n",
      "Epoch 101/150 Loss: 0.3096\n",
      "Epoch 102/150 Loss: 0.3047\n",
      "Epoch 103/150 Loss: 0.3112\n",
      "Epoch 104/150 Loss: 0.3118\n",
      "Epoch 105/150 Loss: 0.3054\n",
      "Epoch 106/150 Loss: 0.3028\n",
      "Epoch 107/150 Loss: 0.2980\n",
      "Epoch 108/150 Loss: 0.3049\n",
      "Epoch 109/150 Loss: 0.2966\n",
      "Epoch 110/150 Loss: 0.2989\n",
      "Epoch 111/150 Loss: 0.2988\n",
      "Epoch 112/150 Loss: 0.2981\n",
      "Epoch 113/150 Loss: 0.2944\n",
      "Epoch 114/150 Loss: 0.2985\n",
      "Epoch 115/150 Loss: 0.2996\n",
      "Epoch 116/150 Loss: 0.2983\n",
      "Epoch 117/150 Loss: 0.3016\n",
      "Epoch 118/150 Loss: 0.2946\n",
      "Epoch 119/150 Loss: 0.2917\n",
      "Epoch 120/150 Loss: 0.2870\n",
      "Epoch 121/150 Loss: 0.2940\n",
      "Epoch 122/150 Loss: 0.2938\n",
      "Epoch 123/150 Loss: 0.2995\n",
      "Epoch 124/150 Loss: 0.2993\n",
      "Epoch 125/150 Loss: 0.2893\n",
      "Epoch 126/150 Loss: 0.2940\n",
      "Epoch 127/150 Loss: 0.2890\n",
      "Epoch 128/150 Loss: 0.2939\n",
      "Epoch 129/150 Loss: 0.2925\n",
      "Epoch 130/150 Loss: 0.2870\n",
      "Epoch 131/150 Loss: 0.2903\n",
      "Epoch 132/150 Loss: 0.2830\n",
      "Epoch 133/150 Loss: 0.2883\n",
      "Epoch 134/150 Loss: 0.2875\n",
      "Epoch 135/150 Loss: 0.2860\n",
      "Epoch 136/150 Loss: 0.2877\n",
      "Epoch 137/150 Loss: 0.2837\n",
      "Epoch 138/150 Loss: 0.2784\n",
      "Epoch 139/150 Loss: 0.2850\n",
      "Epoch 140/150 Loss: 0.2811\n",
      "Epoch 141/150 Loss: 0.2869\n",
      "Epoch 142/150 Loss: 0.2878\n",
      "Epoch 143/150 Loss: 0.2803\n",
      "Epoch 144/150 Loss: 0.2819\n",
      "Epoch 145/150 Loss: 0.2772\n",
      "Epoch 146/150 Loss: 0.2826\n",
      "Epoch 147/150 Loss: 0.2826\n",
      "Epoch 148/150 Loss: 0.2803\n",
      "Epoch 149/150 Loss: 0.2803\n",
      "Epoch 150/150 Loss: 0.2749\n",
      "\n",
      "Trial 1 Results:\n",
      "  Wasserstein: 0.6432\n",
      "\n",
      "=== Trial 2/3 ===\n",
      "Epoch 1/150 Loss: 1.0693\n",
      "Epoch 2/150 Loss: 0.7730\n",
      "Epoch 3/150 Loss: 0.7306\n",
      "Epoch 4/150 Loss: 0.7027\n",
      "Epoch 5/150 Loss: 0.6588\n",
      "Epoch 6/150 Loss: 0.6367\n",
      "Epoch 7/150 Loss: 0.6014\n",
      "Epoch 8/150 Loss: 0.5822\n",
      "Epoch 9/150 Loss: 0.5629\n",
      "Epoch 10/150 Loss: 0.5594\n",
      "Epoch 11/150 Loss: 0.5434\n",
      "Epoch 12/150 Loss: 0.5355\n",
      "Epoch 13/150 Loss: 0.5191\n",
      "Epoch 14/150 Loss: 0.5142\n",
      "Epoch 15/150 Loss: 0.5008\n",
      "Epoch 16/150 Loss: 0.4932\n",
      "Epoch 17/150 Loss: 0.4908\n",
      "Epoch 18/150 Loss: 0.4918\n",
      "Epoch 19/150 Loss: 0.4804\n",
      "Epoch 20/150 Loss: 0.4826\n",
      "Epoch 21/150 Loss: 0.4679\n",
      "Epoch 22/150 Loss: 0.4634\n",
      "Epoch 23/150 Loss: 0.4601\n",
      "Epoch 24/150 Loss: 0.4495\n",
      "Epoch 25/150 Loss: 0.4442\n",
      "Epoch 26/150 Loss: 0.4412\n",
      "Epoch 27/150 Loss: 0.4307\n",
      "Epoch 28/150 Loss: 0.4334\n",
      "Epoch 29/150 Loss: 0.4301\n",
      "Epoch 30/150 Loss: 0.4202\n",
      "Epoch 31/150 Loss: 0.4204\n",
      "Epoch 32/150 Loss: 0.4213\n",
      "Epoch 33/150 Loss: 0.4225\n",
      "Epoch 34/150 Loss: 0.4246\n",
      "Epoch 35/150 Loss: 0.4071\n",
      "Epoch 36/150 Loss: 0.4056\n",
      "Epoch 37/150 Loss: 0.4124\n",
      "Epoch 38/150 Loss: 0.4028\n",
      "Epoch 39/150 Loss: 0.3957\n",
      "Epoch 40/150 Loss: 0.4047\n",
      "Epoch 41/150 Loss: 0.4011\n",
      "Epoch 42/150 Loss: 0.4016\n",
      "Epoch 43/150 Loss: 0.3961\n",
      "Epoch 44/150 Loss: 0.3885\n",
      "Epoch 45/150 Loss: 0.3879\n",
      "Epoch 46/150 Loss: 0.3894\n",
      "Epoch 47/150 Loss: 0.3841\n",
      "Epoch 48/150 Loss: 0.3772\n",
      "Epoch 49/150 Loss: 0.3748\n",
      "Epoch 50/150 Loss: 0.3782\n",
      "Epoch 51/150 Loss: 0.3816\n",
      "Epoch 52/150 Loss: 0.3737\n",
      "Epoch 53/150 Loss: 0.3760\n",
      "Epoch 54/150 Loss: 0.3685\n",
      "Epoch 55/150 Loss: 0.3735\n",
      "Epoch 56/150 Loss: 0.3642\n",
      "Epoch 57/150 Loss: 0.3680\n",
      "Epoch 58/150 Loss: 0.3674\n",
      "Epoch 59/150 Loss: 0.3644\n",
      "Epoch 60/150 Loss: 0.3661\n",
      "Epoch 61/150 Loss: 0.3603\n",
      "Epoch 62/150 Loss: 0.3591\n",
      "Epoch 63/150 Loss: 0.3589\n",
      "Epoch 64/150 Loss: 0.3515\n",
      "Epoch 65/150 Loss: 0.3547\n",
      "Epoch 66/150 Loss: 0.3568\n",
      "Epoch 67/150 Loss: 0.3551\n",
      "Epoch 68/150 Loss: 0.3545\n",
      "Epoch 69/150 Loss: 0.3526\n",
      "Epoch 70/150 Loss: 0.3501\n",
      "Epoch 71/150 Loss: 0.3446\n",
      "Epoch 72/150 Loss: 0.3452\n",
      "Epoch 73/150 Loss: 0.3424\n",
      "Epoch 74/150 Loss: 0.3460\n",
      "Epoch 75/150 Loss: 0.3414\n",
      "Epoch 76/150 Loss: 0.3437\n",
      "Epoch 77/150 Loss: 0.3423\n",
      "Epoch 78/150 Loss: 0.3385\n",
      "Epoch 79/150 Loss: 0.3329\n",
      "Epoch 80/150 Loss: 0.3396\n",
      "Epoch 81/150 Loss: 0.3266\n",
      "Epoch 82/150 Loss: 0.3306\n",
      "Epoch 83/150 Loss: 0.3290\n",
      "Epoch 84/150 Loss: 0.3235\n",
      "Epoch 85/150 Loss: 0.3320\n",
      "Epoch 86/150 Loss: 0.3235\n",
      "Epoch 87/150 Loss: 0.3304\n",
      "Epoch 88/150 Loss: 0.3289\n",
      "Epoch 89/150 Loss: 0.3211\n",
      "Epoch 90/150 Loss: 0.3216\n",
      "Epoch 91/150 Loss: 0.3271\n",
      "Epoch 92/150 Loss: 0.3256\n",
      "Epoch 93/150 Loss: 0.3215\n",
      "Epoch 94/150 Loss: 0.3148\n",
      "Epoch 95/150 Loss: 0.3216\n",
      "Epoch 96/150 Loss: 0.3117\n",
      "Epoch 97/150 Loss: 0.3161\n",
      "Epoch 98/150 Loss: 0.3106\n",
      "Epoch 99/150 Loss: 0.3106\n",
      "Epoch 100/150 Loss: 0.3135\n",
      "Epoch 101/150 Loss: 0.3077\n",
      "Epoch 102/150 Loss: 0.3106\n",
      "Epoch 103/150 Loss: 0.3063\n",
      "Epoch 104/150 Loss: 0.3031\n",
      "Epoch 105/150 Loss: 0.3049\n",
      "Epoch 106/150 Loss: 0.3049\n",
      "Epoch 107/150 Loss: 0.3042\n",
      "Epoch 108/150 Loss: 0.3054\n",
      "Epoch 109/150 Loss: 0.3006\n",
      "Epoch 110/150 Loss: 0.3097\n",
      "Epoch 111/150 Loss: 0.2994\n",
      "Epoch 112/150 Loss: 0.3039\n",
      "Epoch 113/150 Loss: 0.3046\n",
      "Epoch 114/150 Loss: 0.2981\n",
      "Epoch 115/150 Loss: 0.2987\n",
      "Epoch 116/150 Loss: 0.2959\n",
      "Epoch 117/150 Loss: 0.2975\n",
      "Epoch 118/150 Loss: 0.2879\n",
      "Epoch 119/150 Loss: 0.2967\n",
      "Epoch 120/150 Loss: 0.3009\n",
      "Epoch 121/150 Loss: 0.3015\n",
      "Epoch 122/150 Loss: 0.2914\n",
      "Epoch 123/150 Loss: 0.3002\n",
      "Epoch 124/150 Loss: 0.2966\n",
      "Epoch 125/150 Loss: 0.2876\n",
      "Epoch 126/150 Loss: 0.2923\n",
      "Epoch 127/150 Loss: 0.2927\n",
      "Epoch 128/150 Loss: 0.2854\n",
      "Epoch 129/150 Loss: 0.2877\n",
      "Epoch 130/150 Loss: 0.2919\n",
      "Epoch 131/150 Loss: 0.2857\n",
      "Epoch 132/150 Loss: 0.2848\n",
      "Epoch 133/150 Loss: 0.2906\n",
      "Epoch 134/150 Loss: 0.2881\n",
      "Epoch 135/150 Loss: 0.2839\n",
      "Epoch 136/150 Loss: 0.2881\n",
      "Epoch 137/150 Loss: 0.2779\n",
      "Epoch 138/150 Loss: 0.2800\n",
      "Epoch 139/150 Loss: 0.2825\n",
      "Epoch 140/150 Loss: 0.2821\n",
      "Epoch 141/150 Loss: 0.2806\n",
      "Epoch 142/150 Loss: 0.2812\n",
      "Epoch 143/150 Loss: 0.2770\n",
      "Epoch 144/150 Loss: 0.2791\n",
      "Epoch 145/150 Loss: 0.2833\n",
      "Epoch 146/150 Loss: 0.2834\n",
      "Epoch 147/150 Loss: 0.2825\n",
      "Epoch 148/150 Loss: 0.2779\n",
      "Epoch 149/150 Loss: 0.2759\n",
      "Epoch 150/150 Loss: 0.2782\n",
      "\n",
      "Trial 2 Results:\n",
      "  Wasserstein: 0.7982\n",
      "\n",
      "=== Trial 3/3 ===\n",
      "Epoch 1/150 Loss: 1.0446\n",
      "Epoch 2/150 Loss: 0.7598\n",
      "Epoch 3/150 Loss: 0.7136\n",
      "Epoch 4/150 Loss: 0.7016\n",
      "Epoch 5/150 Loss: 0.6637\n",
      "Epoch 6/150 Loss: 0.6390\n",
      "Epoch 7/150 Loss: 0.6034\n",
      "Epoch 8/150 Loss: 0.5796\n",
      "Epoch 9/150 Loss: 0.5518\n",
      "Epoch 10/150 Loss: 0.5466\n",
      "Epoch 11/150 Loss: 0.5332\n",
      "Epoch 12/150 Loss: 0.5275\n",
      "Epoch 13/150 Loss: 0.5324\n",
      "Epoch 14/150 Loss: 0.5007\n",
      "Epoch 15/150 Loss: 0.4979\n",
      "Epoch 16/150 Loss: 0.4993\n",
      "Epoch 17/150 Loss: 0.4867\n",
      "Epoch 18/150 Loss: 0.4687\n",
      "Epoch 19/150 Loss: 0.4721\n",
      "Epoch 20/150 Loss: 0.4665\n",
      "Epoch 21/150 Loss: 0.4608\n",
      "Epoch 22/150 Loss: 0.4624\n",
      "Epoch 23/150 Loss: 0.4587\n",
      "Epoch 24/150 Loss: 0.4560\n",
      "Epoch 25/150 Loss: 0.4391\n",
      "Epoch 26/150 Loss: 0.4369\n",
      "Epoch 27/150 Loss: 0.4452\n",
      "Epoch 28/150 Loss: 0.4336\n",
      "Epoch 29/150 Loss: 0.4468\n",
      "Epoch 30/150 Loss: 0.4298\n",
      "Epoch 31/150 Loss: 0.4225\n",
      "Epoch 32/150 Loss: 0.4226\n",
      "Epoch 33/150 Loss: 0.4199\n",
      "Epoch 34/150 Loss: 0.4160\n",
      "Epoch 35/150 Loss: 0.4175\n",
      "Epoch 36/150 Loss: 0.4077\n",
      "Epoch 37/150 Loss: 0.3997\n",
      "Epoch 38/150 Loss: 0.4038\n",
      "Epoch 39/150 Loss: 0.3974\n",
      "Epoch 40/150 Loss: 0.3978\n",
      "Epoch 41/150 Loss: 0.3964\n",
      "Epoch 42/150 Loss: 0.3992\n",
      "Epoch 43/150 Loss: 0.3881\n",
      "Epoch 44/150 Loss: 0.3938\n",
      "Epoch 45/150 Loss: 0.3887\n",
      "Epoch 46/150 Loss: 0.3875\n",
      "Epoch 47/150 Loss: 0.3845\n",
      "Epoch 48/150 Loss: 0.3780\n",
      "Epoch 49/150 Loss: 0.3849\n",
      "Epoch 50/150 Loss: 0.3873\n",
      "Epoch 51/150 Loss: 0.3761\n",
      "Epoch 52/150 Loss: 0.3737\n",
      "Epoch 53/150 Loss: 0.3718\n",
      "Epoch 54/150 Loss: 0.3638\n",
      "Epoch 55/150 Loss: 0.3648\n",
      "Epoch 56/150 Loss: 0.3657\n",
      "Epoch 57/150 Loss: 0.3723\n",
      "Epoch 58/150 Loss: 0.3655\n",
      "Epoch 59/150 Loss: 0.3664\n",
      "Epoch 60/150 Loss: 0.3625\n",
      "Epoch 61/150 Loss: 0.3596\n",
      "Epoch 62/150 Loss: 0.3555\n",
      "Epoch 63/150 Loss: 0.3625\n",
      "Epoch 64/150 Loss: 0.3571\n",
      "Epoch 65/150 Loss: 0.3562\n",
      "Epoch 66/150 Loss: 0.3528\n",
      "Epoch 67/150 Loss: 0.3522\n",
      "Epoch 68/150 Loss: 0.3490\n",
      "Epoch 69/150 Loss: 0.3510\n",
      "Epoch 70/150 Loss: 0.3453\n",
      "Epoch 71/150 Loss: 0.3416\n",
      "Epoch 72/150 Loss: 0.3436\n",
      "Epoch 73/150 Loss: 0.3448\n",
      "Epoch 74/150 Loss: 0.3377\n",
      "Epoch 75/150 Loss: 0.3413\n",
      "Epoch 76/150 Loss: 0.3397\n",
      "Epoch 77/150 Loss: 0.3444\n",
      "Epoch 78/150 Loss: 0.3374\n",
      "Epoch 79/150 Loss: 0.3368\n",
      "Epoch 80/150 Loss: 0.3275\n",
      "Epoch 81/150 Loss: 0.3354\n",
      "Epoch 82/150 Loss: 0.3297\n",
      "Epoch 83/150 Loss: 0.3332\n",
      "Epoch 84/150 Loss: 0.3322\n",
      "Epoch 85/150 Loss: 0.3233\n",
      "Epoch 86/150 Loss: 0.3169\n",
      "Epoch 87/150 Loss: 0.3236\n",
      "Epoch 88/150 Loss: 0.3225\n",
      "Epoch 89/150 Loss: 0.3193\n",
      "Epoch 90/150 Loss: 0.3196\n",
      "Epoch 91/150 Loss: 0.3193\n",
      "Epoch 92/150 Loss: 0.3169\n",
      "Epoch 93/150 Loss: 0.3214\n",
      "Epoch 94/150 Loss: 0.3161\n",
      "Epoch 95/150 Loss: 0.3126\n",
      "Epoch 96/150 Loss: 0.3171\n",
      "Epoch 97/150 Loss: 0.3094\n",
      "Epoch 98/150 Loss: 0.3066\n",
      "Epoch 99/150 Loss: 0.3073\n",
      "Epoch 100/150 Loss: 0.3084\n",
      "Epoch 101/150 Loss: 0.2990\n",
      "Epoch 102/150 Loss: 0.3056\n",
      "Epoch 103/150 Loss: 0.2999\n",
      "Epoch 104/150 Loss: 0.3081\n",
      "Epoch 105/150 Loss: 0.3089\n",
      "Epoch 106/150 Loss: 0.3048\n",
      "Epoch 107/150 Loss: 0.3002\n",
      "Epoch 108/150 Loss: 0.3034\n",
      "Epoch 109/150 Loss: 0.3043\n",
      "Epoch 110/150 Loss: 0.3067\n",
      "Epoch 111/150 Loss: 0.2981\n",
      "Epoch 112/150 Loss: 0.2996\n",
      "Epoch 113/150 Loss: 0.3009\n",
      "Epoch 114/150 Loss: 0.2928\n",
      "Epoch 115/150 Loss: 0.2998\n",
      "Epoch 116/150 Loss: 0.2975\n",
      "Epoch 117/150 Loss: 0.2964\n",
      "Epoch 118/150 Loss: 0.2986\n",
      "Epoch 119/150 Loss: 0.2961\n",
      "Epoch 120/150 Loss: 0.2900\n",
      "Epoch 121/150 Loss: 0.2938\n",
      "Epoch 122/150 Loss: 0.2890\n",
      "Epoch 123/150 Loss: 0.2970\n",
      "Epoch 124/150 Loss: 0.2908\n",
      "Epoch 125/150 Loss: 0.2884\n",
      "Epoch 126/150 Loss: 0.2918\n",
      "Epoch 127/150 Loss: 0.2875\n",
      "Epoch 128/150 Loss: 0.2903\n",
      "Epoch 129/150 Loss: 0.2885\n",
      "Epoch 130/150 Loss: 0.2795\n",
      "Epoch 131/150 Loss: 0.2822\n",
      "Epoch 132/150 Loss: 0.2907\n",
      "Epoch 133/150 Loss: 0.2831\n",
      "Epoch 134/150 Loss: 0.2868\n",
      "Epoch 135/150 Loss: 0.2841\n",
      "Epoch 136/150 Loss: 0.2901\n",
      "Epoch 137/150 Loss: 0.2866\n",
      "Epoch 138/150 Loss: 0.2947\n",
      "Epoch 139/150 Loss: 0.2841\n",
      "Epoch 140/150 Loss: 0.2888\n",
      "Epoch 141/150 Loss: 0.2808\n",
      "Epoch 142/150 Loss: 0.2859\n",
      "Epoch 143/150 Loss: 0.2821\n",
      "Epoch 144/150 Loss: 0.2811\n",
      "Epoch 145/150 Loss: 0.2825\n",
      "Epoch 146/150 Loss: 0.2782\n",
      "Epoch 147/150 Loss: 0.2834\n",
      "Epoch 148/150 Loss: 0.2839\n",
      "Epoch 149/150 Loss: 0.2774\n",
      "Epoch 150/150 Loss: 0.2758\n",
      "\n",
      "Trial 3 Results:\n",
      "  Wasserstein: 0.7742\n",
      "\n",
      "Overall Results:\n",
      "Mean Wasserstein: 0.7386  0.0681\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################\n",
    "# Step 1: Add Seed Control\n",
    "#############################################\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#############################################\n",
    "# Step 2: Run Multiple Trials\n",
    "#############################################\n",
    "# Best configuration from previous tuning\n",
    "best_config = {\n",
    "    'w1': 0.2,\n",
    "    'w2': 0.55,\n",
    "    'lambda_eng': 0.15,\n",
    "    'lr': 0.001\n",
    "}\n",
    "\n",
    "num_trials = 3\n",
    "trial_results = []\n",
    "\n",
    "print(\"\\nVerifying best configuration with multiple trials:\")\n",
    "for trial in range(num_trials):\n",
    "    set_seed(42 + trial)  # Different seed for each trial\n",
    "    print(f\"\\n=== Trial {trial + 1}/{num_trials} ===\")\n",
    "    \n",
    "    # Create dataset and loader\n",
    "    train_dataset = FraudDataset(X_train_num, X_train_cat)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=40, shuffle=True)\n",
    "    \n",
    "    # Train model and get trained model back\n",
    "    trained_model = train_model(best_config, train_loader)\n",
    "    ws_dist = evaluate_model(trained_model)\n",
    "    \n",
    "    trial_results.append({\n",
    "        'trial': trial + 1,\n",
    "        'wasserstein': ws_dist\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nTrial {trial + 1} Results:\")\n",
    "    print(f\"  Wasserstein: {ws_dist:.4f}\")\n",
    "\n",
    "# Calculate statistics\n",
    "wasserstein_scores = [r['wasserstein'] for r in trial_results]\n",
    "mean_wasserstein = np.mean(wasserstein_scores)\n",
    "std_wasserstein = np.std(wasserstein_scores)\n",
    "\n",
    "print(\"\\nOverall Results:\")\n",
    "print(f\"Mean Wasserstein: {mean_wasserstein:.4f}  {std_wasserstein:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(1, num_trials + 1), wasserstein_scores)\n",
    "plt.axhline(y=mean_wasserstein, color='r', linestyle='--', \n",
    "           label=f'Mean: {mean_wasserstein:.4f}')\n",
    "plt.fill_between(range(1, num_trials + 1), \n",
    "                 mean_wasserstein - std_wasserstein,\n",
    "                 mean_wasserstein + std_wasserstein,\n",
    "                 alpha=0.2, color='r')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Wasserstein Distance')\n",
    "plt.title('Wasserstein Distance Across Trials')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
