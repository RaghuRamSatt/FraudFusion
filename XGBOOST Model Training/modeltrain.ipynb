{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"fraudTrain.csv\"  \n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cols\n",
    "drop_cols = [\"Unnamed: 0\", \"trans_num\", \"first\", \"last\", \"street\", \"cc_num\", \"unix_time\"]\n",
    "df.drop(columns=drop_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Convert datetime features\n",
    "df[\"trans_date_trans_time\"] = pd.to_datetime(df[\"trans_date_trans_time\"])\n",
    "df[\"dob\"] = pd.to_datetime(df[\"dob\"])\n",
    "df[\"hour\"] = df[\"trans_date_trans_time\"].dt.hour\n",
    "df[\"day\"] = df[\"trans_date_trans_time\"].dt.day\n",
    "df[\"weekday\"] = df[\"trans_date_trans_time\"].dt.weekday\n",
    "df[\"month\"] = df[\"trans_date_trans_time\"].dt.month\n",
    "df[\"age\"] = df[\"trans_date_trans_time\"].dt.year - df[\"dob\"].dt.year\n",
    "\n",
    "df.drop(columns=[\"dob\", \"trans_date_trans_time\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance between transaction location and merchant location\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "df[\"distance\"] = haversine(df[\"lat\"], df[\"long\"], df[\"merch_lat\"], df[\"merch_long\"])\n",
    "df.drop(columns=[\"lat\", \"long\", \"merch_lat\", \"merch_long\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Encoding Strategy\n",
    "categorical_cols = [\"merchant\", \"category\", \"gender\", \"state\", \"job\", \"city\"]\n",
    "\n",
    "# Define encoding methods per feature\n",
    "one_hot_cols = [\"category\", \"gender\", \"state\"]\n",
    "target_cols = [\"merchant\", \"job\"]\n",
    "freq_cols = [\"city\"]\n",
    "\n",
    "# One-Hot Encoding\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "encoded_array = ohe.fit_transform(df[one_hot_cols])\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=ohe.get_feature_names_out(one_hot_cols))\n",
    "\n",
    "df = df.drop(columns=one_hot_cols).reset_index(drop=True)\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "# Target Encoding (Fraud Rate Per Category)\n",
    "encoders = {}\n",
    "for col in target_cols:\n",
    "    encoders[col] = df.groupby(col)[\"is_fraud\"].mean()\n",
    "    df[col] = df[col].map(encoders[col]).fillna(df[\"is_fraud\"].mean())  # Unseen categories get global fraud rate\n",
    "\n",
    "# Frequency Encoding\n",
    "for col in freq_cols:\n",
    "    encoders[col] = df[col].value_counts(normalize=True)\n",
    "    df[col] = df[col].map(encoders[col]).fillna(0)  # Unseen categories get 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"is_fraud\"])\n",
    "y = df[\"is_fraud\"]\n",
    "\n",
    "# split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# normalize\n",
    "scaler = StandardScaler()\n",
    "num_cols = [\"amt\", \"city_pop\", \"distance\", \"age\"]\n",
    "X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "X_val[num_cols] = scaler.transform(X_val[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "xgb_model = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    tree_method=\"hist\",  # Default method\n",
    "    device=\"cuda\"  # Enable GPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owner\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [01:35:19] WARNING: D:\\bld\\xgboost-split_1737531311373\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, scoring=\"f1\", cv=3, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# best model\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    257834\n",
      "           1       0.98      0.88      0.92      1501\n",
      "\n",
      "    accuracy                           1.00    259335\n",
      "   macro avg       0.99      0.94      0.96    259335\n",
      "weighted avg       1.00      1.00      1.00    259335\n",
      "\n",
      "Best Hyperparameters: {'colsample_bytree': 1.0, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# evaluate on validation set\n",
    "y_pred = best_model.predict(X_val)\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n",
    "\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset in chunks\n",
    "file_path_test = \"fraudTest.csv\"\n",
    "chunk_size = 10000  # Process 10,000 rows at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(chunk, encoders, scaler, categorical_cols, num_cols):\n",
    "    \"\"\"Preprocesses test data, applying encoding & normalization.\"\"\"\n",
    "    chunk.drop(columns=drop_cols, errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # Convert datetime features\n",
    "    chunk[\"trans_date_trans_time\"] = pd.to_datetime(chunk[\"trans_date_trans_time\"])\n",
    "    chunk[\"dob\"] = pd.to_datetime(chunk[\"dob\"])\n",
    "    chunk[\"hour\"] = chunk[\"trans_date_trans_time\"].dt.hour\n",
    "    chunk[\"day\"] = chunk[\"trans_date_trans_time\"].dt.day\n",
    "    chunk[\"weekday\"] = chunk[\"trans_date_trans_time\"].dt.weekday\n",
    "    chunk[\"month\"] = chunk[\"trans_date_trans_time\"].dt.month\n",
    "    chunk[\"age\"] = chunk[\"trans_date_trans_time\"].dt.year - chunk[\"dob\"].dt.year\n",
    "    chunk.drop(columns=[\"dob\", \"trans_date_trans_time\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # distance\n",
    "    chunk[\"distance\"] = haversine(chunk[\"lat\"], chunk[\"long\"], chunk[\"merch_lat\"], chunk[\"merch_long\"])\n",
    "    chunk.drop(columns=[\"lat\", \"long\", \"merch_lat\", \"merch_long\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # one hot encoding\n",
    "    encoded_array = ohe.transform(chunk[one_hot_cols])\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=ohe.get_feature_names_out(one_hot_cols))\n",
    "\n",
    "    chunk = chunk.drop(columns=one_hot_cols).reset_index(drop=True)\n",
    "    chunk = pd.concat([chunk, encoded_df], axis=1)\n",
    "\n",
    "    # target and freq encoding\n",
    "    for col in target_cols + freq_cols:\n",
    "        if col in encoders:\n",
    "            chunk[col] = chunk[col].map(encoders[col]).fillna(0 if col in freq_cols else df[\"is_fraud\"].mean())\n",
    "\n",
    "    # normalize\n",
    "    chunk[num_cols] = scaler.transform(chunk[num_cols])\n",
    "\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Set Performance:\n",
      "              precision    recall  f1-score      support\n",
      "0              0.998370  0.999876  0.999122  9885.250000\n",
      "1              0.865713  0.539680  0.654513    38.303571\n",
      "accuracy       0.998252  0.998252  0.998252     0.998252\n",
      "macro avg      0.949899  0.787635  0.844675  9923.553571\n",
      "weighted avg   0.998186  0.998252  0.998003  9923.553571\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for chunk in pd.read_csv(file_path_test, chunksize=chunk_size):\n",
    "    chunk = preprocess_data(chunk, encoders, scaler, categorical_cols, num_cols)\n",
    "    X_test_chunk = chunk.drop(columns=[\"is_fraud\"])\n",
    "    y_test_chunk = chunk[\"is_fraud\"]\n",
    "    y_pred_chunk = best_model.predict(X_test_chunk)\n",
    "\n",
    "    results.append(classification_report(y_test_chunk, y_pred_chunk, output_dict=True, zero_division=0))\n",
    "\n",
    "# aggregate reports\n",
    "final_summary = {}\n",
    "\n",
    "for label in [\"0\", \"1\", \"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "    if label in results[0]:  # Ensure label exists in at least one chunk\n",
    "        if isinstance(results[0].get(label, {}), dict):  \n",
    "            final_summary[label] = {\n",
    "                key: np.mean([r.get(label, {}).get(key, 0) for r in results])  \n",
    "                for key in [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "            }\n",
    "        else:\n",
    "            final_summary[label] = np.mean([r.get(label, 0) for r in results])  \n",
    "\n",
    "\n",
    "final_report_df = pd.DataFrame(final_summary).T\n",
    "\n",
    "# Print Final Test Performance\n",
    "print(\"Final Test Set Performance:\")\n",
    "print(final_report_df.to_string())  # Prevent truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
