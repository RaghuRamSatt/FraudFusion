{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 trans_date_trans_time            cc_num  \\\n",
      "0           0   2019-01-01 00:00:18  2703186189652095   \n",
      "1           1   2019-01-01 00:00:44      630423337322   \n",
      "2           2   2019-01-01 00:00:51    38859492057661   \n",
      "3           3   2019-01-01 00:01:16  3534093764340240   \n",
      "4           4   2019-01-01 00:03:06   375534208663984   \n",
      "\n",
      "                             merchant       category     amt      first  \\\n",
      "0          fraud_Rippin, Kub and Mann       misc_net    4.97   Jennifer   \n",
      "1     fraud_Heller, Gutmann and Zieme    grocery_pos  107.23  Stephanie   \n",
      "2                fraud_Lind-Buckridge  entertainment  220.11     Edward   \n",
      "3  fraud_Kutch, Hermiston and Farrell  gas_transport   45.00     Jeremy   \n",
      "4                 fraud_Keeling-Crist       misc_pos   41.96      Tyler   \n",
      "\n",
      "      last gender                        street  ...      lat      long  \\\n",
      "0    Banks      F                561 Perry Cove  ...  36.0788  -81.1781   \n",
      "1     Gill      F  43039 Riley Greens Suite 393  ...  48.8878 -118.2105   \n",
      "2  Sanchez      M      594 White Dale Suite 530  ...  42.1808 -112.2620   \n",
      "3    White      M   9443 Cynthia Court Apt. 038  ...  46.2306 -112.1138   \n",
      "4   Garcia      M              408 Bradley Rest  ...  38.4207  -79.4629   \n",
      "\n",
      "   city_pop                                job         dob  \\\n",
      "0      3495          Psychologist, counselling  1988-03-09   \n",
      "1       149  Special educational needs teacher  1978-06-21   \n",
      "2      4154        Nature conservation officer  1962-01-19   \n",
      "3      1939                    Patent attorney  1967-01-12   \n",
      "4        99     Dance movement psychotherapist  1986-03-28   \n",
      "\n",
      "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
      "0  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315   \n",
      "1  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462   \n",
      "2  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481   \n",
      "3  6b849c168bdad6f867558c3793159a81  1325376076  47.034331 -112.561071   \n",
      "4  a41d7549acf90789359a9aa5346dcb46  1325376186  38.674999  -78.632459   \n",
      "\n",
      "   is_fraud  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "Processed data and scaler saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train = pd.read_csv('fraudTrain.csv')\n",
    "test = pd.read_csv('fraudTest.csv')\n",
    "\n",
    "#combine\n",
    "df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "print(df.head())\n",
    "\n",
    "# drop cols\n",
    "drop_cols = [\"Unnamed: 0\", \"trans_num\", \"first\", \"last\", \"street\", \"cc_num\", \"unix_time\"]\n",
    "df.drop(columns=drop_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Convert datetime features\n",
    "df[\"trans_date_trans_time\"] = pd.to_datetime(df[\"trans_date_trans_time\"])\n",
    "df[\"dob\"] = pd.to_datetime(df[\"dob\"])\n",
    "df[\"hour\"] = df[\"trans_date_trans_time\"].dt.hour\n",
    "df[\"day\"] = df[\"trans_date_trans_time\"].dt.day\n",
    "df[\"weekday\"] = df[\"trans_date_trans_time\"].dt.weekday\n",
    "df[\"month\"] = df[\"trans_date_trans_time\"].dt.month\n",
    "df[\"age\"] = df[\"trans_date_trans_time\"].dt.year - df[\"dob\"].dt.year\n",
    "\n",
    "df.drop(columns=[\"dob\", \"trans_date_trans_time\"], inplace=True)\n",
    "\n",
    "# distance between transaction location and merchant location\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "df[\"distance\"] = haversine(df[\"lat\"], df[\"long\"], df[\"merch_lat\"], df[\"merch_long\"])\n",
    "#df.drop(columns=[\"lat\", \"long\", \"merch_lat\", \"merch_long\"], inplace=True)\n",
    "\n",
    "df['amt'] = np.log1p(df['amt'])\n",
    "df['city_pop'] = np.log1p(df['city_pop'])\n",
    "\n",
    "\n",
    "# Categorical Encoding Strategy\n",
    "categorical_cols = [\"merchant\", \"category\", \"gender\", \"state\", \"job\", \"city\"]\n",
    "\n",
    "# Define encoding methods per feature\n",
    "one_hot_cols = [\"category\", \"gender\", \"state\"]\n",
    "target_cols = [\"merchant\", \"job\"]\n",
    "freq_cols = [\"city\"]\n",
    "\n",
    "# One-Hot Encoding\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "encoded_array = ohe.fit_transform(df[one_hot_cols])\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=ohe.get_feature_names_out(one_hot_cols))\n",
    "\n",
    "df = df.drop(columns=one_hot_cols).reset_index(drop=True)\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "# Target Encoding (Fraud Rate Per Category)\n",
    "encoders = {}\n",
    "for col in target_cols:\n",
    "    encoders[col] = df.groupby(col)[\"is_fraud\"].mean()\n",
    "    df[col] = df[col].map(encoders[col]).fillna(df[\"is_fraud\"].mean())  # Unseen categories get global fraud rate\n",
    "\n",
    "# Frequency Encoding\n",
    "for col in freq_cols:\n",
    "    encoders[col] = df[col].value_counts(normalize=True)\n",
    "    df[col] = df[col].map(encoders[col]).fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "num_features = ['amt', 'city_pop', 'age', 'hour', 'day', 'month', 'weekday','distance', 'lat' , 'long', 'merch_lat', 'merch_long']\n",
    "\n",
    "# Initialize and fit the scaler on these numeric features.\n",
    "scaler = StandardScaler()\n",
    "df[num_features] = scaler.fit_transform(df[num_features])\n",
    "\n",
    "# Separate the target variable from the features.\n",
    "target = 'is_fraud'\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "\n",
    "# 80-20 split for train test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                            stratify=y, random_state=42)\n",
    "\n",
    "# 15-65 split for train validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, \n",
    "                                                  test_size=0.1875, stratify=y_train_val, random_state=42)\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save the processed splits to CSV files.\n",
    "X_train.to_csv('data_withoutembeddings/X_train.csv', index=False)\n",
    "y_train.to_csv('data_withoutembeddings/y_train.csv', index=False)\n",
    "\n",
    "X_val.to_csv('data_withoutembeddings/X_val.csv', index=False)\n",
    "y_val.to_csv('data_withoutembeddings/y_val.csv', index=False)\n",
    "\n",
    "X_test.to_csv('data_withoutembeddings/X_test.csv', index=False)\n",
    "y_test.to_csv('data_withoutembeddings/y_test.csv', index=False)\n",
    "\n",
    "# Also, save the scaler object for later transformation during inference.\n",
    "joblib.dump(scaler, 'data_withoutembeddings/standard_scaler.pkl')\n",
    "\n",
    "print(\"Processed data and scaler saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
