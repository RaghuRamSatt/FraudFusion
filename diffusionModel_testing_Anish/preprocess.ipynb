{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 trans_date_trans_time            cc_num  \\\n",
      "0           0   2019-01-01 00:00:18  2703186189652095   \n",
      "1           1   2019-01-01 00:00:44      630423337322   \n",
      "2           2   2019-01-01 00:00:51    38859492057661   \n",
      "3           3   2019-01-01 00:01:16  3534093764340240   \n",
      "4           4   2019-01-01 00:03:06   375534208663984   \n",
      "\n",
      "                             merchant       category     amt      first  \\\n",
      "0          fraud_Rippin, Kub and Mann       misc_net    4.97   Jennifer   \n",
      "1     fraud_Heller, Gutmann and Zieme    grocery_pos  107.23  Stephanie   \n",
      "2                fraud_Lind-Buckridge  entertainment  220.11     Edward   \n",
      "3  fraud_Kutch, Hermiston and Farrell  gas_transport   45.00     Jeremy   \n",
      "4                 fraud_Keeling-Crist       misc_pos   41.96      Tyler   \n",
      "\n",
      "      last gender                        street  ...      lat      long  \\\n",
      "0    Banks      F                561 Perry Cove  ...  36.0788  -81.1781   \n",
      "1     Gill      F  43039 Riley Greens Suite 393  ...  48.8878 -118.2105   \n",
      "2  Sanchez      M      594 White Dale Suite 530  ...  42.1808 -112.2620   \n",
      "3    White      M   9443 Cynthia Court Apt. 038  ...  46.2306 -112.1138   \n",
      "4   Garcia      M              408 Bradley Rest  ...  38.4207  -79.4629   \n",
      "\n",
      "   city_pop                                job         dob  \\\n",
      "0      3495          Psychologist, counselling  1988-03-09   \n",
      "1       149  Special educational needs teacher  1978-06-21   \n",
      "2      4154        Nature conservation officer  1962-01-19   \n",
      "3      1939                    Patent attorney  1967-01-12   \n",
      "4        99     Dance movement psychotherapist  1986-03-28   \n",
      "\n",
      "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
      "0  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315   \n",
      "1  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462   \n",
      "2  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481   \n",
      "3  6b849c168bdad6f867558c3793159a81  1325376076  47.034331 -112.561071   \n",
      "4  a41d7549acf90789359a9aa5346dcb46  1325376186  38.674999  -78.632459   \n",
      "\n",
      "   is_fraud  \n",
      "0         0  \n",
      "1         0  \n",
      "2         0  \n",
      "3         0  \n",
      "4         0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('fraudTrain.csv')\n",
    "test = pd.read_csv('fraudTest.csv')\n",
    "\n",
    "#combine\n",
    "df = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Unnecessary Columns & Extract Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cols\n",
    "drop_cols = [\"Unnamed: 0\", \"trans_num\", \"first\", \"last\", \"street\", \"cc_num\", \"unix_time\"]\n",
    "df.drop(columns=drop_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# Convert datetime features\n",
    "df[\"trans_date_trans_time\"] = pd.to_datetime(df[\"trans_date_trans_time\"])\n",
    "df[\"dob\"] = pd.to_datetime(df[\"dob\"])\n",
    "df[\"hour\"] = df[\"trans_date_trans_time\"].dt.hour\n",
    "df[\"day\"] = df[\"trans_date_trans_time\"].dt.day\n",
    "df[\"weekday\"] = df[\"trans_date_trans_time\"].dt.weekday\n",
    "df[\"month\"] = df[\"trans_date_trans_time\"].dt.month\n",
    "df[\"age\"] = df[\"trans_date_trans_time\"].dt.year - df[\"dob\"].dt.year\n",
    "\n",
    "df.drop(columns=[\"dob\", \"trans_date_trans_time\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Haversine Distance \n",
    "Between the transaction and merchant locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance between transaction location and merchant location\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Earth radius in km\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "df[\"distance\"] = haversine(df[\"lat\"], df[\"long\"], df[\"merch_lat\"], df[\"merch_long\"])\n",
    "#df.drop(columns=[\"lat\", \"long\", \"merch_lat\", \"merch_long\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log transform Amount and City pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['amt'] = np.log1p(df['amt'])\n",
    "df['city_pop'] = np.log1p(df['city_pop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              amt   city_pop\n",
      "0        1.786747   8.159375\n",
      "1        4.684259   5.010635\n",
      "2        5.398660   8.332068\n",
      "3        3.828641   7.570443\n",
      "4        3.760269   4.605170\n",
      "...           ...        ...\n",
      "1852389  3.801538   6.253829\n",
      "1852390  4.725971  10.266045\n",
      "1852391  4.475972   8.212026\n",
      "1852392  2.196113   4.867534\n",
      "1852393  3.666889  11.661363\n",
      "\n",
      "[1852394 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df[['amt','city_pop']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Categorical Encoding Strategy\n",
    "categorical_cols = [\"merchant\", \"category\", \"gender\", \"state\", \"job\", \"city\"]\n",
    "\n",
    "# Define encoding methods per feature\n",
    "one_hot_cols = [\"category\", \"gender\", \"state\"]\n",
    "target_cols = [\"merchant\", \"job\"]\n",
    "freq_cols = [\"city\"]\n",
    "\n",
    "# One-Hot Encoding\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "encoded_array = ohe.fit_transform(df[one_hot_cols])\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=ohe.get_feature_names_out(one_hot_cols))\n",
    "\n",
    "df = df.drop(columns=one_hot_cols).reset_index(drop=True)\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "# Target Encoding (Fraud Rate Per Category)\n",
    "encoders = {}\n",
    "for col in target_cols:\n",
    "    encoders[col] = df.groupby(col)[\"is_fraud\"].mean()\n",
    "    df[col] = df[col].map(encoders[col]).fillna(df[\"is_fraud\"].mean())  # Unseen categories get global fraud rate\n",
    "\n",
    "# Frequency Encoding\n",
    "for col in freq_cols:\n",
    "    encoders[col] = df[col].value_counts(normalize=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features\n",
    "categorical_cols = [\"merchant\", \"category\", \"gender\", \"state\", \"job\", \"city\"]\n",
    "\n",
    "cat_vocab = {}\n",
    "cat_mapping = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    # factorize returns a tuple: (codes, uniques)\n",
    "    codes, uniques = pd.factorize(df[col])\n",
    "    df[col] = codes  # Replace the column with the factorized integer codes.\n",
    "    cat_vocab[col] = len(uniques)  # Store the vocabulary size.\n",
    "    # Create a mapping dictionary: original value -> integer code.\n",
    "    cat_mapping[col] = {val: idx for idx, val in enumerate(uniques)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['amt', 'city_pop', 'age', 'hour', 'day', 'month', 'weekday','distance', 'lat' , 'long', 'merch_lat', 'merch_long']\n",
    "\n",
    "# Initialize and fit the scaler on these numeric features.\n",
    "scaler = StandardScaler()\n",
    "df[num_features] = scaler.fit_transform(df[num_features])\n",
    "\n",
    "# Separate the target variable from the features.\n",
    "target = 'is_fraud'\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "\n",
    "# 80-20 split for train test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                            stratify=y, random_state=42)\n",
    "\n",
    "# 15-65 split for train validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, \n",
    "                                                  test_size=0.1875, stratify=y_train_val, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data and scaler saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the processed splits to CSV files.\n",
    "X_train.to_csv('preprocessed_data/X_train.csv', index=False)\n",
    "y_train.to_csv('preprocessed_data/y_train.csv', index=False)\n",
    "\n",
    "X_val.to_csv('preprocessed_data/X_val.csv', index=False)\n",
    "y_val.to_csv('preprocessed_data/y_val.csv', index=False)\n",
    "\n",
    "X_test.to_csv('preprocessed_data/X_test.csv', index=False)\n",
    "y_test.to_csv('preprocessed_data/y_test.csv', index=False)\n",
    "\n",
    "# Also, save the scaler object for later transformation during inference.\n",
    "joblib.dump(scaler, 'preprocessed_data/standard_scaler.pkl')\n",
    "# categorical vocab/mapping\n",
    "joblib.dump(cat_vocab, 'preprocessed_data/cat_vocab.pkl')\n",
    "joblib.dump(cat_mapping, 'preprocessed_data/cat_mapping.pkl')\n",
    "\n",
    "print(\"Processed data and scaler saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
