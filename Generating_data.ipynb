{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded StandardScaler and categorical artifacts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raghu\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.5.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Generating synthetic samples...\n",
      "Found 4 peaks in amount distribution at: [-0.82075799 -0.50234721  1.72147407  2.53519049]\n",
      "Synthetic numeric samples shape (normalized): torch.Size([8000, 11])\n",
      "Synthetic categorical samples shape: torch.Size([8000, 8])\n",
      "Normalized synthetic data saved to CSV - now in the SAME SCALE as X_train\n",
      "Original scale synthetic data also saved for reference\n",
      "\n",
      "Verification that X_train and synthetic_fraud_v7_normalized are in the same scale:\n",
      "\n",
      "X_train statistics (normalized):\n",
      "               amt          lat         long     city_pop    merch_lat  \\\n",
      "count  6273.000000  6273.000000  6273.000000  6273.000000  6273.000000   \n",
      "mean      1.569536     0.035681     0.003129     0.027991     0.033742   \n",
      "std       1.263539     1.006742     1.033924     0.996909     1.006928   \n",
      "min      -2.180321    -3.650266    -5.487712    -2.113985    -3.795280   \n",
      "25%       1.512012    -0.699740    -0.477387    -0.685151    -0.689590   \n",
      "50%       1.883656     0.180715     0.208114    -0.170742     0.178678   \n",
      "75%       2.539076     0.717226     0.739789     0.626364     0.690039   \n",
      "max       2.868764     5.551446     1.620433     2.662121     5.611313   \n",
      "\n",
      "        merch_long          age   trans_hour    trans_day  trans_month  \\\n",
      "count  6273.000000  6273.000000  6273.000000  6273.000000  6273.000000   \n",
      "mean      0.003081     0.159343     0.169482    -0.016483    -0.219734   \n",
      "std       1.034498     1.075626     1.424235     0.988414     1.013631   \n",
      "min      -5.546844    -1.771955    -1.878901    -1.673090    -1.796249   \n",
      "25%      -0.480703    -0.738028    -1.585463    -0.884468    -1.212299   \n",
      "50%       0.204386     0.066137     1.348917    -0.095846    -0.336375   \n",
      "75%       0.746149     0.870301     1.495636     0.805436     0.539550   \n",
      "max       1.690968     2.823273     1.495636     1.706718     1.415474   \n",
      "\n",
      "       trans_dayofweek  \n",
      "count      6273.000000  \n",
      "mean          0.071232  \n",
      "std           0.937990  \n",
      "min          -1.350082  \n",
      "25%          -0.895119  \n",
      "50%           0.014806  \n",
      "75%           0.924731  \n",
      "max           1.379694  \n",
      "\n",
      "Synthetic data statistics (normalized):\n",
      "               amt          lat         long     city_pop    merch_lat  \\\n",
      "count  8000.000000  8000.000000  8000.000000  8000.000000  8000.000000   \n",
      "mean      1.569450     0.142021    -0.136729    -0.069066     0.142815   \n",
      "std       1.263599     0.926634     1.040429     0.737851     0.930577   \n",
      "min      -2.180321    -3.135524    -6.133509    -2.639523    -3.209828   \n",
      "25%       1.511951    -0.522118    -0.888371    -0.558090    -0.519720   \n",
      "50%       1.882178     0.266420     0.046772    -0.094246     0.261288   \n",
      "75%       2.539085     0.846891     0.698022     0.391027     0.848235   \n",
      "max       2.868764     4.787196     1.723118     3.837163     5.220203   \n",
      "\n",
      "        merch_long          age   trans_hour    trans_day  trans_month  \\\n",
      "count  8000.000000  8000.000000  8000.000000  8000.000000  8000.000000   \n",
      "mean     -0.140338     0.200027     0.246444    -0.027587    -0.217574   \n",
      "std       1.035336     0.858277     1.370310     0.900171     0.857670   \n",
      "min      -5.663681    -2.679804    -1.878901    -1.673090    -1.796249   \n",
      "25%      -0.888252    -0.397427    -1.439613    -0.798012    -0.971418   \n",
      "50%       0.040795     0.186053     1.324904    -0.075785    -0.267742   \n",
      "75%       0.694670     0.775443     1.438124     0.739058     0.554746   \n",
      "max       1.805976     3.278707     1.495636     1.660585     1.415474   \n",
      "\n",
      "       trans_dayofweek  \n",
      "count      8000.000000  \n",
      "mean          0.057745  \n",
      "std           0.941929  \n",
      "min          -1.350082  \n",
      "25%          -0.905285  \n",
      "50%           0.020929  \n",
      "75%           0.943540  \n",
      "max           1.379694  \n",
      "\n",
      "Both should show similar scales - generally mean near 0 and std near 1 for normalized features\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import joblib\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load preprocessing artifacts\n",
    "scaler = joblib.load(r\"D:\\DS Northeastern\\DS 5500 - Capstone\\FraudFusion\\Data\\processed\\standard_scaler.pkl\")\n",
    "cat_vocab = joblib.load(r\"D:\\DS Northeastern\\DS 5500 - Capstone\\FraudFusion\\Data\\processed\\cat_vocab.pkl\")\n",
    "cat_mapping = joblib.load(r\"D:\\DS Northeastern\\DS 5500 - Capstone\\FraudFusion\\Data\\processed\\cat_mapping.pkl\")\n",
    "print(\"Loaded StandardScaler and categorical artifacts.\")\n",
    "\n",
    "# Define features (same as in preprocessing)\n",
    "numeric_features = ['amt', 'lat', 'long', 'city_pop', 'merch_lat', 'merch_long',\n",
    "                   'age', 'trans_hour', 'trans_day', 'trans_month', 'trans_dayofweek']\n",
    "cat_features = ['merchant', 'category', 'gender', 'street', 'city', 'state', 'zip', 'job']\n",
    "\n",
    "# Load training data to get engineered feature ranges\n",
    "X_train_df = pd.read_csv(r\"D:\\DS Northeastern\\DS 5500 - Capstone\\FraudFusion\\Data\\processed\\X_train.csv\")\n",
    "y_train_df = pd.read_csv(r\"D:\\DS Northeastern\\DS 5500 - Capstone\\FraudFusion\\Data\\processed\\y_train.csv\")\n",
    "y_train = y_train_df.iloc[:, 0]\n",
    "fraud_mask = (y_train == 1)\n",
    "X_train_num = X_train_df[numeric_features].loc[fraud_mask].values\n",
    "\n",
    "# Engineered feature indices and min/max\n",
    "eng_features = ['trans_hour', 'trans_day', 'trans_month', 'trans_dayofweek']\n",
    "eng_indices = [numeric_features.index(feat) for feat in eng_features]\n",
    "amt_idx = numeric_features.index('amt')\n",
    "eng_min_np = np.min(X_train_num[:, eng_indices], axis=0)\n",
    "eng_max_np = np.max(X_train_num[:, eng_indices], axis=0)\n",
    "eng_min = torch.tensor(eng_min_np, dtype=torch.float32).to(device)\n",
    "eng_max = torch.tensor(eng_max_np, dtype=torch.float32).to(device)\n",
    "\n",
    "# Store real fraud amount distribution\n",
    "real_fraud_amt_scaled = X_train_num[:, amt_idx].copy()\n",
    "\n",
    "# Diffusion parameters\n",
    "T_train = 800\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "beta = torch.linspace(beta_start, beta_end, T_train).to(device)\n",
    "alpha = 1.0 - beta\n",
    "alpha_hat = torch.cumprod(alpha, dim=0)\n",
    "\n",
    "# Function for getting empirical CDF\n",
    "def get_cdf(data):\n",
    "    \"\"\"Compute empirical CDF from data\"\"\"\n",
    "    sorted_data = np.sort(data)\n",
    "    ecdf_y = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "    return sorted_data, ecdf_y\n",
    "\n",
    "# Distribution matching function\n",
    "def get_distribution_transform_function(source_vals, target_vals):\n",
    "    \"\"\"\n",
    "    Creates a function that transforms values from source distribution to target distribution\n",
    "    through quantile matching\n",
    "    \"\"\"\n",
    "    source_sorted, source_cdf = get_cdf(source_vals)\n",
    "    target_sorted, target_cdf = get_cdf(target_vals)\n",
    "    \n",
    "    source_to_quantile = interp1d(source_sorted, source_cdf, \n",
    "                                bounds_error=False, fill_value=(0, 1))\n",
    "    quantile_to_target = interp1d(target_cdf, target_sorted, \n",
    "                                bounds_error=False, \n",
    "                                fill_value=(min(target_sorted), max(target_sorted)))\n",
    "    \n",
    "    def transform(vals):\n",
    "        quantiles = source_to_quantile(vals)\n",
    "        return quantile_to_target(quantiles)\n",
    "    \n",
    "    return transform\n",
    "\n",
    "def match_distribution(values, target_values):\n",
    "    \"\"\"\n",
    "    Transform values to match the distribution of target_values\n",
    "    \"\"\"\n",
    "    transform = get_distribution_transform_function(values, target_values)\n",
    "    return transform(values)\n",
    "\n",
    "# Create cyclic features for generation\n",
    "def create_cyclic_features(data, feature_indices, periods):\n",
    "    \"\"\"Create sine and cosine features for cyclical data\"\"\"\n",
    "    cyclic_data = np.zeros((data.shape[0], len(feature_indices) * 2))\n",
    "    \n",
    "    for i, (idx, period) in enumerate(zip(feature_indices, periods)):\n",
    "        # Normalize to [0, 2Ï€]\n",
    "        values = data[:, idx].copy()\n",
    "        normalized = 2 * np.pi * values / period\n",
    "        \n",
    "        # Create sin and cos features\n",
    "        cyclic_data[:, i*2] = np.sin(normalized)\n",
    "        cyclic_data[:, i*2+1] = np.cos(normalized)\n",
    "        \n",
    "    return cyclic_data\n",
    "\n",
    "# Define periods for each time feature\n",
    "hour_period = 24.0\n",
    "day_period = 31.0\n",
    "month_period = 12.0\n",
    "dow_period = 7.0\n",
    "periods = [hour_period, day_period, month_period, dow_period]\n",
    "\n",
    "# Original scale data for creating cyclic features\n",
    "X_train_num_original = scaler.inverse_transform(X_train_num)\n",
    "cyclic_fraud = create_cyclic_features(\n",
    "    X_train_num_original, \n",
    "    [numeric_features.index(feat) for feat in eng_features], \n",
    "    periods\n",
    ")\n",
    "\n",
    "# Define the model\n",
    "class CombinedNoisePredictor(nn.Module):\n",
    "    def __init__(self, num_input_dim, cat_vocab_sizes, cyclic_dim=8, cat_embed_dim=4, hidden_dim=256):\n",
    "        super(CombinedNoisePredictor, self).__init__()\n",
    "        self.embeddings = nn.ModuleDict()\n",
    "        for col, vocab in cat_vocab_sizes.items():\n",
    "            self.embeddings[col] = nn.Embedding(vocab, cat_embed_dim)\n",
    "        cat_total_dim = len(cat_vocab_sizes) * cat_embed_dim\n",
    "        \n",
    "        combined_input_dim = num_input_dim + cat_total_dim + cyclic_dim\n",
    "        \n",
    "        self.fc1 = nn.Linear(combined_input_dim + 1, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, combined_input_dim)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm3 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        nn.init.xavier_uniform_(self.fc4.weight)\n",
    "        \n",
    "    def forward(self, x_num, x_cat, x_cyclic, t):\n",
    "        embeds = []\n",
    "        for i, col in enumerate(self.embeddings):\n",
    "            emb = self.embeddings[col](x_cat[:, i])\n",
    "            embeds.append(emb)\n",
    "        x_cat_emb = torch.cat(embeds, dim=1)\n",
    "        \n",
    "        x = torch.cat([x_num, x_cat_emb, x_cyclic], dim=1)\n",
    "        \n",
    "        t_norm = t.unsqueeze(1).float() / T_train\n",
    "        x_input = torch.cat([x, t_norm], dim=1)\n",
    "        \n",
    "        h = self.activation(self.fc1(x_input))\n",
    "        h = self.norm1(h)\n",
    "        h = self.dropout(h)\n",
    "        \n",
    "        h_res = h\n",
    "        h = self.activation(self.fc2(h))\n",
    "        h = self.norm2(h)\n",
    "        h = self.dropout(h)\n",
    "        h = h + 0.1 * h_res\n",
    "        \n",
    "        h_res = h\n",
    "        h = self.activation(self.fc3(h))\n",
    "        h = self.norm3(h)\n",
    "        h = self.dropout(h)\n",
    "        h = h + 0.1 * h_res\n",
    "        \n",
    "        out = self.fc4(h)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Instantiate the model with the same parameters\n",
    "cat_vocab_sizes = {col: cat_vocab[col] for col in cat_features}\n",
    "num_input_dim = len(numeric_features)\n",
    "cyclic_dim = len(eng_features) * 2\n",
    "\n",
    "model = CombinedNoisePredictor(\n",
    "    num_input_dim=num_input_dim,\n",
    "    cat_vocab_sizes=cat_vocab_sizes,\n",
    "    cyclic_dim=cyclic_dim,\n",
    "    cat_embed_dim=4,\n",
    "    hidden_dim=256\n",
    ").to(device)\n",
    "\n",
    "# Load the trained model weights\n",
    "model.load_state_dict(torch.load(r\"D:\\DS Northeastern\\DS 5500 - Capstone\\FraudFusion\\baseline_improved_v7.pth\", \n",
    "                               map_location=device))\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Define the function to generate synthetic fraud (same as original)\n",
    "def generate_synthetic_fraud(model, num_samples, T_gen=600):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Create categorical samples\n",
    "        cat_samples = {}\n",
    "        for col in cat_vocab_sizes:\n",
    "            vocab_size = cat_vocab_sizes[col]\n",
    "            cat_samples[col] = torch.randint(0, vocab_size, (num_samples,), device=device, dtype=torch.long)\n",
    "        x_cat = torch.stack([cat_samples[col] for col in cat_features], dim=1)\n",
    "        \n",
    "        X_fraud_tensor = torch.tensor(X_train_num, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Better initialization with bimodal distribution awareness\n",
    "        idx1 = torch.randint(0, X_fraud_tensor.shape[0] // 2, (num_samples // 2,), device=device)\n",
    "        idx2 = torch.randint(X_fraud_tensor.shape[0] // 2, X_fraud_tensor.shape[0], (num_samples - num_samples // 2,), device=device)\n",
    "        idx = torch.cat([idx1, idx2])\n",
    "        \n",
    "        noise = torch.randn(num_samples, num_input_dim).to(device) * 0.3\n",
    "        x_t_num = X_fraud_tensor[idx] + X_fraud_tensor.std(dim=0, keepdim=True) * noise\n",
    "        \n",
    "        X_cyclic_fraud_tensor = torch.tensor(cyclic_fraud, dtype=torch.float32).to(device)\n",
    "        x_cyclic = X_cyclic_fraud_tensor[idx]\n",
    "        \n",
    "        # Enhanced handling for amount to better match bimodal distribution\n",
    "        amt_idx = numeric_features.index('amt')\n",
    "        \n",
    "        fraud_amts = X_fraud_tensor[:, amt_idx]\n",
    "        \n",
    "        fraud_amts_np = fraud_amts.cpu().numpy()\n",
    "        kde = stats.gaussian_kde(fraud_amts_np)\n",
    "        x_grid = np.linspace(fraud_amts_np.min(), fraud_amts_np.max(), 1000)\n",
    "        kde_values = kde(x_grid)\n",
    "        \n",
    "        peaks, _ = find_peaks(kde_values, height=0.05*kde_values.max())\n",
    "        peak_x = x_grid[peaks]\n",
    "        print(f\"Found {len(peak_x)} peaks in amount distribution at: {peak_x}\")\n",
    "        \n",
    "        if len(peak_x) >= 2:\n",
    "            peak_x = sorted(peak_x)\n",
    "            \n",
    "            high_peak_samples = int(num_samples * 0.9)\n",
    "            low_peak_samples = num_samples - high_peak_samples\n",
    "            \n",
    "            low_peak = torch.tensor(peak_x[0], device=device)\n",
    "            high_peak = torch.tensor(peak_x[-1], device=device)\n",
    "            \n",
    "            peak_distance = high_peak - low_peak\n",
    "            low_noise = torch.randn(low_peak_samples, device=device) * (0.10 * peak_distance)\n",
    "            high_noise = torch.randn(high_peak_samples, device=device) * (0.08 * peak_distance)\n",
    "            \n",
    "            low_amts = low_peak + low_noise\n",
    "            high_amts = high_peak + high_noise\n",
    "            \n",
    "            amt_values = torch.cat([low_amts, high_amts])\n",
    "            perm = torch.randperm(num_samples)\n",
    "            amt_values = amt_values[perm]\n",
    "            \n",
    "            x_t_num[:, amt_idx] = amt_values\n",
    "        else:\n",
    "            sorted_amts, _ = torch.sort(fraud_amts)\n",
    "            n = sorted_amts.size(0)\n",
    "            \n",
    "            lower_idx = torch.randint(0, n // 3, (num_samples // 10,), device=device)\n",
    "            upper_idx = torch.randint(2 * n // 3, n, (num_samples - num_samples // 10,), device=device)\n",
    "            \n",
    "            amt_indices = torch.cat([lower_idx, upper_idx])\n",
    "            amt_noise = torch.randn(num_samples, device=device) * 0.08\n",
    "            \n",
    "            x_t_num[:, amt_idx] = sorted_amts[amt_indices] + amt_noise\n",
    "        \n",
    "        # Reverse diffusion process\n",
    "        for t_step in reversed(range(1, T_gen)):\n",
    "            t = torch.full((num_samples,), t_step, device=device, dtype=torch.long)\n",
    "            \n",
    "            pred_noise = model(x_t_num, x_cat, x_cyclic, t)\n",
    "            pred_noise_numeric = pred_noise[:, :num_input_dim]\n",
    "            \n",
    "            pred_noise_numeric = torch.clamp(pred_noise_numeric, -5.0, 5.0)\n",
    "            \n",
    "            beta_t = beta[t].unsqueeze(1)\n",
    "            sqrt_alpha_t = torch.sqrt(alpha[t]).unsqueeze(1)\n",
    "            sqrt_one_minus_alpha_hat_t = torch.sqrt(1 - alpha_hat[t]).unsqueeze(1)\n",
    "            \n",
    "            noise_scale = torch.sqrt(beta_t)\n",
    "            if t_step < 200:\n",
    "                noise_scale = noise_scale * (t_step / 200.0)\n",
    "            \n",
    "            z = torch.randn_like(x_t_num) * noise_scale if t_step > 1 else torch.zeros_like(x_t_num)\n",
    "            \n",
    "            x_t_num = (x_t_num - (beta_t / (sqrt_one_minus_alpha_hat_t + 1e-8)) * pred_noise_numeric) / (sqrt_alpha_t + 1e-8) + z\n",
    "            \n",
    "            x_t_num = torch.clamp(x_t_num, -10.0, 10.0)\n",
    "        \n",
    "        # Clip engineered features to observed range\n",
    "        x_t_num_clipped = x_t_num.clone()\n",
    "        x0_est_eng = x_t_num[:, eng_indices]\n",
    "        x0_est_eng = torch.max(torch.min(x0_est_eng, eng_max.unsqueeze(0)), eng_min.unsqueeze(0))\n",
    "        x_t_num_clipped[:, eng_indices] = x0_est_eng\n",
    "        \n",
    "        # Post-processing step to directly fix amount distribution\n",
    "        syn_amt_values = x_t_num_clipped[:, amt_idx].cpu().numpy()\n",
    "        \n",
    "        transformed_amt = match_distribution(syn_amt_values, real_fraud_amt_scaled)\n",
    "        \n",
    "        x_t_num_clipped[:, amt_idx] = torch.tensor(transformed_amt, dtype=torch.float32).to(device)\n",
    "        \n",
    "        return x_t_num_clipped, x_cat\n",
    "\n",
    "# Generate synthetic samples\n",
    "num_synthetic = 8000\n",
    "print(\"Generating synthetic samples...\")\n",
    "synthetic_num_norm, synthetic_cat = generate_synthetic_fraud(model, num_synthetic)\n",
    "print(\"Synthetic numeric samples shape (normalized):\", synthetic_num_norm.shape)\n",
    "print(\"Synthetic categorical samples shape:\", synthetic_cat.shape)\n",
    "\n",
    "# IMPORTANT CHANGE: Skip inverse transformation to keep the same scale as X_train\n",
    "synthetic_num_norm_np = synthetic_num_norm.cpu().numpy()\n",
    "\n",
    "# Create dataframe directly from normalized values\n",
    "synthetic_numeric_df = pd.DataFrame(synthetic_num_norm_np, columns=numeric_features)\n",
    "synthetic_cat_df = pd.DataFrame(synthetic_cat.cpu().numpy(), columns=cat_features)\n",
    "synthetic_full_df = pd.concat([synthetic_numeric_df, synthetic_cat_df], axis=1)\n",
    "\n",
    "# Save the normalized synthetic data (same scale as X_train)\n",
    "synthetic_full_df.to_csv(r\"D:\\DS Northeastern\\DS 5500 - Capstone\\FraudFusion\\Data\\synthetic_fraud_v7_normalized_8kpoints.csv\", index=False)\n",
    "print(\"Normalized synthetic data saved to CSV - now in the SAME SCALE as X_train\")\n",
    "\n",
    "# If you also want the original scale version for reference\n",
    "synthetic_num_original = scaler.inverse_transform(synthetic_num_norm_np)\n",
    "synthetic_original_df = pd.DataFrame(synthetic_num_original, columns=numeric_features)\n",
    "synthetic_full_original_df = pd.concat([synthetic_original_df, synthetic_cat_df], axis=1)\n",
    "synthetic_full_original_df.to_csv(r\"D:\\DS Northeastern\\DS 5500 - Capstone\\FraudFusion\\Data\\synthetic_fraud_v7_original_8kpoints.csv\", index=False)\n",
    "print(\"Original scale synthetic data also saved for reference\")\n",
    "\n",
    "# Quick verification\n",
    "print(\"\\nVerification that X_train and synthetic_fraud_v7_normalized are in the same scale:\")\n",
    "X_fraud_tensor = torch.tensor(X_train_num, dtype=torch.float32)\n",
    "X_train_subset = pd.DataFrame(X_fraud_tensor.numpy(), columns=numeric_features).describe()\n",
    "synthetic_subset = synthetic_numeric_df.describe()\n",
    "\n",
    "print(\"\\nX_train statistics (normalized):\")\n",
    "print(X_train_subset)\n",
    "print(\"\\nSynthetic data statistics (normalized):\")\n",
    "print(synthetic_subset)\n",
    "\n",
    "print(\"\\nBoth should show similar scales - generally mean near 0 and std near 1 for normalized features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
