{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FraudDiffuse Implementation: Diffusion-Based Synthetic Fraud Data Generation\n",
    "\n",
    "## Overview\n",
    "This notebook implements the FraudDiffuse model from the paper \"FraudDiffuse: Diffusion-aided Synthetic Fraud Augmentation for Improved Fraud Detection\". The goal is to generate synthetic fraud cases that maintain the statistical properties of real fraud while being diverse enough to improve fraud detection models.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "### 1. Base Architecture\n",
    "- **Diffusion Model**: Progressive noising and denoising of features\n",
    "- **Conditional Generation**: Uses fraud/non-fraud labels to guide generation\n",
    "- **Time Embedding**: Sinusoidal embeddings for timestep information\n",
    "\n",
    "### 2. FraudDiffuse Innovations\n",
    "- **Adaptive Prior**: Uses non-fraud distribution statistics as prior\n",
    "- **Probability-Based Loss**: Ensures generated samples follow expected distributions\n",
    "- **Contrastive Learning**: Triplet loss to maintain fraud/non-fraud distinctions\n",
    "\n",
    "### 3. Loss Components\n",
    "- **MSE Loss**: Basic reconstruction loss\n",
    "- **Probability Loss**: Based on z-scores from non-fraud distribution\n",
    "- **Triplet Loss**: Contrastive loss for fraud/non-fraud separation\n",
    "\n",
    "## Implementation Notes\n",
    "- Features are normalized and scaled appropriately\n",
    "- Loss components are balanced using weights\n",
    "- Early stopping is implemented to prevent overfitting\n",
    "- GPU acceleration is used where available\n",
    "\n",
    "## Training Process\n",
    "1. Load and preprocess fraud transaction data\n",
    "2. Train diffusion model with combined losses\n",
    "3. Generate synthetic fraud samples\n",
    "4. Evaluate quality of generated samples\n",
    "\n",
    "## Evaluation Metrics\n",
    "We'll evaluate the generated samples using:\n",
    "- Distribution similarity to real fraud cases\n",
    "- Statistical measures (mean, variance, correlations)\n",
    "- Fraud detection model performance improvement\n",
    "\n",
    "## Usage\n",
    "The model can be used to:\n",
    "1. Generate synthetic fraud cases\n",
    "2. Augment fraud detection training data\n",
    "3. Study fraud patterns and characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "GPU Device Name: NVIDIA GeForce RTX 4060\n",
      "GPU Memory Usage: 0.00MB / 8187.50MB\n",
      "Reinitializing datasets...\n",
      "\n",
      "Testing full train loader...\n",
      "Iterator creation time: 0.000s\n",
      "First batch load time: 0.027s\n",
      "Batch shapes: features=torch.Size([128, 25]), labels=torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# First define the Dataset class\n",
    "class FraudDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        # Load data in chunks to handle large files better\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        \n",
    "        # Convert to tensors once during initialization\n",
    "        self.features = torch.FloatTensor(\n",
    "            self.data.drop(['is_fraud'], axis=1).values\n",
    "        )\n",
    "        self.labels = torch.FloatTensor(\n",
    "            self.data['is_fraud'].values\n",
    "        )\n",
    "        \n",
    "        self.num_features = self.features.shape[1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': self.features[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Check GPU availability and print info\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"GPU Memory Usage: {torch.cuda.memory_allocated(0)/1024**2:.2f}MB / {torch.cuda.get_device_properties(0).total_memory/1024**2:.2f}MB\")\n",
    "\n",
    "# 2. Reinitialize the datasets with the modified settings\n",
    "print(\"Reinitializing datasets...\")\n",
    "train_dataset = FraudDataset('Data/processed/train.csv')\n",
    "val_dataset = FraudDataset('Data/processed/val.csv')\n",
    "test_dataset = FraudDataset('Data/processed/test.csv')\n",
    "\n",
    "\n",
    "# 3. Create new data loaders with conservative settings\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Start with 0 workers\n",
    "    pin_memory=False,  # Disable pin memory initially\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# 4. Test the full train loader\n",
    "print(\"\\nTesting full train loader...\")\n",
    "start_time = time.time()\n",
    "train_iter = iter(train_loader)\n",
    "print(f\"Iterator creation time: {time.time() - start_time:.3f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "first_batch = next(train_iter)\n",
    "print(f\"First batch load time: {time.time() - start_time:.3f}s\")\n",
    "print(f\"Batch shapes: features={first_batch['features'].shape}, labels={first_batch['label'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test gradual optimization of data loading\n",
    "# optimizations = [\n",
    "#     {\n",
    "#         \"name\": \"Base (current)\",\n",
    "#         \"settings\": {\"num_workers\": 0, \"pin_memory\": False}\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"With pin_memory\",\n",
    "#         \"settings\": {\"num_workers\": 0, \"pin_memory\": True}\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"With 1 worker\",\n",
    "#         \"settings\": {\"num_workers\": 1, \"pin_memory\": True}\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"With 2 workers\",\n",
    "#         \"settings\": {\"num_workers\": 2, \"pin_memory\": True}\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# print(\"Testing DataLoader optimizations...\")\n",
    "# for opt in optimizations:\n",
    "#     print(f\"\\nTesting {opt['name']}:\")\n",
    "#     test_loader = DataLoader(\n",
    "#         train_dataset,\n",
    "#         batch_size=128,\n",
    "#         shuffle=True,\n",
    "#         **opt['settings']\n",
    "#     )\n",
    "    \n",
    "#     # Test iteration speed\n",
    "#     start_time = time.time()\n",
    "#     test_iter = iter(test_loader)\n",
    "#     first_batch = next(test_iter)\n",
    "#     batch_time = time.time() - start_time\n",
    "    \n",
    "#     print(f\"Batch load time: {batch_time:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like I am running into issues with PyTorch's DataLoader multiprocessing on Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First, the time embedding component\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "# 2. The U-Net building block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv1d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose1d(out_ch, out_ch, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv1d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv1d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv1d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm1d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm1d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # First convolution\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        \n",
    "        # Time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # Reshape time embedding to match the feature dimension\n",
    "        time_emb = time_emb.unsqueeze(-1).repeat(1, 1, h.shape[-1])\n",
    "        \n",
    "        # Add time embedding\n",
    "        h = h + time_emb\n",
    "        \n",
    "        # Second convolution and transform\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        return self.transform(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized on: cuda\n"
     ]
    }
   ],
   "source": [
    "# 3. The complete U-Net architecture\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_features, time_emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Condition embedding\n",
    "        self.condition_embedding = nn.Sequential(\n",
    "            nn.Linear(1, time_emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim)\n",
    "        )\n",
    "        \n",
    "        # Initial projection\n",
    "        self.proj = nn.Linear(num_features, 64)\n",
    "        \n",
    "        # Encoder\n",
    "        self.enc1 = Block(1, 64, time_emb_dim)\n",
    "        self.enc2 = Block(64, 128, time_emb_dim)\n",
    "        self.enc3 = Block(128, 256, time_emb_dim)\n",
    "        self.enc4 = Block(256, 512, time_emb_dim)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv1d(512, 512, 3, padding=1),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec4 = Block(512, 256, time_emb_dim, up=True)\n",
    "        self.dec3 = Block(256, 128, time_emb_dim, up=True)\n",
    "        self.dec2 = Block(128, 64, time_emb_dim, up=True)\n",
    "        self.dec1 = Block(64, 1, time_emb_dim, up=True)\n",
    "        \n",
    "        # Final projection\n",
    "        self.final = nn.Linear(64, num_features)\n",
    "        \n",
    "    def forward(self, x, t, y):\n",
    "        # Time embedding\n",
    "        t = self.time_mlp(t)\n",
    "        \n",
    "        # Condition embedding\n",
    "        y = y.view(-1, 1)\n",
    "        y = self.condition_embedding(y)\n",
    "        \n",
    "        # Initial projection and reshape\n",
    "        x = self.proj(x)\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        \n",
    "        # U-Net forward pass\n",
    "        x1 = self.enc1(x, t)\n",
    "        x2 = self.enc2(x1, t)\n",
    "        x3 = self.enc3(x2, t)\n",
    "        x4 = self.enc4(x3, t)\n",
    "        \n",
    "        x4 = self.bottleneck(x4)\n",
    "        \n",
    "        x = self.dec4(x4, t)\n",
    "        x = self.dec3(x + x3, t)\n",
    "        x = self.dec2(x + x2, t)\n",
    "        x = self.dec1(x + x1, t)\n",
    "        \n",
    "        # Reshape and final projection\n",
    "        x = x.squeeze(1)  # Remove channel dimension\n",
    "        x = self.final(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model and move to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet(num_features=train_dataset.num_features).to(device)\n",
    "print(f\"Model initialized on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\Env1\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing single training iteration...\n",
      "Single iteration test complete!\n",
      "Loss: 1.097803\n",
      "GPU Memory Usage: 93.7MB / 8187.50MB\n"
     ]
    }
   ],
   "source": [
    "# 5. Diffusion process\n",
    "class DiffusionModel:\n",
    "    def __init__(self, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        \n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "        \n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.num_timesteps)\n",
    "    \n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None]\n",
    "        ε = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * ε, ε\n",
    "\n",
    "# Initialize diffusion model and optimizer\n",
    "diffusion = DiffusionModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "# Test a single training iteration\n",
    "print(\"\\nTesting single training iteration...\")\n",
    "model.train()\n",
    "batch = next(iter(train_loader))\n",
    "x = batch['features'].to(device)\n",
    "labels = batch['label'].to(device)\n",
    "\n",
    "# Sample timestep\n",
    "t = torch.randint(0, diffusion.num_timesteps, (x.shape[0],), device=device).long()\n",
    "\n",
    "# Get noisy features and noise\n",
    "noisy_x, noise = diffusion.noise_images(x, t)\n",
    "\n",
    "# Forward pass\n",
    "predicted_noise = model(noisy_x, t, labels)\n",
    "\n",
    "# Calculate loss\n",
    "loss = F.mse_loss(noise, predicted_noise)\n",
    "\n",
    "# Backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Single iteration test complete!\")\n",
    "print(f\"Loss: {loss.item():.6f}\")\n",
    "print(f\"GPU Memory Usage: {torch.cuda.memory_allocated()/1024**2:.1f}MB / {torch.cuda.get_device_properties(0).total_memory/1024**2:.2f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the full training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training on device: cuda\n",
      "Number of epochs: 100\n",
      "Batch size: 128\n",
      "Training batches per epoch: 9407\n",
      "Initial learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 9407/9407 [02:40<00:00, 58.48it/s, loss=0.9808, avg_loss=0.9998, gpu_mem=92MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100 Summary:\n",
      "Train Loss: 0.999816\n",
      "Val Loss: 1.000211\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 2.8 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 9407/9407 [03:13<00:00, 48.65it/s, loss=1.0235, avg_loss=1.0000, gpu_mem=92MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/100 Summary:\n",
      "Train Loss: 1.000012\n",
      "Val Loss: 0.998341\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 3.5 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 9407/9407 [03:48<00:00, 41.22it/s, loss=0.9912, avg_loss=0.9997, gpu_mem=92MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/100 Summary:\n",
      "Train Loss: 0.999748\n",
      "Val Loss: 1.000156\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 4.1 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 9407/9407 [03:16<00:00, 47.91it/s, loss=1.0022, avg_loss=1.0003, gpu_mem=92MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/100 Summary:\n",
      "Train Loss: 1.000306\n",
      "Val Loss: 0.999662\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 3.4 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 9407/9407 [03:13<00:00, 48.65it/s, loss=1.0832, avg_loss=0.9999, gpu_mem=92MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/100 Summary:\n",
      "Train Loss: 0.999902\n",
      "Val Loss: 0.999844\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 3.5 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 9407/9407 [03:22<00:00, 46.38it/s, loss=1.0145, avg_loss=1.0000, gpu_mem=92MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/100 Summary:\n",
      "Train Loss: 0.999981\n",
      "Val Loss: 0.999655\n",
      "Learning Rate: 0.000050\n",
      "Epoch Time: 3.7 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 9407/9407 [03:23<00:00, 46.20it/s, loss=1.0605, avg_loss=1.0000, gpu_mem=92MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/100 Summary:\n",
      "Train Loss: 0.999979\n",
      "Val Loss: 0.999658\n",
      "Learning Rate: 0.000050\n",
      "Epoch Time: 3.7 minutes\n",
      "\n",
      "Early stopping triggered after 7 epochs\n",
      "\n",
      "Training completed!\n",
      "Total training time: 0.4 hours\n",
      "Best validation loss: 0.998341\n"
     ]
    }
   ],
   "source": [
    "# First add the sampling function\n",
    "@torch.no_grad()\n",
    "def sample_features(model, diffusion, n_samples, labels, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Start from random noise\n",
    "    x = torch.randn((n_samples, model.num_features)).to(device)\n",
    "    \n",
    "    # Gradually denoise\n",
    "    for i in tqdm(reversed(range(diffusion.num_timesteps)), desc='Sampling'):\n",
    "        t = torch.full((n_samples,), i, device=device, dtype=torch.long)\n",
    "        predicted_noise = model(x, t, labels)\n",
    "        \n",
    "        alpha = diffusion.alpha[t][:, None]\n",
    "        alpha_hat = diffusion.alpha_hat[t][:, None]\n",
    "        beta = diffusion.beta[t][:, None]\n",
    "        \n",
    "        if i > 0:\n",
    "            noise = torch.randn_like(x)\n",
    "        else:\n",
    "            noise = torch.zeros_like(x)\n",
    "            \n",
    "        x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Training configuration and setup\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 5\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Create directories for checkpoints and samples\n",
    "import os\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('samples', exist_ok=True)\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "# Main training loop\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training on device: {device}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Batch size: {train_loader.batch_size}\")\n",
    "print(f\"Training batches per epoch: {len(train_loader)}\")\n",
    "print(f\"Initial learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Training phase\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get batch data\n",
    "        x = batch['features'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Sample timestep\n",
    "        t = torch.randint(0, diffusion.num_timesteps, (x.shape[0],), device=device).long()\n",
    "        \n",
    "        # Get noisy features and noise\n",
    "        noisy_x, noise = diffusion.noise_images(x, t)\n",
    "        \n",
    "        # Predict noise\n",
    "        predicted_noise = model(noisy_x, t, labels)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.mse_loss(noise, predicted_noise)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (batch_idx + 1)\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'avg_loss': f\"{avg_loss:.4f}\",\n",
    "            'gpu_mem': f\"{torch.cuda.memory_allocated()/1024**2:.0f}MB\"\n",
    "        })\n",
    "    \n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x = batch['features'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            t = torch.randint(0, diffusion.num_timesteps, (x.shape[0],), device=device).long()\n",
    "            noisy_x, noise = diffusion.noise_images(x, t)\n",
    "            predicted_noise = model(noisy_x, t, labels)\n",
    "            val_loss += F.mse_loss(noise, predicted_noise).item()\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Print epoch summary\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} Summary:\")\n",
    "    print(f\"Train Loss: {train_loss:.6f}\")\n",
    "    print(f\"Val Loss: {val_loss:.6f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(f\"Epoch Time: {epoch_time/60:.1f} minutes\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'history': history,\n",
    "        }, 'checkpoints/best_model.pt')\n",
    "        print(\"Saved new best model!\")\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping_counter >= early_stopping_patience:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "    \n",
    "    # Generate and save samples every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(\"\\nGenerating samples...\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Generate fraud samples\n",
    "            fraud_samples = sample_features(\n",
    "                model, \n",
    "                diffusion, \n",
    "                n_samples=100, \n",
    "                labels=torch.ones(100).to(device),\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            # Save samples\n",
    "            torch.save(fraud_samples.cpu(), f'samples/fraud_samples_epoch_{epoch+1}.pt')\n",
    "            \n",
    "            # Plot training history\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(history['train_loss'], label='Train Loss')\n",
    "            plt.plot(history['val_loss'], label='Val Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(history['lr'], label='Learning Rate')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Learning Rate')\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'samples/training_history_epoch_{epoch+1}.png')\n",
    "            plt.close()\n",
    "\n",
    "total_training_time = time.time() - training_start_time\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Total training time: {total_training_time/3600:.1f} hours\")\n",
    "print(f\"Best validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the baseline diffusion model implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in the FraudDiffuse Model to the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 9407/9407 [01:08<00:00, 137.80it/s, loss=3.82e+9, avg_loss=4.77e+9, gpu_mem=81MB] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100 Summary:\n",
      "Train Loss: 4773571137.626023\n",
      "Val Loss: 4218566401.002303\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.2 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 9407/9407 [01:06<00:00, 142.28it/s, loss=3.45e+9, avg_loss=3.79e+9, gpu_mem=81MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/100 Summary:\n",
      "Train Loss: 3785866024.303604\n",
      "Val Loss: 3686459207.753109\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.2 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 9407/9407 [01:08<00:00, 137.77it/s, loss=2.94e+9, avg_loss=3.59e+9, gpu_mem=81MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/100 Summary:\n",
      "Train Loss: 3585768195.986818\n",
      "Val Loss: 3457558228.193459\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.2 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 9407/9407 [01:20<00:00, 116.68it/s, loss=3.28e+9, avg_loss=3.39e+9, gpu_mem=26MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/100 Summary:\n",
      "Train Loss: 3389646098.015521\n",
      "Val Loss: 3301076337.260249\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.4 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 9407/9407 [01:24<00:00, 110.67it/s, loss=3.74e+9, avg_loss=3.27e+9, gpu_mem=26MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/100 Summary:\n",
      "Train Loss: 3265047856.875944\n",
      "Val Loss: 3230065718.301244\n",
      "Learning Rate: 0.000050\n",
      "Epoch Time: 1.5 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 9407/9407 [01:09<00:00, 135.68it/s, loss=3.68e+9, avg_loss=3.2e+9, gpu_mem=26MB] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/100 Summary:\n",
      "Train Loss: 3197526347.872010\n",
      "Val Loss: 3192289668.716721\n",
      "Learning Rate: 0.000050\n",
      "Epoch Time: 1.2 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 9407/9407 [01:21<00:00, 115.88it/s, loss=1.98e+9, avg_loss=3.16e+9, gpu_mem=26MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/100 Summary:\n",
      "Train Loss: 3163393336.427766\n",
      "Val Loss: 3232300259.169046\n",
      "Learning Rate: 0.000050\n",
      "Epoch Time: 1.5 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 9407/9407 [01:29<00:00, 105.65it/s, loss=4.03e+9, avg_loss=3.14e+9, gpu_mem=26MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/100 Summary:\n",
      "Train Loss: 3139622344.824067\n",
      "Val Loss: 3144024757.357900\n",
      "Learning Rate: 0.000050\n",
      "Epoch Time: 1.6 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 9407/9407 [01:28<00:00, 106.28it/s, loss=2.47e+9, avg_loss=3.12e+9, gpu_mem=26MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/100 Summary:\n",
      "Train Loss: 3117264546.262145\n",
      "Val Loss: 3139966354.159374\n",
      "Learning Rate: 0.000050\n",
      "Epoch Time: 1.6 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 9407/9407 [01:28<00:00, 106.43it/s, loss=3.66e+9, avg_loss=3.12e+9, gpu_mem=26MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/100 Summary:\n",
      "Train Loss: 3116782505.732327\n",
      "Val Loss: 3053407211.541225\n",
      "Learning Rate: 0.000025\n",
      "Epoch Time: 1.6 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 9407/9407 [01:27<00:00, 108.01it/s, loss=3.01e+9, avg_loss=3.07e+9, gpu_mem=26MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/100 Summary:\n",
      "Train Loss: 3073021358.596790\n",
      "Val Loss: 3103807486.526025\n",
      "Learning Rate: 0.000025\n",
      "Epoch Time: 1.6 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 9407/9407 [01:24<00:00, 111.22it/s, loss=4.05e+9, avg_loss=3.08e+9, gpu_mem=26MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/100 Summary:\n",
      "Train Loss: 3081908854.556819\n",
      "Val Loss: 3062055300.421926\n",
      "Learning Rate: 0.000025\n",
      "Epoch Time: 1.5 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 9407/9407 [01:22<00:00, 113.54it/s, loss=2.28e+9, avg_loss=3.05e+9, gpu_mem=26MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/100 Summary:\n",
      "Train Loss: 3052357906.083555\n",
      "Val Loss: 3079562436.864118\n",
      "Learning Rate: 0.000025\n",
      "Epoch Time: 1.5 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 9407/9407 [01:20<00:00, 116.19it/s, loss=2.82e+9, avg_loss=3.07e+9, gpu_mem=26MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/100 Summary:\n",
      "Train Loss: 3065496078.967577\n",
      "Val Loss: 3086353911.863657\n",
      "Learning Rate: 0.000025\n",
      "Epoch Time: 1.4 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 9407/9407 [01:19<00:00, 118.32it/s, loss=2.94e+9, avg_loss=3.06e+9, gpu_mem=26MB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/100 Summary:\n",
      "Train Loss: 3061312181.461465\n",
      "Val Loss: 3057757389.413174\n",
      "Learning Rate: 0.000013\n",
      "Epoch Time: 1.4 minutes\n",
      "\n",
      "Early stopping triggered after 15 epochs\n",
      "\n",
      "Training completed!\n",
      "Total training time: 0.4 hours\n",
      "Best validation loss: 3053407211.541225\n"
     ]
    }
   ],
   "source": [
    "# 1. Required imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 2. Dataset class\n",
    "class FraudDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.num_features = features.shape[1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# 3. Load and prepare data\n",
    "train_data = pd.read_csv('Data/processed/train.csv')\n",
    "val_data = pd.read_csv('Data/processed/val.csv')\n",
    "test_data = pd.read_csv('Data/processed/test.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "train_features = train_data.drop('is_fraud', axis=1).values\n",
    "train_labels = train_data['is_fraud'].values\n",
    "val_features = val_data.drop('is_fraud', axis=1).values\n",
    "val_labels = val_data['is_fraud'].values\n",
    "test_features = test_data.drop('is_fraud', axis=1).values\n",
    "test_labels = test_data['is_fraud'].values\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FraudDataset(train_features, train_labels)\n",
    "val_dataset = FraudDataset(val_features, val_labels)\n",
    "test_dataset = FraudDataset(test_features, test_labels)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "# 4. Time embedding\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "# 5. Model architecture\n",
    "class FraudDiffuseNet(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size=256, time_dim=32):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalEmbedding(time_dim),\n",
    "            nn.Linear(time_dim, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Label embedding\n",
    "        self.label_emb = nn.Embedding(2, hidden_size)\n",
    "        \n",
    "        # Feature transformation\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_features + hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_features)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t, labels):\n",
    "        # Time and label embeddings\n",
    "        t = self.time_mlp(t)\n",
    "        t = t + self.label_emb(labels)\n",
    "        \n",
    "        # Concatenate features with time embedding\n",
    "        x_t = torch.cat([x, t], dim=1)\n",
    "        \n",
    "        # Transform features\n",
    "        return self.net(x_t)\n",
    "\n",
    "# 6. Diffusion process with adaptive prior\n",
    "class FraudDiffusion:\n",
    "    def __init__(self, train_dataset, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        \n",
    "        # Calculate non-fraud distribution parameters\n",
    "        non_fraud_mask = train_dataset.labels == 0\n",
    "        non_fraud_features = train_dataset.features[non_fraud_mask]\n",
    "        self.mu_nf = non_fraud_features.mean(dim=0)\n",
    "        self.sigma_nf = non_fraud_features.std(dim=0)\n",
    "        \n",
    "        # Setup noise schedule\n",
    "        self.beta = self.prepare_noise_schedule()\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "        \n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.num_timesteps)\n",
    "    \n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None]\n",
    "        # Use non-fraud distribution for noise\n",
    "        ε = torch.randn_like(x) * self.sigma_nf + self.mu_nf\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * ε, ε\n",
    "    \n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.num_timesteps, size=(n,))\n",
    "\n",
    "# 7. Loss functions\n",
    "def probability_loss(predicted_noise, prior_mu, prior_sigma):\n",
    "    z_score = (predicted_noise - prior_mu) / prior_sigma\n",
    "    return 2 * torch.distributions.Normal(0, 1).cdf(torch.abs(z_score)).mean()\n",
    "\n",
    "# UPDATED triplet loss function\n",
    "def triplet_loss(anchor, positive, negative, margin=1.0):\n",
    "    # Make sure we have equal number of samples by taking the minimum\n",
    "    min_samples = min(len(positive), len(negative))\n",
    "    \n",
    "    # Randomly sample to get equal sizes\n",
    "    if len(positive) > min_samples:\n",
    "        idx = torch.randperm(len(positive))[:min_samples]\n",
    "        positive = positive[idx]\n",
    "    if len(negative) > min_samples:\n",
    "        idx = torch.randperm(len(negative))[:min_samples]\n",
    "        negative = negative[idx]\n",
    "    \n",
    "    # Also sample the anchor to match\n",
    "    if len(anchor) > min_samples:\n",
    "        idx = torch.randperm(len(anchor))[:min_samples]\n",
    "        anchor = anchor[idx]\n",
    "    \n",
    "    # Now calculate distances with balanced samples\n",
    "    pos_dist = F.pairwise_distance(anchor, positive)\n",
    "    neg_dist = F.pairwise_distance(anchor, negative)\n",
    "    \n",
    "    return torch.mean(torch.clamp(pos_dist - neg_dist + margin, min=0))\n",
    "\n",
    "# 8. Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = FraudDiffuseNet(num_features=train_dataset.num_features).to(device)\n",
    "diffusion = FraudDiffusion(train_dataset)\n",
    "diffusion.beta = diffusion.beta.to(device)\n",
    "diffusion.alpha = diffusion.alpha.to(device)\n",
    "diffusion.alpha_hat = diffusion.alpha_hat.to(device)\n",
    "diffusion.mu_nf = diffusion.mu_nf.to(device)\n",
    "diffusion.sigma_nf = diffusion.sigma_nf.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Loss weights\n",
    "w1 = 0.1  # Weight for probability loss\n",
    "w2 = 0.1  # Weight for triplet loss\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 5\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('checkpoints/frauddiffuse', exist_ok=True)\n",
    "os.makedirs('samples/frauddiffuse', exist_ok=True)\n",
    "\n",
    "# 9. Training loop\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    for batch_features, batch_labels in progress_bar:\n",
    "        batch_features = batch_features.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        batch_size = batch_features.shape[0]\n",
    "        \n",
    "        # Sample timesteps\n",
    "        t = diffusion.sample_timesteps(batch_size).to(device)\n",
    "        \n",
    "        # Get noisy features and noise\n",
    "        x_t, noise = diffusion.noise_images(batch_features, t)\n",
    "        \n",
    "        # Predict noise\n",
    "        predicted_noise = model(x_t, t, batch_labels)\n",
    "        \n",
    "        # Calculate losses\n",
    "        mse_loss = F.mse_loss(noise, predicted_noise)\n",
    "        prob_loss = probability_loss(predicted_noise, diffusion.mu_nf, diffusion.sigma_nf)\n",
    "        \n",
    "        # UPDATED triplet loss calculation\n",
    "        trip_loss = torch.tensor(0.0).to(device)\n",
    "        fraud_mask = batch_labels == 1\n",
    "        non_fraud_mask = batch_labels == 0\n",
    "        if fraud_mask.any() and non_fraud_mask.any():\n",
    "            fraud_samples = predicted_noise[fraud_mask]\n",
    "            non_fraud_samples = predicted_noise[non_fraud_mask]\n",
    "            if len(fraud_samples) > 0 and len(non_fraud_samples) > 0:\n",
    "                # Use the smaller batch as anchor\n",
    "                if len(fraud_samples) <= len(non_fraud_samples):\n",
    "                    trip_loss = triplet_loss(fraud_samples, fraud_samples, non_fraud_samples)\n",
    "                else:\n",
    "                    trip_loss = triplet_loss(non_fraud_samples, non_fraud_samples, fraud_samples)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = mse_loss + w1 * prob_loss + w2 * trip_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'avg_loss': np.mean(epoch_losses),\n",
    "            'gpu_mem': f\"{torch.cuda.memory_allocated()/1024**2:.0f}MB\"\n",
    "        })\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for val_features, val_labels in val_loader:\n",
    "            val_features = val_features.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "            batch_size = val_features.shape[0]\n",
    "            \n",
    "            t = diffusion.sample_timesteps(batch_size).to(device)\n",
    "            x_t, noise = diffusion.noise_images(val_features, t)\n",
    "            predicted_noise = model(x_t, t, val_labels)\n",
    "            \n",
    "            val_loss = F.mse_loss(noise, predicted_noise)\n",
    "            val_losses.append(val_loss.item())\n",
    "    \n",
    "    # Calculate average losses\n",
    "    train_loss = np.mean(epoch_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print epoch summary\n",
    "    epoch_time = (time.time() - epoch_start_time) / 60\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} Summary:\")\n",
    "    print(f\"Train Loss: {train_loss:.6f}\")\n",
    "    print(f\"Val Loss: {val_loss:.6f}\")\n",
    "    print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    print(f\"Epoch Time: {epoch_time:.1f} minutes\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'checkpoints/frauddiffuse/best_model.pt')\n",
    "        print(\"Saved new best model!\")\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping_counter >= early_stopping_patience:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "# Print final training summary\n",
    "total_training_time = time.time() - training_start_time\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Total training time: {total_training_time/3600:.1f} hours\")\n",
    "print(f\"Best validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class Changes:\n",
    "- Added MinMaxScaler with feature range (-1, 1) instead of standard scaling\n",
    "- Added scaler parameter to allow reuse of the same scaling for validation/test sets\n",
    "- Added inverse_transform method to convert generated samples back to original scale\n",
    "- Added fit parameter to control when scaling is fitted\n",
    "\n",
    "\n",
    "# Model Architecture Changes:\n",
    "- Added LayerNorm to help with numerical stability\n",
    "- Changed ReLU to GELU activation for better gradient flow\n",
    "- Added Dropout layers (0.1) to prevent overfitting\n",
    "- Enhanced time embedding with GELU and Dropout\n",
    "\n",
    "\n",
    "# Diffusion Process Changes:\n",
    "- Added clamp(min=1e-5) to prevent division by zero in standard deviation\n",
    "- Scaled noise magnitude (ε 0.1) to prevent extreme values\n",
    "- Added numerical stability in noise_images method\n",
    "\n",
    "# Loss Function Changes:\n",
    "- Added epsilon (1e-5) to prevent division by zero in probability_loss\n",
    "- Scaled distances in triplet_loss (multiplied by 0.1)\n",
    "- Reduced margin in triplet_loss from 1.0 to 0.1\n",
    "- Added better handling of uneven batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing data...\n",
      "\n",
      "Features being used:\n",
      "['cc_num', 'merchant', 'category', 'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip', 'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time', 'merch_lat', 'merch_long', 'trans_hour', 'trans_day', 'trans_month', 'trans_dayofweek']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\Env1\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Training on device: cuda\n",
      "Number of features: 24\n",
      "Number of epochs: 100\n",
      "Batch size: 128\n",
      "Initial learning rate: 1e-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 9407/9407 [01:06<00:00, 140.96it/s, loss=0.0558, prob_loss=0.3549, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100 Summary:\n",
      "Train Loss: 0.232400\n",
      "Val Loss: 0.020758\n",
      "Probability Loss: 0.323599\n",
      "Triplet Loss: 0.000126\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.2 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 9407/9407 [01:03<00:00, 147.98it/s, loss=0.0554, prob_loss=0.3650, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/100 Summary:\n",
      "Train Loss: 0.055730\n",
      "Val Loss: 0.019081\n",
      "Probability Loss: 0.360616\n",
      "Triplet Loss: 0.000189\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.1 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 9407/9407 [00:57<00:00, 163.01it/s, loss=0.0551, prob_loss=0.3607, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/100 Summary:\n",
      "Train Loss: 0.055412\n",
      "Val Loss: 0.019043\n",
      "Probability Loss: 0.362016\n",
      "Triplet Loss: 0.000177\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.0 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 9407/9407 [01:24<00:00, 111.61it/s, loss=0.0542, prob_loss=0.3556, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/100 Summary:\n",
      "Train Loss: 0.055189\n",
      "Val Loss: 0.018684\n",
      "Probability Loss: 0.361937\n",
      "Triplet Loss: 0.000174\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.5 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 9407/9407 [01:18<00:00, 119.24it/s, loss=0.0543, prob_loss=0.3641, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/100 Summary:\n",
      "Train Loss: 0.055003\n",
      "Val Loss: 0.018466\n",
      "Probability Loss: 0.363193\n",
      "Triplet Loss: 0.000171\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.4 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 9407/9407 [01:14<00:00, 125.53it/s, loss=0.0563, prob_loss=0.3626, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/100 Summary:\n",
      "Train Loss: 0.054876\n",
      "Val Loss: 0.018525\n",
      "Probability Loss: 0.363490\n",
      "Triplet Loss: 0.000181\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.3 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 9407/9407 [01:23<00:00, 112.98it/s, loss=0.0546, prob_loss=0.3577, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/100 Summary:\n",
      "Train Loss: 0.054797\n",
      "Val Loss: 0.018767\n",
      "Probability Loss: 0.363645\n",
      "Triplet Loss: 0.000196\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.4 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 9407/9407 [01:14<00:00, 126.65it/s, loss=0.0549, prob_loss=0.3672, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/100 Summary:\n",
      "Train Loss: 0.054741\n",
      "Val Loss: 0.018052\n",
      "Probability Loss: 0.363680\n",
      "Triplet Loss: 0.000201\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.3 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 9407/9407 [01:07<00:00, 138.86it/s, loss=0.0549, prob_loss=0.3621, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/100 Summary:\n",
      "Train Loss: 0.054701\n",
      "Val Loss: 0.018181\n",
      "Probability Loss: 0.363684\n",
      "Triplet Loss: 0.000185\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.2 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 9407/9407 [01:14<00:00, 126.85it/s, loss=0.0539, prob_loss=0.3606, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/100 Summary:\n",
      "Train Loss: 0.054652\n",
      "Val Loss: 0.017993\n",
      "Probability Loss: 0.363641\n",
      "Triplet Loss: 0.000180\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.3 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 9407/9407 [01:10<00:00, 134.24it/s, loss=0.0534, prob_loss=0.3615, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/100 Summary:\n",
      "Train Loss: 0.054627\n",
      "Val Loss: 0.018216\n",
      "Probability Loss: 0.363659\n",
      "Triplet Loss: 0.000185\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.2 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 9407/9407 [01:05<00:00, 142.79it/s, loss=0.0534, prob_loss=0.3667, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/100 Summary:\n",
      "Train Loss: 0.054604\n",
      "Val Loss: 0.017729\n",
      "Probability Loss: 0.363654\n",
      "Triplet Loss: 0.000181\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.2 minutes\n",
      "Saved new best model!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 9407/9407 [01:07<00:00, 139.38it/s, loss=0.0543, prob_loss=0.3641, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/100 Summary:\n",
      "Train Loss: 0.054569\n",
      "Val Loss: 0.018037\n",
      "Probability Loss: 0.363606\n",
      "Triplet Loss: 0.000193\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.2 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 9407/9407 [01:16<00:00, 123.59it/s, loss=0.0547, prob_loss=0.3657, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/100 Summary:\n",
      "Train Loss: 0.054512\n",
      "Val Loss: 0.018289\n",
      "Probability Loss: 0.363212\n",
      "Triplet Loss: 0.000179\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.3 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 9407/9407 [01:11<00:00, 131.09it/s, loss=0.0530, prob_loss=0.3569, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/100 Summary:\n",
      "Train Loss: 0.054448\n",
      "Val Loss: 0.017934\n",
      "Probability Loss: 0.362927\n",
      "Triplet Loss: 0.000199\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.3 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 9407/9407 [01:08<00:00, 138.30it/s, loss=0.0543, prob_loss=0.3614, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/100 Summary:\n",
      "Train Loss: 0.054415\n",
      "Val Loss: 0.018093\n",
      "Probability Loss: 0.362909\n",
      "Triplet Loss: 0.000201\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.2 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 9407/9407 [00:58<00:00, 160.63it/s, loss=0.0537, prob_loss=0.3554, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/100 Summary:\n",
      "Train Loss: 0.054407\n",
      "Val Loss: 0.018125\n",
      "Probability Loss: 0.362895\n",
      "Triplet Loss: 0.000195\n",
      "Learning Rate: 0.000100\n",
      "Epoch Time: 1.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 9407/9407 [00:58<00:00, 161.68it/s, loss=0.0553, prob_loss=0.3644, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/100 Summary:\n",
      "Train Loss: 0.054395\n",
      "Val Loss: 0.018023\n",
      "Probability Loss: 0.362924\n",
      "Triplet Loss: 0.000192\n",
      "Learning Rate: 0.000050\n",
      "Epoch Time: 1.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 9407/9407 [01:02<00:00, 151.62it/s, loss=0.0544, prob_loss=0.3586, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/100 Summary:\n",
      "Train Loss: 0.054365\n",
      "Val Loss: 0.017924\n",
      "Probability Loss: 0.362959\n",
      "Triplet Loss: 0.000189\n",
      "Learning Rate: 0.000050\n",
      "Epoch Time: 1.1 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████| 9407/9407 [01:07<00:00, 140.10it/s, loss=0.0553, prob_loss=0.3637, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/100 Summary:\n",
      "Train Loss: 0.054356\n",
      "Val Loss: 0.018140\n",
      "Probability Loss: 0.362954\n",
      "Triplet Loss: 0.000176\n",
      "Learning Rate: 0.000050\n",
      "Epoch Time: 1.2 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 9407/9407 [01:08<00:00, 138.12it/s, loss=0.0534, prob_loss=0.3593, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21/100 Summary:\n",
      "Train Loss: 0.054353\n",
      "Val Loss: 0.018133\n",
      "Probability Loss: 0.362958\n",
      "Triplet Loss: 0.000196\n",
      "Learning Rate: 0.000050\n",
      "Epoch Time: 1.2 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 9407/9407 [01:03<00:00, 148.22it/s, loss=0.0549, prob_loss=0.3673, trip_loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22/100 Summary:\n",
      "Train Loss: 0.054345\n",
      "Val Loss: 0.017887\n",
      "Probability Loss: 0.362949\n",
      "Triplet Loss: 0.000206\n",
      "Learning Rate: 0.000050\n",
      "Epoch Time: 1.1 minutes\n",
      "\n",
      "Early stopping triggered after 22 epochs\n",
      "\n",
      "Training completed!\n",
      "Total training time: 0.4 hours\n",
      "Best validation loss: 0.017729\n"
     ]
    }
   ],
   "source": [
    "# 1. Required imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 2. Dataset class with MinMax scaling\n",
    "class FraudDataset(Dataset):\n",
    "    def __init__(self, features, labels, scaler=None, fit=False):\n",
    "        if scaler is None and fit:\n",
    "            self.scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "            self.features = torch.FloatTensor(self.scaler.fit_transform(features))\n",
    "        elif scaler is not None:\n",
    "            self.scaler = scaler\n",
    "            self.features = torch.FloatTensor(self.scaler.transform(features))\n",
    "        else:\n",
    "            self.scaler = None\n",
    "            self.features = torch.FloatTensor(features)\n",
    "            \n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.num_features = features.shape[1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "    \n",
    "    def inverse_transform(self, features):\n",
    "        if self.scaler is not None:\n",
    "            return self.scaler.inverse_transform(features)\n",
    "        return features\n",
    "\n",
    "# 3. Time embedding\n",
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "# 4. Model architecture\n",
    "class FraudDiffuseNet(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size=256, time_dim=32):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalEmbedding(time_dim),\n",
    "            nn.Linear(time_dim, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Label embedding\n",
    "        self.label_emb = nn.Embedding(2, hidden_size)\n",
    "        \n",
    "        # Feature transformation\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(num_features + hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, num_features)\n",
    "        )\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm = nn.LayerNorm(num_features)\n",
    "        \n",
    "    def forward(self, x, t, labels):\n",
    "        # Time and label embeddings\n",
    "        t = self.time_mlp(t)\n",
    "        t = t + self.label_emb(labels)\n",
    "        \n",
    "        # Concatenate features with time embedding\n",
    "        x_t = torch.cat([x, t], dim=1)\n",
    "        \n",
    "        # Transform features\n",
    "        return self.norm(self.net(x_t))\n",
    "\n",
    "# 5. Diffusion process\n",
    "class FraudDiffusion:\n",
    "    def __init__(self, train_dataset, num_timesteps=1000, beta_start=1e-4, beta_end=0.02):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "        \n",
    "        # Calculate non-fraud distribution parameters\n",
    "        non_fraud_mask = train_dataset.labels == 0\n",
    "        non_fraud_features = train_dataset.features[non_fraud_mask]\n",
    "        self.mu_nf = non_fraud_features.mean(dim=0)\n",
    "        self.sigma_nf = non_fraud_features.std(dim=0).clamp(min=1e-5)\n",
    "        \n",
    "        # Setup noise schedule\n",
    "        self.beta = self.prepare_noise_schedule()\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "        \n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.num_timesteps)\n",
    "    \n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None]\n",
    "        ε = torch.randn_like(x)\n",
    "        scaled_noise = ε * 0.1\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * scaled_noise, scaled_noise\n",
    "    \n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.num_timesteps, size=(n,))\n",
    "\n",
    "# 6. Loss functions\n",
    "def probability_loss(predicted_noise, prior_mu, prior_sigma):\n",
    "    z_score = (predicted_noise - prior_mu) / (prior_sigma + 1e-5)\n",
    "    return torch.mean(torch.abs(z_score))\n",
    "\n",
    "def triplet_loss(anchor, positive, negative, margin=0.1):\n",
    "    min_samples = min(len(positive), len(negative))\n",
    "    \n",
    "    if len(positive) > min_samples:\n",
    "        idx = torch.randperm(len(positive))[:min_samples]\n",
    "        positive = positive[idx]\n",
    "    if len(negative) > min_samples:\n",
    "        idx = torch.randperm(len(negative))[:min_samples]\n",
    "        negative = negative[idx]\n",
    "    if len(anchor) > min_samples:\n",
    "        idx = torch.randperm(len(anchor))[:min_samples]\n",
    "        anchor = anchor[idx]\n",
    "    \n",
    "    pos_dist = F.pairwise_distance(anchor, positive) * 0.1\n",
    "    neg_dist = F.pairwise_distance(anchor, negative) * 0.1\n",
    "    \n",
    "    return torch.mean(torch.clamp(pos_dist - neg_dist + margin, min=0))\n",
    "\n",
    "# 7. Load and prepare data\n",
    "print(\"Loading and preparing data...\")\n",
    "train_data = pd.read_csv('Data/processed/train.csv').drop('Unnamed: 0', axis=1, errors='ignore')\n",
    "val_data = pd.read_csv('Data/processed/val.csv').drop('Unnamed: 0', axis=1, errors='ignore')\n",
    "test_data = pd.read_csv('Data/processed/test.csv').drop('Unnamed: 0', axis=1, errors='ignore')\n",
    "\n",
    "# Print feature names for verification\n",
    "print(\"\\nFeatures being used:\")\n",
    "print(train_data.drop('is_fraud', axis=1).columns.tolist())\n",
    "\n",
    "# Prepare features and labels\n",
    "train_features = train_data.drop('is_fraud', axis=1).values\n",
    "train_labels = train_data['is_fraud'].values\n",
    "val_features = val_data.drop('is_fraud', axis=1).values\n",
    "val_labels = val_data['is_fraud'].values\n",
    "test_features = test_data.drop('is_fraud', axis=1).values\n",
    "test_labels = test_data['is_fraud'].values\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FraudDataset(train_features, train_labels, scaler=None, fit=True)\n",
    "val_dataset = FraudDataset(val_features, val_labels, scaler=train_dataset.scaler)\n",
    "test_dataset = FraudDataset(test_features, test_labels, scaler=train_dataset.scaler)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "# 8. Training setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = FraudDiffuseNet(num_features=train_features.shape[1]).to(device)\n",
    "diffusion = FraudDiffusion(train_dataset)\n",
    "\n",
    "# Move diffusion parameters to device\n",
    "diffusion.beta = diffusion.beta.to(device)\n",
    "diffusion.alpha = diffusion.alpha.to(device)\n",
    "diffusion.alpha_hat = diffusion.alpha_hat.to(device)\n",
    "diffusion.mu_nf = diffusion.mu_nf.to(device)\n",
    "diffusion.sigma_nf = diffusion.sigma_nf.to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "os.makedirs('samples', exist_ok=True)\n",
    "os.makedirs('metrics', exist_ok=True)\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 10\n",
    "early_stopping_counter = 0\n",
    "training_start_time = time.time()\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'prob_loss': [], 'triplet_loss': [],\n",
    "    'learning_rate': []\n",
    "}\n",
    "\n",
    "# 9. Training loop\n",
    "print(\"\\nStarting training...\")\n",
    "print(f\"Training on device: {device}\")\n",
    "print(f\"Number of features: {train_features.shape[1]}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Batch size: 128\")\n",
    "print(f\"Initial learning rate: 1e-4\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Training metrics\n",
    "    train_losses = []\n",
    "    prob_losses = []\n",
    "    triplet_losses = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    for batch_features, batch_labels in progress_bar:\n",
    "        batch_features = batch_features.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        # Sample timesteps\n",
    "        t = diffusion.sample_timesteps(batch_features.shape[0]).to(device)\n",
    "        \n",
    "        # Get noisy features and noise\n",
    "        x_t, noise = diffusion.noise_images(batch_features, t)\n",
    "        \n",
    "        # Predict noise\n",
    "        predicted_noise = model(x_t, t, batch_labels)\n",
    "        \n",
    "        # Calculate losses\n",
    "        mse_loss = F.mse_loss(noise, predicted_noise)\n",
    "        prob_loss = probability_loss(predicted_noise, diffusion.mu_nf, diffusion.sigma_nf)\n",
    "        \n",
    "        # Calculate triplet loss if we have both fraud and non-fraud samples\n",
    "        trip_loss = torch.tensor(0.0).to(device)\n",
    "        fraud_mask = batch_labels == 1\n",
    "        non_fraud_mask = batch_labels == 0\n",
    "        \n",
    "        if fraud_mask.any() and non_fraud_mask.any():\n",
    "            fraud_samples = predicted_noise[fraud_mask]\n",
    "            non_fraud_samples = predicted_noise[non_fraud_mask]\n",
    "            if len(fraud_samples) > 0 and len(non_fraud_samples) > 0:\n",
    "                trip_loss = triplet_loss(fraud_samples, fraud_samples, non_fraud_samples)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = mse_loss + 0.1 * prob_loss + 0.1 * trip_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record losses\n",
    "        train_losses.append(loss.item())\n",
    "        prob_losses.append(prob_loss.item())\n",
    "        triplet_losses.append(trip_loss.item())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{loss.item():.4f}\",\n",
    "            'prob_loss': f\"{prob_loss.item():.4f}\",\n",
    "            'trip_loss': f\"{trip_loss.item():.4f}\"\n",
    "        })\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for val_features, val_labels in val_loader:\n",
    "            val_features = val_features.to(device)\n",
    "            val_labels = val_labels.to(device)\n",
    "            \n",
    "            t = diffusion.sample_timesteps(val_features.shape[0]).to(device)\n",
    "            x_t, noise = diffusion.noise_images(val_features, t)\n",
    "            predicted_noise = model(x_t, t, val_labels)\n",
    "            \n",
    "            val_loss = F.mse_loss(noise, predicted_noise)\n",
    "            val_losses.append(val_loss.item())\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    train_loss = np.mean(train_losses)\n",
    "    val_loss = np.mean(val_losses)\n",
    "    epoch_prob_loss = np.mean(prob_losses)\n",
    "    epoch_triplet_loss = np.mean(triplet_losses)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Update history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['prob_loss'].append(epoch_prob_loss)\n",
    "    history['triplet_loss'].append(epoch_triplet_loss)\n",
    "    history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # Print epoch summary\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} Summary:\")\n",
    "    print(f\"Train Loss: {train_loss:.6f}\")\n",
    "    print(f\"Val Loss: {val_loss:.6f}\")\n",
    "    print(f\"Probability Loss: {epoch_prob_loss:.6f}\")\n",
    "    print(f\"Triplet Loss: {epoch_triplet_loss:.6f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(f\"Epoch Time: {epoch_time/60:.1f} minutes\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'history': history,\n",
    "            'scaler': train_dataset.scaler,\n",
    "            'feature_names': train_data.drop('is_fraud', axis=1).columns.tolist()\n",
    "        }, 'checkpoints/best_model_v2.pt')\n",
    "        print(\"Saved new best model!\")\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping_counter >= early_stopping_patience:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "# Print final training summary\n",
    "total_training_time = time.time() - training_start_time\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Total training time: {total_training_time/3600:.1f} hours\")\n",
    "print(f\"Best validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
